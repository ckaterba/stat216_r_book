# Distribution calculations

```{r, echo = F}
library(tidyverse)
```


The second module of STAT216 at FVCC focuses on the basics of probability theory. We start out learning the foundations: interpretations of probability (frequentist vs Bayesian) along with the notions of independence, mutually exclusive events, conditional probability, and Bayes' Theorem.  

We then move on to thinking about **random variables**. Recall that a **random variable** is simply the assignment of a *number* to every element of the sample space for the random process in question. From this assignment we can assign probabilities to the values our random variable can take on. Random variables also give us a way to define the average outcome of a random process and measure the spread of outcomes around that average.  

Recall that random variables come in two primary flavors, *discrete* and *continuous*.  Continuous random variables can take on at least some interval worth of real numbers. Discrete random variables, on the other hand, can only take on values with sufficiently large gaps in between.  For example, the height of a STAT216 student is a continuous random variable, but the number of books a STAT216 student owns in discrete. Distinguishing between these two types of random variables is important because the flavor dictates quite a bit about how we use and make calculations associated with a given random variable.  The probability distribution of a discrete random variable can be represented in a tabular form, but the probability distribution of a continuous random variable must be represented by a *function*.  The table below helps compare and contrast discrete and continuous probability distributions.

| Discrete | Continuous 
| :---:|  :---:|
| all probabilities $\geq 0$ | graph of function above or on $x$-axis | 
| sum of probabilities is 1 | area between graph of function and $x$-axis is one| 

**Short technical note**: Using the right mathematical perspective you do not actually treat discrete and continuous random variables any differently, but this perspective takes a lot of technical machinery and isn't necessary. Suffice it to say that the analogies above are more than just analogies. 


There are many, many different types of each flavor of random variable - too many to name or describe - but statisticians and mathematicians have studied many *families* of random variables that have commonalities. In STAT216, we learn about a handful of these families, discussed below. You can skip the details for now and come back to these as we learn more about them in class.

**Discrete distributions**:

- Bernoulli random variables: only two outcomes, typically 0 and 1 with 0 corresponding to failure and 1 to success for the random process you're studying. 

|$x_i$ |  0 | 1 | 
|:---: | :---: | :---: |
|$P(X = x_i)$ | $1-p$ | $p$ | 

- Geometric random variables: repeat independent identical (iid) trials of a Bernoulli random variable until the first success and count the *number of trials*. If $p$ is the probability of success in a single trial,

\[ P(X = k) = (1-p)^{k-1}p \]

- Binomial random variables: repeat a fixed number $n$ of iid trials of a Bernoulli random variable and *count the number of successes*, $k$.

\[ P(X = k) = {n \choose k} p^k (1-p)^{n-k}\]


**Continuous distributions**:

- Normal distributions: a family of symmetric, unimodal continuous distributions determined by an average and standard deviation. 

- Student's $t$-distribution: a family of symmetric, unimondal continuous distributions determined by a single quantity: degrees of freedom, $df$.  These distributions have fatter tails than standard normal distribution. 

- $\chi_^2$-distributions: a family of right-skewed distributions determined by a single parameter, $df$, or degrees of freedom. A $\chi^2$-distribution is a sum of $df$ squared standard normal distributions. 

- $F$-distributions; a family of right-skewed distributions determined by two degrees of freedom $df_1$ and $df_2$. Roughly speaking, an $F$-distribution is the ratio of two $\chi^2$ distributions 

The remainder of this chapter focuses on how to make calculations for each of these distributions in R. This may come as a welcome relief from the last few chapters because we'll be using R more as a big calculator again for a bit. 

## Finite discrete distribution calculations

This short section will describe how to calculate the expected value, variance, and standard deviation of a finite, discrete probability distribution.  The key is to remember that R performs all list calculations component-wise. With this in mind, recall that the expected value and variance of a finite discrete random variable $X$ are

\[ E(X) = \sum_{i= 1}^n x_i P(X = x_i) 
\quad \text{and} \quad
V(X) = \sum_{i = 1}^n P(X = x_i)\left(x_i - E(X) \right)^2\]

so $E(X)$ is just the average outcome of $X$, weighted by the probabilities associated to each value of $X$.  Similarly $V(X)$ is just the average squared deviation of the values of $X$ from the mean. 
Let's learn how to make these calculations in R from an example.  Suppose you and your friends invent a silly game to pass the time based on flipping a coin and rolling a standard six-sided die.  

- If you flip a heads and roll an even number, you win a dollar. 

- If you flip a heads and roll an *odd prime*, you win two dollars. 

- If you flip a tails and roll a 6, you win five dollars. 

- Otherwise, you win nothing. 

**Question** How much should you and your friends charge to play the game? 

To answer this, we need to know the expected winnings since expected value gives us the average outcome.  You and your friends need to charge at least the expected winnings. To make this calculation in R we need to store the values of our random variable and the associated probabilities in lists.  

The sample space for our game has size 12; you can think about is as the collection of ordered pairs \(\{ (x,y) | x \in \{H, T\}, y \in \{1,2,\ldots, 6\} \} \). There are 3 ways to flip a heads and roll an even number, two ways to flip a heads and roll an odd prime, and only one way to flip a tails and roll a six. Thus the probability distribution of our winnings is 

| $x_i$ | 1 | 2 | 5 | 0 |
|:---: | :---: |:---: |:---: |:---: |
| $P(X = x_i)$ | 3/12 | 2/12 | 1/12 | 6/12|

Storing this in R

```{r}
val <- c(1,2,5,0)
prob <- c(3/12, 2/12, 1/12, 6/12)
```

Now, to calculate the expected value, we just need to add up the product of `val` and `prob`:

```{r}
exp_val <- sum(val*prob)
exp_val
```
So on average, a player of this game will win one dollar. This means you and your friends should charge *at least* one dollar to play the game. 

Let's also calculate the variance and standard deviation of the winnings for this game. The strategy for calculating variance is exactly the same as with the expected value. 

```{r}
varX <- sum(prob*(val - exp_val)^2)
varX
```

As usual, the standard deviation is just the square root of the variance so

```{r}
sdX <- sqrt(varX)
sdX
```

This means, on average, someone playing this game will win \$1 with an average change between games of about \$1.41.  

Next up, we'll learn about making probability calculations for some of the named distributions discussed in the beginning of this chapter.

## Named distribution calculations

R has a robust library of probability distribution calculators built in. The functions are organized so that they behave similarly for all of the families of distributions.  The basic syntax is 

\[ \text{function type} +  \text{distribution} \]

The table below describes the type of function and describes what they do for both continuous and discrete probability distributions

| Function | input | Continuous | Discrete |
|:---: | :---: | :---: |:---: |
| `d` (density) | value of random variable | value of function / height of graph of probability density function at a given $x$-value | Probability that random variable takes on a specific value | 
| `p` (percentile) | value of random variable| returns a percentile, ie a lower tail probability of associated to a value of the random variable, $P(X \leq x)$. | Same as continuous | 
| `q` (quantile) | a percentile |returns a quantile, ie the value of a random variable corresponding to the percentile input| Same as continuous|
| `r` (random) | whole number $n$ |$n$ randomly sampled numbers from the distribution | Same as continuous |

Note that the biggest difference lies in the the density functions. This is because probabilities correspond to areas under the graph of the probability density function for continuous random variables, so the probability of observing any particular value is zero in this case. For a discrete probability distribution, however, the density function actually returns probabilities. 

The percentile `p` and the quantile `q` functions both have an option called `lower.tail` that is a logical, so either `TRUE` or `FALSE`.  

- If `lower.tail=TRUE`, the percentile functions return $P(X \leq x)$ and the quantile functions return the value $x$ such that $P(X \leq x)$.

- If `lower.tail=FALSE`, the percentile functions return $P(X > x)$ and the quantile functions return the value $x$ such that $P(X > x)$.

We will look at examples in what follows that will clarify these functions and their arguments. 

### Discrete random variables

Let's start off with some named families of discrete random variables. We'll only look at binomial and geometric distributions, but once you have these down, you should be be able to figure out how to use any other discrete random variable distribution functions such as those associated to Poisson or hypergeometric random variables. Note: we do not cover Poisson on or hypergeometric distributions in this class!

#### Binomial distribution

We'll start with the definition and a motivating example. 

**Definition:** Suppose you repeat independent and identical trials of a Bernoulli experiment a fixed number $n$ times. The random variable $X$ counting the *number of successes $k$* is called a **binomial random variable**.  If the probability of success in each trial is $p$, then the probability of failure is $1-p$ and A little work shows 

\[ P(X = k) = { n \choose k } p^k (1-p)^{n-k}. \]

In class we learn formulas for the expected value and variance of geometric random variables:

\[ E(X) = np \quad \text{and} \quad V(X) = np(1-p). \]

The R shorthand for binomial distributions is `binom`, so the primary functions for binomial calculations and their outputs are:

- `dbinom(k, n, p)` \( = P( X = k) = { n \choose k } p^k (1-p)^{n-k}\)

- `pbinom(k, n, p)` \(= P( X \leq k) = \sum_{i=0}^k { n \choose i } p^i (1-p)^{n-i}\)

- `pbinom(k, n, p, lower.tail = FALSE)`  \( = P( X > k) = \sum_{i=k+1}^n { n \choose i } p^i (1-p)^{n-i}\)

- `qbinom(prob, n, p)` is the integer $k$ such that \(P(X \leq k) = \) `prob`.

- `qbinom(prob, n, p, lower.tail = FALSE)` is the integer $k$ such that \( P(X > k) = \) `prob`.

- `rbinom(N, n, p)` generates `N` numbers between $0$ and $n$ by repeating the binomial experiment `N` times. 

Now let's look at an example to see how we can use these functions in practice. 

**Example:** A [Gallup poll](https://news.gallup.com/poll/15370/party-affiliation.aspx) showed that roughly 43\% of all voters in the USA identify as politically independent as of September 16, 2022.  You decide to loosely test this claim in the Flathead Valley by polling a random sample of 15 people about their political ideology. 

Assuming the distribution of political party affiliation in the Flathead is the same as the nation, counting the number of independents in your poll determines a binomial random variable. 

We can get a feel for the distribution of this variable with a bar chart.

```{r}
bf <- tibble(k = 0:15, 
             prob = dbinom(k, 15, .43))

 ggplot(bf, aes(x = as.factor(k), y = prob)) + 
   geom_col(color = "black", fill = "steelblue") +
   xlab("k") +
   ylab("P(X=k)") +
   labs(title = "Binomial Prob dist w/ p = .43 and n = 15")
```

Now let's use R to answer the following questions: 

1. What is the probability of exactly 4 independents in your poll?

    We want $P(X = 4)$, with $n = 15$, $k = 4$ and $p = .43$, so
    
    ```{r}
    dbinom(4, 15, .43)
    ```

2.  What is the probability of at most 4 independents in your poll?  

    We want $P(X \leq 4)$, with $n = 15$, $k = 4$ and $p = .43$, so
    
    ```{r}
    pbinom(4, 15, .43)
    ```

3. What is the probability of at least 4 independents in your poll? 

    We want $P(X \geq 4) = P(X > 3)$, with $n = 15$, $k = 4$ and $p = .43$, so
    
    ```{r}
    pbinom(3, 15, .43, lower.tail= FALSE)
    #or
    1 - pbinom(3, 15, .43)
    ```

4. What is the third quartile of this distribution? 

    We want to find the value $k$ so that $P(X \leq k) = .75$, so
    
    ```{r}
    qbinom(.75, 15, .43)
    ```
    
    Now note:
    
    ```{r}
    pbinom(8, 15, .43)
    ```


    which is greater than .75; R finds the closest integer valued quantile without going under the given percentile.

#### Geometric distribution

As above, we'll start with a definition then move to a motivating example.

**Definition:** Suppose you repeat independent and identical trials of a Bernoulli experiment *until your first success*.  The random variable $X$ counting the number of trials you perform is a **geometric random variable**.  If the probability of success in each trial is $p$, then the probability of failure is $1-p$ and some work shows 

\[ P(X = k ) = p(1-p)^{k-1}\]

In class we learn formulas for the expected value and variance of geometric random variables:

\[ E(X) = \frac{1}{p} \quad \text{and} \quad V(X) = \frac{1-p}{p^2}. \]

The R shorthand for binomial distributions is `geom`. R requires that you enter *the number of failures before your first success*, so the primary functions for binomial calculations and their outputs are:

- `dgeom(k-1, n, p)` \( = P(X = k ) = p(1-p)^{k-1}\)

- `pgeom(k-1, n, p)` \(= P( X \leq k) = \sum_{i=1}^{k} p (1-p)^{i-1}\)

- `pgeom(k-1, n, p, lower.tail = FALSE)`  \( = P( X > k) = \sum_{i=k+1}^\infty p (1-p)^{i-1}\)

- `qgeom(prob, n, p)` is the integer $k$ such that \(P(X \leq k) = \) `prob`.

- `qgeom(prob, n, p, lower.tail = FALSE)` is the integer $k$ such that \( P(X > k) = \) `prob`.

- `rbinom(N, n, p)` generates `N` integers by performing the experiment $N$ times and count the number of trials until the first success. 

Let's use the same example, but in a slightly different context to help differentiate between binomial and geometric random variables. 

**Example:** A [Gallup poll](https://news.gallup.com/poll/15370/party-affiliation.aspx) showed that roughly 43\% of all voters in the USA identify as politically independent as of September 16, 2022.  You decide to loosely test this claim in the Flathead Valley by polling random people until you find someone who identify as politically independent. You count the number of people you survey. 

Assuming the distribution of political party affiliation in the Flathead is the same as the nation, counting the number of people you survey until your first independent determines a geometric random variable.  

We can get a feel for the distribution of this variable with a bar chart.

```{r}
gf <- tibble(k = 1:15, 
             prob = dgeom(k-1,.43))

ggplot(gf, aes(x = as.factor(k), y = prob)) + 
  geom_col(color = "black", fill = "steelblue") +
  xlab("k") +
  ylab("P(X=k)") +
  labs(title = "Geometric Prob. dist. w/ p = .43 for k = 1 to 15")
```

1. What is the probability you the 6th person you poll is the first independent? 

    We're trying to find $P(X = 6)$, so
    
    ```{r}
    dgeom(5, .43)
    # or
    (1-.43)^5*.43
    ```
2. What is the probability you will survey at most 6 people before meeting your first independent? 

    We're trying to find $P(X \leq 6)$ so
    
    ```{r}
    pgeom(5, .43)
    ```
    
3. What is the probability  you will survey at least 6 people before meeting your first independent?

    We're trying to find $P(X \geq 6) = P(X > 5)$ so
    
    ```{r}
    pgeom(4, .43, lower.tail = FALSE) # 1 - P(X <= 5) = P(X >5) = P(X >=6)
    # or
    1 - (dgeom(0, .43) + dgeom(1, .43) + dgeom(2, .43) + dgeom(3, .43)
         + dgeom(4, .43))
    ```
    
4. What is the third quartile of this distribution? 

    ```{r}
    qgeom(.75, .43)
    ```
  As with a binomial random variable, note that
  
    ```{r}
    pgeom(2, .43)
    ```
    
    is greater than .75; R finds the nearest integer yielding a larger percentile. 
    
### Continuous random variables
