<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Inferential statistics: Take 1 | R and RStudio for STAT216</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.33 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Inferential statistics: Take 1 | R and RStudio for STAT216" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Inferential statistics: Take 1 | R and RStudio for STAT216" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Charles Katerba" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="distribution-calculations.html"/>
<link rel="next" href="inferential-statistics-take-2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R for STAT216 @ FVCC</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#organization"><i class="fa fa-check"></i><b>1.1</b> Organization</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#getting-started"><i class="fa fa-check"></i><b>1.2</b> Getting started</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#code-in-this-book"><i class="fa fa-check"></i><b>1.3</b> Code in this book</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#help"><i class="fa fa-check"></i><b>1.4</b> Help</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#debugging"><i class="fa fa-check"></i><b>1.5</b> Debugging</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#other-resources"><i class="fa fa-check"></i><b>1.6</b> Other resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rstudio-basics.html"><a href="rstudio-basics.html"><i class="fa fa-check"></i><b>2</b> RStudio basics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rstudio-basics.html"><a href="rstudio-basics.html#a-big-calculator"><i class="fa fa-check"></i><b>2.1</b> A big calculator</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="rstudio-basics.html"><a href="rstudio-basics.html#numbers"><i class="fa fa-check"></i><b>2.1.1</b> Numbers</a></li>
<li class="chapter" data-level="2.1.2" data-path="rstudio-basics.html"><a href="rstudio-basics.html#strings"><i class="fa fa-check"></i><b>2.1.2</b> Strings</a></li>
<li class="chapter" data-level="2.1.3" data-path="rstudio-basics.html"><a href="rstudio-basics.html#logicals-and-logical-operators"><i class="fa fa-check"></i><b>2.1.3</b> Logicals and Logical operators</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="rstudio-basics.html"><a href="rstudio-basics.html#naming-things"><i class="fa fa-check"></i><b>2.2</b> Naming things</a></li>
<li class="chapter" data-level="2.3" data-path="rstudio-basics.html"><a href="rstudio-basics.html#lists-and-vectors"><i class="fa fa-check"></i><b>2.3</b> Lists and vectors</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="rstudio-basics.html"><a href="rstudio-basics.html#creating-lists"><i class="fa fa-check"></i><b>2.3.1</b> Creating lists</a></li>
<li class="chapter" data-level="2.3.2" data-path="rstudio-basics.html"><a href="rstudio-basics.html#subsetting-lists"><i class="fa fa-check"></i><b>2.3.2</b> Subsetting lists</a></li>
<li class="chapter" data-level="2.3.3" data-path="rstudio-basics.html"><a href="rstudio-basics.html#special-lists"><i class="fa fa-check"></i><b>2.3.3</b> Special lists</a></li>
<li class="chapter" data-level="2.3.4" data-path="rstudio-basics.html"><a href="rstudio-basics.html#list-arithmetic"><i class="fa fa-check"></i><b>2.3.4</b> List arithmetic</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rstudio-basics.html"><a href="rstudio-basics.html#packages"><i class="fa fa-check"></i><b>2.4</b> Packages</a></li>
<li class="chapter" data-level="2.5" data-path="rstudio-basics.html"><a href="rstudio-basics.html#data-frames-and-tibbles"><i class="fa fa-check"></i><b>2.5</b> Data frames and tibbles</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="rstudio-basics.html"><a href="rstudio-basics.html#creating-data-frames"><i class="fa fa-check"></i><b>2.5.1</b> Creating data frames</a></li>
<li class="chapter" data-level="2.5.2" data-path="rstudio-basics.html"><a href="rstudio-basics.html#using-data-frames"><i class="fa fa-check"></i><b>2.5.2</b> Using data frames</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="rstudio-basics.html"><a href="rstudio-basics.html#r-markdown-documents"><i class="fa fa-check"></i><b>2.6</b> R Markdown documents</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="cross.html"><a href="cross.html"><i class="fa fa-check"></i><b>3</b> Summary statistics and data visualization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="cross.html"><a href="cross.html#summary-statistics-in-r"><i class="fa fa-check"></i><b>3.1</b> Summary statistics in R</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="cross.html"><a href="cross.html#numerical-variables"><i class="fa fa-check"></i><b>3.1.1</b> Numerical variables</a></li>
<li class="chapter" data-level="3.1.2" data-path="cross.html"><a href="cross.html#categorical-variables"><i class="fa fa-check"></i><b>3.1.2</b> Categorical variables</a></li>
<li class="chapter" data-level="3.1.3" data-path="cross.html"><a href="cross.html#combining-it-all"><i class="fa fa-check"></i><b>3.1.3</b> Combining it all</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="cross.html"><a href="cross.html#data-visualization"><i class="fa fa-check"></i><b>3.2</b> Data visualization</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="cross.html"><a href="cross.html#base-r"><i class="fa fa-check"></i><b>3.2.1</b> Base R</a></li>
<li class="chapter" data-level="3.2.2" data-path="cross.html"><a href="cross.html#ggplot2"><i class="fa fa-check"></i><b>3.2.2</b> <code>ggplot2</code></a></li>
<li class="chapter" data-level="3.2.3" data-path="cross.html"><a href="cross.html#saving-plots"><i class="fa fa-check"></i><b>3.2.3</b> Saving plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="wrangling-data.html"><a href="wrangling-data.html"><i class="fa fa-check"></i><b>4</b> Wrangling data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="wrangling-data.html"><a href="wrangling-data.html#the-goal-tidy-data."><i class="fa fa-check"></i><b>4.1</b> The goal: “tidy” data.</a></li>
<li class="chapter" data-level="4.2" data-path="wrangling-data.html"><a href="wrangling-data.html#the-pipe"><i class="fa fa-check"></i><b>4.2</b> The pipe</a></li>
<li class="chapter" data-level="4.3" data-path="wrangling-data.html"><a href="wrangling-data.html#common-tidying-operations"><i class="fa fa-check"></i><b>4.3</b> Common tidying operations</a></li>
<li class="chapter" data-level="4.4" data-path="wrangling-data.html"><a href="wrangling-data.html#mutate-group-by-and-summarize"><i class="fa fa-check"></i><b>4.4</b> Mutate, group by, and summarize</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="distribution-calculations.html"><a href="distribution-calculations.html"><i class="fa fa-check"></i><b>5</b> Distribution calculations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="distribution-calculations.html"><a href="distribution-calculations.html#finite-discrete-distribution-calculations"><i class="fa fa-check"></i><b>5.1</b> Finite discrete distribution calculations</a></li>
<li class="chapter" data-level="5.2" data-path="distribution-calculations.html"><a href="distribution-calculations.html#named-distribution-calculations"><i class="fa fa-check"></i><b>5.2</b> Named distribution calculations</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="distribution-calculations.html"><a href="distribution-calculations.html#discrete-random-variables"><i class="fa fa-check"></i><b>5.2.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="5.2.2" data-path="distribution-calculations.html"><a href="distribution-calculations.html#continuous-random-variables"><i class="fa fa-check"></i><b>5.2.2</b> Continuous random variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html"><i class="fa fa-check"></i><b>6</b> Inferential statistics: Take 1</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#basics-of-statistical-inference"><i class="fa fa-check"></i><b>6.1</b> Basics of statistical inference</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#sampling-distributions"><i class="fa fa-check"></i><b>6.1.1</b> Sampling distributions</a></li>
<li class="chapter" data-level="6.1.2" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#confidence-intervals"><i class="fa fa-check"></i><b>6.1.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="6.1.3" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#hypothesis-tests"><i class="fa fa-check"></i><b>6.1.3</b> Hypothesis tests</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#analyzing-categorical-variables"><i class="fa fa-check"></i><b>6.2</b> Analyzing categorical variables</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#single-sample-proportion"><i class="fa fa-check"></i><b>6.2.1</b> Single sample proportion</a></li>
<li class="chapter" data-level="6.2.2" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#two-sample-proportion"><i class="fa fa-check"></i><b>6.2.2</b> Two sample proportion</a></li>
<li class="chapter" data-level="6.2.3" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#chi-squared-tests"><i class="fa fa-check"></i><b>6.2.3</b> Chi-squared tests</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#analyzing-numerical-variables"><i class="fa fa-check"></i><b>6.3</b> Analyzing numerical variables</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#single-sample-mean"><i class="fa fa-check"></i><b>6.3.1</b> Single sample mean</a></li>
<li class="chapter" data-level="6.3.2" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#two-sample-mean"><i class="fa fa-check"></i><b>6.3.2</b> Two sample mean</a></li>
<li class="chapter" data-level="6.3.3" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#paired-data"><i class="fa fa-check"></i><b>6.3.3</b> Paired data</a></li>
<li class="chapter" data-level="6.3.4" data-path="inferential-statistics-take-1.html"><a href="inferential-statistics-take-1.html#analysis-of-variance-anova"><i class="fa fa-check"></i><b>6.3.4</b> Analysis of variance (ANOVA)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html"><i class="fa fa-check"></i><b>7</b> Inferential statistics, take 2</a>
<ul>
<li class="chapter" data-level="7.1" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#analyzing-categorical-variables-1"><i class="fa fa-check"></i><b>7.1</b> Analyzing categorical variables</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#single-sample-proportions"><i class="fa fa-check"></i><b>7.1.1</b> Single sample proportions</a></li>
<li class="chapter" data-level="7.1.2" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#two-sample-proportions"><i class="fa fa-check"></i><b>7.1.2</b> Two sample proportions</a></li>
<li class="chapter" data-level="7.1.3" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#chi-squared-goodness-of-fit-test"><i class="fa fa-check"></i><b>7.1.3</b> Chi-squared goodness of fit test</a></li>
<li class="chapter" data-level="7.1.4" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#chi-squared-test-of-independence-1"><i class="fa fa-check"></i><b>7.1.4</b> Chi-squared test of independence</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#analyzing-numerical-variables-1"><i class="fa fa-check"></i><b>7.2</b> Analyzing numerical variables</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#single-sample-mean-1"><i class="fa fa-check"></i><b>7.2.1</b> Single sample mean</a></li>
<li class="chapter" data-level="7.2.2" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#two-sample-mean-1"><i class="fa fa-check"></i><b>7.2.2</b> Two sample mean</a></li>
<li class="chapter" data-level="7.2.3" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#paired-data-1"><i class="fa fa-check"></i><b>7.2.3</b> Paired data</a></li>
<li class="chapter" data-level="7.2.4" data-path="inferential-statistics-take-2.html"><a href="inferential-statistics-take-2.html#analysis-of-variance-anova-1"><i class="fa fa-check"></i><b>7.2.4</b> Analysis of variance (ANOVA)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="modeling-relationships.html"><a href="modeling-relationships.html"><i class="fa fa-check"></i><b>8</b> Modeling relationships</a>
<ul>
<li class="chapter" data-level="8.1" data-path="modeling-relationships.html"><a href="modeling-relationships.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="8.2" data-path="modeling-relationships.html"><a href="modeling-relationships.html#multiple-linear-regression"><i class="fa fa-check"></i><b>8.2</b> Multiple linear regression</a></li>
<li class="chapter" data-level="8.3" data-path="modeling-relationships.html"><a href="modeling-relationships.html#bonus-topic-logistic-regression"><i class="fa fa-check"></i><b>8.3</b> Bonus topic: logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R and RStudio for STAT216</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inferential-statistics-take-1" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Inferential statistics: Take 1<a href="inferential-statistics-take-1.html#inferential-statistics-take-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter and the next are going to introduce inferential statistics in R, but from two different perspectives. This chapter will focus on doing all of the calculations involved in statistical inference “by hand” (where here we really mean using R to implement the formulas/ideas). The next chapter will show you how to perform many of the same calculations using functions native to R. It is valuable knowing how to do both so that you can check one or the other or possibly write your own type of statistical test one day!</p>
<p>Both chapters will be organized according to the type of variables we will be analyzing. So, for example, we’ll start with statistical inference for a single categorical variable. One can (and probably should) read the “by hand” section of this chapter in tandem with the corresponding native function section of the next chapter. This chapter will include a brief summary of the ideas involved in statistical inference, but the next will just go straight into the details.</p>
<p>The code chunk below attaches all packages we’ll use in this chapter and sets a <em>seed</em> which basically determines a starting point for some of the randomization we’ll employ in the chapter. Setting a seed insures that the same random samples will be drawn every time the code is run.</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="inferential-statistics-take-1.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb349-2"><a href="inferential-statistics-take-1.html#cb349-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb349-3"><a href="inferential-statistics-take-1.html#cb349-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(openintro)</span>
<span id="cb349-4"><a href="inferential-statistics-take-1.html#cb349-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(infer)</span>
<span id="cb349-5"><a href="inferential-statistics-take-1.html#cb349-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb349-6"><a href="inferential-statistics-take-1.html#cb349-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1187</span>)</span></code></pre></div>
<div id="basics-of-statistical-inference" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Basics of statistical inference<a href="inferential-statistics-take-1.html#basics-of-statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will learn about three primary types of statistical inference in this class, each aimed at answering different types of questions. To illustrate the types of questions we endeavor to answer, think about analyzing the political landscape in America. You could ask:</p>
<ol style="list-style-type: decimal">
<li><p>What proportion of Americans identify as politically independent?</p></li>
<li><p>Are there more politically independent people in Montana than in Idaho?</p></li>
<li><p>How what proportion of voters will identify as politically independent in the future?</p></li>
</ol>
<p>The first question is asking for an estimate of a population parameter (the true proportion of independents in America). The second question, on the other hand, is asking about a difference between two population parameters. The third requires a prediction based on past data. These types of questions are certainly related (for instance, you could answer the second question by estimating each of the two population parameters), but the most common tools for answering them are slightly different.</p>
<p>Our main tool for answering the first question above is a <strong>confidence interval</strong>, which uses an estimate of the amount of variable we expect between <em>samples</em> to provided a range of plausible values for the population parameter. The tool we use to answer the second question is called a <strong>hypothesis test</strong>; these test assess how likely or unlikely your <em>sample</em> is if there were no differences. Hypothesis tests involve understanding the amount of variability we expect to see between samples. We access this this quantity by understanding the distribution of possible samples. The probability distribution associated to all possible samples is called a <strong>sampling distribution</strong>, which we discuss more in the next section.</p>
<div id="sampling-distributions" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Sampling distributions<a href="inferential-statistics-take-1.html#sampling-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Taking a simple random sample is, of course, a random process. Assigning a number like the sample proportion or sample average to this sample, then, naturally turns sample statistics into random variables. Since we can think of each sample statistic as a random variable, each sample statistic has an associated probability distribution called a <strong>sampling distribution</strong>! The overall population under consideration determines the sampling distribution and we almost never have access to population-level data, so you may wonder how or if we have any hope of understanding the sampling distribution of our statistic of interest. In particular, we hope to understand the shape, the center, and the spread of the sampling distribution. There are two primary was of doing this: through simulation and through theory. This class touches briefly on the simulation approach, but focuses mostly on the theoretical approach to accessing sampling distributions.</p>
<p>Before moving on to describe these approaches, we need a quick definition that helps us differentiate from talking about a population distribution and talking about a sampling distribution. The <strong>standard error</strong>, abbreviated SE, of a sampling distribution is simply its standard deviation.</p>
<p>One of the miracles of statistics (Note: it is not a <em>real</em> miracle since it is a mathematical theorem, but it amazes the author to this day.) is the <strong>Central Limit Theorem</strong>. We will state the Central Limit Theorem in a few different ways throughout this chapter, but each version essentially says the same thing. Namely, if you have a sufficiently large (caution: this means different things in different contexts) and high quality sample (IE a sample with independent observations, usually obtained through random sampling), the sampling distribution will be symmetric and unimodal with its center at the true population parameter. Moreover, as the sample size increases, the standard error decreases. In other words, all sample statistics will cluster around the true population parameter and observing a sample stat above the population parameter is just as likely as observing one below. The fact that the standard error decreases as the sample size increases codifies our intuition that “large samples are usually better,” since it implies that a sample statistic calculated from a larger sample is more likely to be close to the true population parameter than one calculated from a smaller sample.</p>
<p>Before moving on to discuss how we use sampling distributions, let’s try to make this a bit more concrete by simulating a sampling distribution using a technique called <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)"><strong>bootstrapping</strong></a> which takes a large, random sample, and resamples it many times <em>with replacement</em> to simulate the process of taking many different samples. We’ll use the <code>gss2010</code> data set from the <code>openintro</code> package which records the highest educational attainment (among other things) of 2044 randomly sampled Americans in 2010.</p>
<p>Suppose we’re interested in knowing more about the proportion of all Americans who do not have a high school diploma. The <code>gss2010</code> data set serves as our sample and we can use the sample proportion <span class="math inline">\(\hat{p}\)</span>, read as “p hat”, as a <strong>point estimate</strong> for the true population proportion <span class="math inline">\(p\)</span>.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="inferential-statistics-take-1.html#cb350-1" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> <span class="fu">sum</span>(gss2010<span class="sc">$</span>degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>) <span class="sc">/</span> <span class="fu">length</span>(gss2010<span class="sc">$</span>degree)</span>
<span id="cb350-2"><a href="inferential-statistics-take-1.html#cb350-2" aria-hidden="true" tabindex="-1"></a>pHat</span></code></pre></div>
<pre><code>## [1] 0.1492172</code></pre>
<p>So our sample indicates that in 2010, about 15% of the American population had less than a high school diploma. But this is only a sample of 2044 Americans. If we took another sample of 2044 people and calculated the same proportion, how different do we expect that proportion to be? We can simulate this by drawing from our original sample, but doing so <em>with replacement</em>.</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="inferential-statistics-take-1.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Take sample </span></span>
<span id="cb352-2"><a href="inferential-statistics-take-1.html#cb352-2" aria-hidden="true" tabindex="-1"></a>sample2 <span class="ot">&lt;-</span> <span class="fu">sample</span>( gss2010<span class="sc">$</span>degree, <span class="at">size =</span> <span class="dv">2044</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb352-3"><a href="inferential-statistics-take-1.html#cb352-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Find proportion of high school dropouts</span></span>
<span id="cb352-4"><a href="inferential-statistics-take-1.html#cb352-4" aria-hidden="true" tabindex="-1"></a>pHat2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(sample2 <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>) <span class="sc">/</span> <span class="fu">length</span>(sample2)</span>
<span id="cb352-5"><a href="inferential-statistics-take-1.html#cb352-5" aria-hidden="true" tabindex="-1"></a>pHat2</span></code></pre></div>
<pre><code>## [1] 0.1364971</code></pre>
<p>This sample proportion is different, but not by too much. Of course, 2 different samples don’t provide enough evidence to make any conclusions about the amount of variability we should expect among <em>all</em> possible samples. To estimate the standard error, we need to simulate many more samples. We’ll use the following code, using commands from the <code>infer</code> package, to do this. The code below takes 5000 samples of size 2044 from our original sample again with replacement, changes the degree column in each sample to a binary outcome (no HS diploma, or other), counts the number of people in each sample without a high school diploma, then calculates the corresponding sample proportion.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="inferential-statistics-take-1.html#cb354-1" aria-hidden="true" tabindex="-1"></a>simulated_sampling_distr <span class="ot">&lt;-</span> gss2010 <span class="sc">%&gt;%</span></span>
<span id="cb354-2"><a href="inferential-statistics-take-1.html#cb354-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">2044</span>, <span class="at">reps =</span> <span class="dv">5000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb354-3"><a href="inferential-statistics-take-1.html#cb354-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">degree =</span> <span class="fu">if_else</span>(degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>, <span class="st">&quot;LT HIGH SCHOOL&quot;</span>, <span class="st">&quot;other&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb354-4"><a href="inferential-statistics-take-1.html#cb354-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(degree) <span class="sc">%&gt;%</span></span>
<span id="cb354-5"><a href="inferential-statistics-take-1.html#cb354-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p_hat =</span> n <span class="sc">/</span> <span class="fu">sum</span>(n)) <span class="sc">%&gt;%</span> </span>
<span id="cb354-6"><a href="inferential-statistics-take-1.html#cb354-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>( degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>)</span>
<span id="cb354-7"><a href="inferential-statistics-take-1.html#cb354-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb354-8"><a href="inferential-statistics-take-1.html#cb354-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(simulated_sampling_distr)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
## # Groups:   replicate [6]
##   replicate degree             n p_hat
##       &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;
## 1         1 LT HIGH SCHOOL   322 0.158
## 2         2 LT HIGH SCHOOL   315 0.154
## 3         3 LT HIGH SCHOOL   317 0.155
## 4         4 LT HIGH SCHOOL   310 0.152
## 5         5 LT HIGH SCHOOL   314 0.154
## 6         6 LT HIGH SCHOOL   311 0.152</code></pre>
<p>We’ve simulated a sampling distribution! Let’s look at a histogram of this simulated distribution to assess it’s shape and spread. Remember, the Central Limit Theorem says this distribution should likely by symmetric and unimodal.</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="inferential-statistics-take-1.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(simulated_sampling_distr, <span class="fu">aes</span>(<span class="at">x =</span> p_hat)) <span class="sc">+</span> </span>
<span id="cb356-2"><a href="inferential-statistics-take-1.html#cb356-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">15</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span> </span>
<span id="cb356-3"><a href="inferential-statistics-take-1.html#cb356-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribution of sample proportions, n = 2044&quot;</span>, </span>
<span id="cb356-4"><a href="inferential-statistics-take-1.html#cb356-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Estimating SE for 2010 proportion of Americans without a HS diploma&quot;</span>,</span>
<span id="cb356-5"><a href="inferential-statistics-take-1.html#cb356-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Sample proportion, p-hat&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-196-1.png" width="672" /></p>
<p>Voila! Our sampling distribution does seem to be symmetric and unimodal. Where precisely is the center?</p>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="inferential-statistics-take-1.html#cb357-1" aria-hidden="true" tabindex="-1"></a><span class="co">#average sample proportion</span></span>
<span id="cb357-2"><a href="inferential-statistics-take-1.html#cb357-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(simulated_sampling_distr<span class="sc">$</span>p_hat)</span></code></pre></div>
<pre><code>## [1] 0.1492041</code></pre>
<p>What, approximately, is the <strong>standard error</strong>? Simply the standard deviation of the column <code>p_hat</code>!</p>
<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb359-1"><a href="inferential-statistics-take-1.html#cb359-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(simulated_sampling_distr<span class="sc">$</span>p_hat)</span></code></pre></div>
<pre><code>## [1] 0.007862322</code></pre>
<p>This means if we were to repeatedly sample 2044 Americans and record the proportion of people with less than a college degree, we should expect to see a difference of about 0.8% between the proportions on average. As will see in the next two sections, <strong>understanding the standard error is the key to performing inferential statistics</strong>. However, before we move on to using sampling distributions, let’s simulate the Central Limit Theorem’s other main claim: the standard error decreases as the sample size increases.</p>
<p>To show this, we’ll go the opposite direction and take smaller samples from the <code>gss2010</code> data set. We should see the standard error of this simulated distribution <em>increase</em>. The code below simulates a sampling distribution with sample size <span class="math inline">\(n = 100\)</span>.</p>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb361-1"><a href="inferential-statistics-take-1.html#cb361-1" aria-hidden="true" tabindex="-1"></a>sampling_dist2 <span class="ot">&lt;-</span> gss2010 <span class="sc">%&gt;%</span></span>
<span id="cb361-2"><a href="inferential-statistics-take-1.html#cb361-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rep_sample_n</span>(<span class="at">size =</span> <span class="dv">100</span>, <span class="at">reps =</span> <span class="dv">5000</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span></span>
<span id="cb361-3"><a href="inferential-statistics-take-1.html#cb361-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>( <span class="at">degree =</span> <span class="fu">if_else</span>(degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>, <span class="st">&quot;LT HIGH SCHOOL&quot;</span>, <span class="st">&quot;other&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb361-4"><a href="inferential-statistics-take-1.html#cb361-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(degree) <span class="sc">%&gt;%</span></span>
<span id="cb361-5"><a href="inferential-statistics-take-1.html#cb361-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">p_hat =</span> n <span class="sc">/</span> <span class="fu">sum</span>(n)) <span class="sc">%&gt;%</span> </span>
<span id="cb361-6"><a href="inferential-statistics-take-1.html#cb361-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>( degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>)</span>
<span id="cb361-7"><a href="inferential-statistics-take-1.html#cb361-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb361-8"><a href="inferential-statistics-take-1.html#cb361-8" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(sampling_dist2)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
## # Groups:   replicate [6]
##   replicate degree             n p_hat
##       &lt;int&gt; &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;
## 1         1 LT HIGH SCHOOL     8  0.08
## 2         2 LT HIGH SCHOOL    20  0.2 
## 3         3 LT HIGH SCHOOL    21  0.21
## 4         4 LT HIGH SCHOOL    15  0.15
## 5         5 LT HIGH SCHOOL    15  0.15
## 6         6 LT HIGH SCHOOL    20  0.2</code></pre>
<p>Note that just looking at the first 6 sample proportions we can already see more variability than our first distribution. Let’s visualize this one.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="inferential-statistics-take-1.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(sampling_dist2, <span class="fu">aes</span>(<span class="at">x =</span> p_hat)) <span class="sc">+</span> </span>
<span id="cb363-2"><a href="inferential-statistics-take-1.html#cb363-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">15</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span> </span>
<span id="cb363-3"><a href="inferential-statistics-take-1.html#cb363-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribution of sample proportions, n = 100&quot;</span>, </span>
<span id="cb363-4"><a href="inferential-statistics-take-1.html#cb363-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Estimating SE for 2010 proportion of Americans without a HS diploma&quot;</span>,</span>
<span id="cb363-5"><a href="inferential-statistics-take-1.html#cb363-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Sample proportion, p-hat&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-200-1.png" width="672" /></p>
<p>Notice that the center is about the same as our first sampling distribution, but our second is more spread out as we claimed above! To check these claims, let’s look at the average sample proportion and the estimated standard error.</p>
<div class="sourceCode" id="cb364"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb364-1"><a href="inferential-statistics-take-1.html#cb364-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(sampling_dist2<span class="sc">$</span>p_hat)</span></code></pre></div>
<pre><code>## [1] 0.14884</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="inferential-statistics-take-1.html#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(sampling_dist2<span class="sc">$</span>p_hat)</span></code></pre></div>
<pre><code>## [1] 0.0353421</code></pre>
<p>The average sample proportions are super close, but the standard error of our distribution with <span class="math inline">\(n = 100\)</span> is about 5 times as large as the standard error of the distribution with <span class="math inline">\(n = 2044\)</span>. This fits our intuition: small samples are less reliable because they have more variability around the population parameter.</p>
<p>Now that we have a feel for sampling distributions, let’s use these simulated sampling distributions to make some inferences.</p>
</div>
<div id="confidence-intervals" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Confidence intervals<a href="inferential-statistics-take-1.html#confidence-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that a <strong>confidence interval</strong> provides a range of plausible values for a population parameter. For example, we will use our point estimates from the previous section to estimate the true proportion of Americans without a high school diploma. The width of a confidence level depends on two quantities:</p>
<ul>
<li><p>the <strong>confidence level</strong>: a measure of the likelihood that our interval contains to true population parameter. Typical confidence levels are 90%, 95%, and 99%.</p></li>
<li><p>the <strong>standard error</strong>: the standard deviation of the sampling distribution.</p></li>
</ul>
<p>Your confidence level determines a <em>critical value</em> <span class="math inline">\(CV\)</span>. If your confidence level is <span class="math inline">\(CL\)</span>, then <span class="math inline">\(CL\)</span>% of all sample proportions will fall within <span class="math inline">\(CV\)</span> standard deviations of the true population parameter within the sampling distribution. Thus, as your confidence level increase, the critical value increases, and so your confidence interval ends up getting wider. This makes sense: to be more confident in your estimate, your range of plausible values better be “larger.”</p>
<p>Now we can describe the general formula for confidence intervals:</p>
<p><span class="math display">\[ \text{point estimate} \pm CV \cdot SE\]</span></p>
<p>In the sections to come, the details of your point estimates along with the procedure to calculate the standard error and critical value will change, but the general formula for a confidence interval is always the same. With this in mind, it becomes much more important to be able to correctly interpret confidence intervals. Before closing out this section, let’s use our simulated sampling distribution from the previous section to estimate the proportion of Americans without a high school diploma in 2010. In particular, we’ll calculate a 95% confidence interval.</p>
<p>Looking back at the histogram of <code>simulated_sampling_distr</code>, we see that it is approximately normally distributed. Recall that roughly 95% of all observations in a normal distribution fall within two standard deviations from the center. Thus, as a rough approximation, our critical value is <span class="math inline">\(CV = 2\)</span>.</p>
<p>We’ve also already found our point estimate, the sample proportion of Americans without a high school diploma <code>pHat</code>, and our standard error, the standard deviation of the sampling distribution.</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="inferential-statistics-take-1.html#cb368-1" aria-hidden="true" tabindex="-1"></a>pHat</span></code></pre></div>
<pre><code>## [1] 0.1492172</code></pre>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="inferential-statistics-take-1.html#cb370-1" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sd</span>(simulated_sampling_distr<span class="sc">$</span>p_hat)</span>
<span id="cb370-2"><a href="inferential-statistics-take-1.html#cb370-2" aria-hidden="true" tabindex="-1"></a>SE</span></code></pre></div>
<pre><code>## [1] 0.007862322</code></pre>
<p>Using the formula, <span class="math inline">\(\text{point estimate} \pm CV \cdot SE\)</span>, our 95% confidence interval is</p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="inferential-statistics-take-1.html#cb372-1" aria-hidden="true" tabindex="-1"></a>CI <span class="ot">&lt;-</span> pHat <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>SE</span>
<span id="cb372-2"><a href="inferential-statistics-take-1.html#cb372-2" aria-hidden="true" tabindex="-1"></a>CI</span></code></pre></div>
<pre><code>## [1] 0.1334926 0.1649419</code></pre>
<p>In other words, with 95% confidence, we estimate that between 13.3% and 16.5% of all Americans in 2010 did not have a high school diploma.</p>
</div>
<div id="hypothesis-tests" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Hypothesis tests<a href="inferential-statistics-take-1.html#hypothesis-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use confidence intervals to estimate population parameters, but sometimes we only want to figure out if a population parameter differs from some value. This situation arises when you ask questions like “Will a majority of American Republicans vote for Donald Trump in the 2024 presidential election primary?” or “Do more than 10% of Americans fail to graduate high school?” To answer these types of questions statistically, we perform <strong>hypothesis tests</strong>. Every hypothesis test has two competing claims:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>, the null hypothesis. This typically represents the status quo, that there is no difference, that there is no effect, or that the variables independent.</p></li>
<li><p><span class="math inline">\(H_a\)</span>: the research or alternative hypothesis. This represents, well, your research question. There is a difference, or an effect, or the variables in question are dependent.</p></li>
</ul>
<p>We always express the null and alternative hypothesis as statements involving population parameters. For instance, if <span class="math inline">\(p\)</span> represents the proportion of Americans without a high school diploma in 2010, we would turn the question “Do more than 10% of Americans fail to graduate high school?” into a hypothesis test as</p>
<ul>
<li><p><span class="math inline">\(H_0 : p = .1\)</span> (or <span class="math inline">\(p \leq .1\)</span>)</p></li>
<li><p><span class="math inline">\(H_a : p &gt; .1\)</span></p></li>
</ul>
<p>We use sample data to evaluate hypothesis tests and do so by determining how likely or unlikely our sample data would be <em>if the null hypothesis were true</em>. This is called the <span class="math inline">\(p\)</span>-value of a sample. More precisely, the <strong><span class="math inline">\(p\)</span>-value</strong> of a sample is the probability of observing a sample at least as a favorable for the alternative hypothesis as your own, assuming the null hypothesis is true.</p>
<p>Let’s think about what a <span class="math inline">\(p\)</span>-value tells is. If you have a relatively small <span class="math inline">\(p\)</span>-value, say .01, then there would be a 1% chance of observing the data you have (or data that’s even more extreme) if the null were true. We have two competing possibilities here: either the null hypothesis is true and we have witnessed a rare event simply by chance or the alternative hypothesis is true. Unfortunately (and this is especially if you only have a single sample) there’s no way to tell which world you’re living in! As an attempt to remedy this dilemma, we must set a <strong>significance level</strong> <em>before</em> performing our hypothesis test. The most common significance levels are <span class="math inline">\(\alpha = .01, .05, \text{ and } .1\)</span>. You use the significance level <span class="math inline">\(\alpha\)</span> to evaluate the hypothesis test. In particular, if</p>
<ul>
<li><p>the <span class="math inline">\(p\)</span>-value is less than <span class="math inline">\(\alpha\)</span>, reject the null and accept the alternative. Or</p></li>
<li><p>the <span class="math inline">\(p\)</span>-value is greater than or equal to <span class="math inline">\(\alpha\)</span>, you fail to reject the null.</p></li>
</ul>
<p>Note that a hypothesis test is like a jury trial: a jury either finds the defendant guilty or not guilty, they never deem a defendant innocent. Similarly, we can never <em>prove</em> the null hypothesis; instead, we can only collect enough evidence to believe that it is false. With this in mind, we think of the <strong>significance level</strong> <span class="math inline">\(\alpha\)</span> as the probability of a false positive in a hypothesis test. In other words, if the null were true and you could repeatedly perform the same hypothesis test, you would end up rejecting the null <span class="math inline">\(\alpha\)</span>% of the time.</p>
<p>In general, there are three broad types of hypothesis tests: one-tailed upper, one-tailed lower, and two-tailed tests. The type of test determines how you calculate the <span class="math inline">\(p\)</span>-value of your sample because it determines what counts as “more favorable for the alternative hypothesis.”</p>
<p>To illustrate this idea, let’s pretend we’re performing a hypothesis test for a proportion with</p>
<p><span class="math display">\[ H_0:  p = .5 \]</span></p>
<p>and suppose we took a sample with size <span class="math inline">\(n = 200\)</span> and <span class="math inline">\(\hat{p} = .55\)</span>. Below we’ll look at the three types of alternative hypotheses and visualize the corresponding <span class="math inline">\(p\)</span>-values for our imaginary sample.</p>
<ol style="list-style-type: decimal">
<li><strong>Two-sided hypothesis test</strong>. In this case, we’re simply checking to see if the population parameter deviates from the null value. For our example:
<span class="math display">\[ H_a: p \neq .5  \]</span></li>
</ol>
<p>Samples that are at least as favorable for <span class="math inline">\(H_a\)</span> as our own come in two flavors: samples with a proportion of at least .55 and those with a proportion of <em>at most</em> .45. Notice that the samples with proportions of at most .45 really are just as favorable as those with proportions of at least .55 when we’re checking for a difference because they are equally as far away from the null proportion of .5. Thus, the <span class="math inline">\(p\)</span>-value of a two-sided hypothesis tests will be a two-tailed probability. The plot below helps visualize this <span class="math inline">\(p\)</span>-value as an area in the sampling distribution. We have hidden the plotting code here to prioritize the concepts.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-204-1.png" width="672" /></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>One-sided upper hypothesis test</strong>: In this case, we’re simply checking to see if the population parameter is greater than the null value. For our example:</li>
</ol>
<p><span class="math display">\[ H_a: p &gt; .5  \]</span></p>
<p>Samples that are at least as favorable for <span class="math inline">\(H_a\)</span> as our own only one flavor this time: samples with a proportion of at least .55. Thus the <span class="math inline">\(p\)</span>-value for this hypothesis test is an upper-tail area. The plot below helps us visualize this.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-205-1.png" width="672" /></p>
<ol start="3" style="list-style-type: decimal">
<li><strong>One-sided lower hypothesis test</strong>: In this case, we’re simply checking to see if the population parameter is less than the null value. For our example:</li>
</ol>
<p><span class="math display">\[ H_a: p &gt; .5  \]</span></p>
<p>Samples that are at least as favorable for <span class="math inline">\(H_a\)</span> as our own only one flavor: samples with a proportion of at most .55. Thus the <span class="math inline">\(p\)</span>-value for this hypothesis test is a lower-tail area in the sampling distribution. The plot below helps us visualize this.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-206-1.png" width="672" /></p>
<p>Our <span class="math inline">\(p\)</span>-value is large because our sample proportion is above the null value when we thought it was going to be below. We should revise our alternative hypothesis in this case.</p>
<p>Let’s finish this section using the high school diploma question we started this chapter with. In particular, in 2010 did more than 10% of Americans fail to graduate high school? As a hypothesis test:</p>
<ul>
<li><p><span class="math inline">\(H_0: p = .1\)</span></p></li>
<li><p><span class="math inline">\(H_a: p &gt; .1\)</span></p></li>
</ul>
<p>Let’s set our significance level to <span class="math inline">\(\alpha .05\)</span>. To evaluate the hypothesis test need to calculate a <span class="math inline">\(p\)</span>-value. To accomplish this, we need to be able to assess the likelihood of our sample assuming the null hypothesis is true. Here we’re still using the <code>gss2010</code> data for our sample; recall the recall the sample proportion is <span class="math inline">\(\hat{p} \approx 0.149\)</span> and the sample size is <span class="math inline">\(n = 2044\)</span>.</p>
<p>In the next section we’ll learn some theory that describes the sampling distribution, but we will simulate a sampling distribution under the null hypothesis.</p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="inferential-statistics-take-1.html#cb374-1" aria-hidden="true" tabindex="-1"></a><span class="co"># generates results from 5000 binomial experiments w/ probability of </span></span>
<span id="cb374-2"><a href="inferential-statistics-take-1.html#cb374-2" aria-hidden="true" tabindex="-1"></a><span class="co"># success = .1 and sample size of n = 2044, returns sample proportion. </span></span>
<span id="cb374-3"><a href="inferential-statistics-take-1.html#cb374-3" aria-hidden="true" tabindex="-1"></a>null_dist <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">p_hat =</span> <span class="fu">rbinom</span>(<span class="dv">5000</span>, <span class="dv">2044</span>, .<span class="dv">1</span>)<span class="sc">/</span><span class="dv">2044</span>)</span>
<span id="cb374-4"><a href="inferential-statistics-take-1.html#cb374-4" aria-hidden="true" tabindex="-1"></a><span class="co"># center of distribution</span></span>
<span id="cb374-5"><a href="inferential-statistics-take-1.html#cb374-5" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(null_dist<span class="sc">$</span>p_hat)</span></code></pre></div>
<pre><code>## [1] 0.1000021</code></pre>
<div class="sourceCode" id="cb376"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb376-1"><a href="inferential-statistics-take-1.html#cb376-1" aria-hidden="true" tabindex="-1"></a><span class="co"># standard error</span></span>
<span id="cb376-2"><a href="inferential-statistics-take-1.html#cb376-2" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sd</span>(null_dist<span class="sc">$</span>p_hat)</span>
<span id="cb376-3"><a href="inferential-statistics-take-1.html#cb376-3" aria-hidden="true" tabindex="-1"></a>SE</span></code></pre></div>
<pre><code>## [1] 0.006583714</code></pre>
<p>Let’s visualize the null distribution and our sample proportion <span class="math inline">\(\hat{p}\)</span>’s location in the nul distribution.</p>
<div class="sourceCode" id="cb378"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb378-1"><a href="inferential-statistics-take-1.html#cb378-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(null_dist, <span class="fu">aes</span>(<span class="at">x =</span> p_hat)) <span class="sc">+</span> </span>
<span id="cb378-2"><a href="inferential-statistics-take-1.html#cb378-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">15</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span> </span>
<span id="cb378-3"><a href="inferential-statistics-take-1.html#cb378-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> pHat, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb378-4"><a href="inferential-statistics-take-1.html#cb378-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Null distribution with p = .1 and n = 2044, pHat = .149&quot;</span>, </span>
<span id="cb378-5"><a href="inferential-statistics-take-1.html#cb378-5" aria-hidden="true" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Sample proportion, p-hat&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-208-1.png" width="672" /></p>
<p>As we’d expect the null sampling distribution is approximately normal. The plot above indicates that our sample proportion <span class="math inline">\(\hat{p}\)</span> would be quite unlikely if the null hypothesis were true. To estimate exactly how unlikely, let’s <em>estimate</em> the <span class="math inline">\(p\)</span>-value of our sample.</p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="inferential-statistics-take-1.html#cb379-1" aria-hidden="true" tabindex="-1"></a><span class="co"># count the number of simulated samples with proportions at least as large as </span></span>
<span id="cb379-2"><a href="inferential-statistics-take-1.html#cb379-2" aria-hidden="true" tabindex="-1"></a><span class="co"># our actual sample proportion, divide by number of samples in simulated dist.</span></span>
<span id="cb379-3"><a href="inferential-statistics-take-1.html#cb379-3" aria-hidden="true" tabindex="-1"></a>pValue <span class="ot">&lt;-</span> <span class="fu">sum</span>(null_dist<span class="sc">$</span>p_hat <span class="sc">&gt;=</span> pHat ) <span class="sc">/</span> <span class="fu">length</span>(null_dist<span class="sc">$</span>p_hat)</span>
<span id="cb379-4"><a href="inferential-statistics-take-1.html#cb379-4" aria-hidden="true" tabindex="-1"></a>pValue</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>This means that if 10% of Americans didn’t have high school diplomas in 2010, the probability of finding a sample of 2044 Americans of which 14.9% or more do not have a high school diploma is essentially 0.</p>
<p>Because our <span class="math inline">\(p\)</span>-value is less than the significance level <span class="math inline">\(\alpha = .05\)</span>, we reject the null hypothesis and accept the alternative. In other words, our data provide compelling evidence to believe that more than 10% of Americans in 2010 did not have a high school diploma.</p>
<p>We finish this section by noting that we spent a lot of time on this example and a lot of time/space on explanation here. Once you get the hang of this, you will perform and interpret hypothesis tests quickly and efficiently. Moreover, most of what we’ll do in this class will <em>not</em> require simulating sampling distributions. For more on this, read on!</p>
</div>
</div>
<div id="analyzing-categorical-variables" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Analyzing categorical variables<a href="inferential-statistics-take-1.html#analyzing-categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now we will move in to using theoretical sampling distributions to calculate confidence intervals and perform hypothesis tests. The key almost all of the theoretical sampling distributions is the <strong>central limit theorem</strong>, which states that, if you have a large enough sample of independent observations, the sampling distribution will be approximately normal. Moreover, the standard error decreases as your sample size increase. In the following sections we will see a few different incarnations of the central limit theorem; the big picture will stay the same while the fine details change as we analyze different types of data.</p>
<p>This section focus on analyzing categorical variables. When making inferences about categorical variables it is best to analyze <em>proportions</em> because these generalize to populations much better than counts. In particular we want to look at the proportion of responses in a sample that take on a specific value of the categorical variable. We’ve seen this already, of course (see the last section for an example). The following subsections describe how to make inferences using a single sample proportion, two sample proportions, or (in some sense) many sample proportions.</p>
<div id="single-sample-proportion" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Single sample proportion<a href="inferential-statistics-take-1.html#single-sample-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When analyzing a single categorical variable, the most basic parameter to make an inference about is the population proportion <span class="math inline">\(p\)</span>, the proportion of the population taking on a particular value of the categorical variable. For example, the proportion of American adults in 2010 that do not have a high school diploma. We use sample proportions <span class="math inline">\(\hat{p}\)</span> to make inferences about <span class="math inline">\(p\)</span> and under the right circumstances can apply the Central Limit Theorem to understand the distribution of sample proportions.</p>
<ul>
<li><p><strong>Sample statistic</strong>: sample proportion <span class="math inline">\(\hat{p}\)</span></p></li>
<li><p><strong>Population parameter</strong>: population proportion <span class="math inline">\(p\)</span></p></li>
<li><p>The <strong>Central Limit Theorem for a Single-sample Proportion</strong> states that if you</p>
<ul>
<li><p>have a sample of <span class="math inline">\(n\)</span> independent observations (hard to guarantee, but safe to assume if n &lt; 10% of the populatino and you employ random sampling) and</p></li>
<li><p>and at least 10 (expected) success and failures (ie <span class="math inline">\(\hat{p}n \geq 10\)</span> and <span class="math inline">\((1- \hat{p})n \geq 10\)</span> )</p></li>
</ul>
<p>Then the sampling distribution of <span class="math inline">\(\hat{p}\)</span> is approximately normal with a mean of <span class="math inline">\(p\)</span> and standard error <span class="math inline">\(SE = \sqrt{ \frac{p(1-p)}{n}}\)</span>. In symbols:
<span class="math display">\[ \hat{p} \sim N \left( \text{mean} = p, SE =\sqrt{ \frac{p(1-p)}{n}} \right) \]</span></p>
<p>Moreover, if <span class="math inline">\(p\)</span> is unknown (as it usually is!),
<span class="math display">\[ SE \approx \sqrt{ \frac{\hat{p}(1-\hat{p})}{n}}\]</span>
In other words, you can approximate the standard error using the sample proportion.</p></li>
</ul>
<p>In this situation, the hypotheses you need to meet in order to apply the Central Limit Theorem are typically easy to satisfy, which means we can use the CLT to make inferences in all of our favorite ways!</p>
<div id="confidence-intervals-1" class="section level4 hasAnchor" number="6.2.1.1">
<h4><span class="header-section-number">6.2.1.1</span> Confidence intervals<a href="inferential-statistics-take-1.html#confidence-intervals-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose you have a sample of <span class="math inline">\(n\)</span> observations that meets the conditions for applying the Central Limit Theorem. Then the distribution of sample proportions is approximately normal, centered at the true population proportion with a standard error of</p>
<p><span class="math display">\[ SE \approx \sqrt{ \frac{\hat{p}(1-\hat{p})}{n}}.\]</span></p>
<p>We can use these facts to calculate a confidence interval to estimate the population proportion. Recall that every confidence interval has the form</p>
<p><span class="math display">\[ \text{point estimate} \pm CV \cdot SE\]</span></p>
<p>where <span class="math inline">\(CV\)</span> is the critical value determined by the confidence level. The <strong>CLT</strong> provides the standard error and our sample gives the point estimate, so we only need to worry about the critical value. Recall that if your confidence level is <span class="math inline">\(CL\)</span>, then <span class="math inline">\(CL\)</span>% of all sample proportions will fall within <span class="math inline">\(CV\)</span> standard deviations of the true population parameter within the sampling distribution. The sampling distribution is normal by the Central Limit theorem, so we can use the <code>qnorm(...)</code> function to find precise critical values.</p>
<p>As an example, let’s find the critical value for a 95% confidence interval. We want to find the value <span class="math inline">\(z^\ast\)</span> so that 95% of all sample proportions fall within <span class="math inline">\(z^\ast\)</span> standard deviations of the population proportion. In other words, we need to find <span class="math inline">\(z^\ast\)</span> so that <span class="math inline">\(P ( |Z| \leq z^\ast ) = .95\)</span>. The graph below helps visualize the setup.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-210-1.png" width="672" /></p>
<p>We can see that the bounds are almost <span class="math inline">\(\pm2\)</span>, but not quite. If the central area is .95, then the tail area is .05. The upper and lower tails are symmetric, so the upper tail area is .025. Thus, to find our critical value, we simply evaluate</p>
<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb381-1"><a href="inferential-statistics-take-1.html#cb381-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qnorm</span>(.<span class="dv">025</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<p>Let’s use the same idea to find the critical values for some common confidence levels.</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="inferential-statistics-take-1.html#cb383-1" aria-hidden="true" tabindex="-1"></a>cvTable <span class="ot">&lt;-</span> <span class="fu">tibble</span>( <span class="at">conf.level =</span> <span class="fu">c</span>(.<span class="dv">9</span>, .<span class="dv">95</span>, .<span class="dv">98</span>, .<span class="dv">99</span>), </span>
<span id="cb383-2"><a href="inferential-statistics-take-1.html#cb383-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">crit.val =</span> <span class="fu">qnorm</span>( (<span class="dv">1</span> <span class="sc">-</span> conf.level)<span class="sc">/</span><span class="dv">2</span>, <span class="at">lower.tail =</span>F))</span>
<span id="cb383-3"><a href="inferential-statistics-take-1.html#cb383-3" aria-hidden="true" tabindex="-1"></a>cvTable</span></code></pre></div>
<pre><code>## # A tibble: 4 × 2
##   conf.level crit.val
##        &lt;dbl&gt;    &lt;dbl&gt;
## 1       0.9      1.64
## 2       0.95     1.96
## 3       0.98     2.33
## 4       0.99     2.58</code></pre>
<p>With that taken care of, we can calculate confidence intervals for any proportion we want at any confidence level! In particular, an <span class="math inline">\(\alpha\)</span>% confidence level for a single proportion is</p>
<p><span class="math display">\[\hat{p} \pm z^\ast_\alpha \cdot \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}.  \]</span></p>
<p>Note there that this really is the same formula as above just with the details of the <strong>CLT</strong> filled in.</p>
<p>Let’s close this off with an example, recreating what we did in the last section to estimate the proportion of American adults without high school degrees. As a reminder, this is from the <code>gss2010</code> data set from the <code>openintro</code> package.</p>
<p>The code chunk below goes through the calculation of a 95% confidence interval for the proportion in question.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="inferential-statistics-take-1.html#cb385-1" aria-hidden="true" tabindex="-1"></a><span class="co"># components of CI</span></span>
<span id="cb385-2"><a href="inferential-statistics-take-1.html#cb385-2" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> <span class="fu">sum</span>( gss2010<span class="sc">$</span>degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>) <span class="sc">/</span> <span class="fu">length</span>(gss2010<span class="sc">$</span>degree)</span>
<span id="cb385-3"><a href="inferential-statistics-take-1.html#cb385-3" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(pHat<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>pHat)<span class="sc">/</span><span class="fu">length</span>(gss2010<span class="sc">$</span>degree))</span>
<span id="cb385-4"><a href="inferential-statistics-take-1.html#cb385-4" aria-hidden="true" tabindex="-1"></a>zCrit <span class="ot">&lt;-</span> <span class="fu">qnorm</span>( (<span class="dv">1</span> <span class="sc">-</span> .<span class="dv">95</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb385-5"><a href="inferential-statistics-take-1.html#cb385-5" aria-hidden="true" tabindex="-1"></a><span class="co"># CI</span></span>
<span id="cb385-6"><a href="inferential-statistics-take-1.html#cb385-6" aria-hidden="true" tabindex="-1"></a>pHat <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span>zCrit<span class="sc">*</span>SE</span></code></pre></div>
<pre><code>## [1] 0.1337709 0.1646636</code></pre>
<p>So with 95% confidence, between 13.4% and 16.5% of all Americans in 2010 did not have a high school diploma. Compare this to the simulated example from the previous section: the confidence intervals are almost identical! You can think of this as the central limit theorem verifying simulations or simulations verifying the central limit theorem. In their case, it’s cool!</p>
<p>Next up, we’ll learn how to perform hypothesis tests using theoretical sampling distributions.</p>
</div>
<div id="hypothesis-testing" class="section level4 hasAnchor" number="6.2.1.2">
<h4><span class="header-section-number">6.2.1.2</span> Hypothesis testing<a href="inferential-statistics-take-1.html#hypothesis-testing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose you have a sample of <span class="math inline">\(n\)</span> observations and you’d like to perform a hypothesis test using this sample. Recall that you assume the null hypothesis is true when performing a hypothesis test. Thus, in order to apply the Central Limit Theorem, we need to make sure our sample is sufficiently large <em>under the null hypothesis</em>.</p>
<p>More explicitly, suppose the null proportion is <span class="math inline">\(p_0\)</span>. If</p>
<ul>
<li><p>your sample has <span class="math inline">\(n\)</span> independent observations and</p></li>
<li><p>your sample has at least 10 <em>expected</em> success and failures, ie <span class="math inline">\(n p_0 \geq 10\)</span> and <span class="math inline">\(n (1- p_0) \geq 10\)</span></p></li>
</ul>
<p>then the distribution of sample proportions is approximately normal:</p>
<p><span class="math display">\[ \hat{p} \sim N\left( \text{mean} = p_0 , SE = \sqrt{\frac{p_0(1-p_0)}{n}} \right). \]</span></p>
<p>We can use use this sampling distribution to calculate <span class="math inline">\(p\)</span>-values thus execute hypothesis tests. We will show how to do this first with the toy examples from the last section which demonstrate the three types of hypothesis tests then one last time using the familiar <code>gss2010</code> data.</p>
<p>For the next three examples, suppose we have a sample of <span class="math inline">\(n = 200\)</span> independent observations and a sample proportion of <span class="math inline">\(\hat{p} = .55\)</span>. We will perform three hypothesis tests to see if our sample provides sufficient evidence to conclude that the true population proportion is greater than .5, less than .5, or different from .5 using a siginificance level of <span class="math inline">\(\alpha = .05\)</span>.</p>
<p>In all cases, note that <span class="math inline">\(.5*200 = 100 \geq 10\)</span>, so the Central Limit Theorem applies. Furthermore</p>
<p><span class="math display">\[ \hat{p} \sim N\left(\text{mean} = .5, SE = \sqrt{\frac{.5\cdot .5}{200}} \right)  \]</span></p>
<ol style="list-style-type: decimal">
<li><p><strong>Two-sided hypothesis test:</strong> Our hypothesis test is</p>
<p><span class="math display">\[\begin{array}{cc}
H_0: p = .5 \\
H_a: p \neq .5
\end{array}
\]</span></p>
<p>and the image below helps us visualize the <span class="math inline">\(p\)</span>-value of our sample with <span class="math inline">\(\hat{p} = .55\)</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-214-1.png" width="672" /></p>
<p>The upper and lower tails of the <span class="math inline">\(p\)</span>-value are symmetric about the mean, so we can calculate one of them and double it.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="inferential-statistics-take-1.html#cb387-1" aria-hidden="true" tabindex="-1"></a>pNull <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb387-2"><a href="inferential-statistics-take-1.html#cb387-2" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(pNull<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> pNull)<span class="sc">/</span><span class="dv">200</span>)</span>
<span id="cb387-3"><a href="inferential-statistics-take-1.html#cb387-3" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> .<span class="dv">55</span></span>
<span id="cb387-4"><a href="inferential-statistics-take-1.html#cb387-4" aria-hidden="true" tabindex="-1"></a>pVal <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(.<span class="dv">55</span>, <span class="at">mean =</span> pNull, <span class="at">sd =</span> SE, <span class="at">lower.tail =</span> F)</span>
<span id="cb387-5"><a href="inferential-statistics-take-1.html#cb387-5" aria-hidden="true" tabindex="-1"></a>pVal</span></code></pre></div>
<pre><code>## [1] 0.1572992</code></pre>
<p>With a <span class="math inline">\(p\)</span>-value of approximately .16 we fail to reject the null hypothesis. The data do not provide compelling evidence of a difference.</p></li>
<li><p><strong>One-Sided Upper Hypothesis test:</strong> Our hypothesis test is</p>
<p><span class="math display">\[\begin{array}{cc}
H_0: p = .5 \\
H_a: p &gt; .5
\end{array}
\]</span></p>
<p>and the image below helps us visualize the <span class="math inline">\(p\)</span>-value of our sample with <span class="math inline">\(\hat{p} = .55\)</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-216-1.png" width="672" /></p>
<p>The <span class="math inline">\(p\)</span>-value is the shaded upper tail area and can be calculated as follows.</p>
<div class="sourceCode" id="cb389"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb389-1"><a href="inferential-statistics-take-1.html#cb389-1" aria-hidden="true" tabindex="-1"></a>pNull <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb389-2"><a href="inferential-statistics-take-1.html#cb389-2" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(pNull<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> pNull)<span class="sc">/</span><span class="dv">200</span>)</span>
<span id="cb389-3"><a href="inferential-statistics-take-1.html#cb389-3" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> .<span class="dv">55</span></span>
<span id="cb389-4"><a href="inferential-statistics-take-1.html#cb389-4" aria-hidden="true" tabindex="-1"></a>pVal <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(.<span class="dv">55</span>, <span class="at">mean =</span> pNull, <span class="at">sd =</span> SE, <span class="at">lower.tail =</span> F)</span>
<span id="cb389-5"><a href="inferential-statistics-take-1.html#cb389-5" aria-hidden="true" tabindex="-1"></a>pVal</span></code></pre></div>
<pre><code>## [1] 0.0786496</code></pre>
<p>With a <span class="math inline">\(p\)</span>-value of approximately .08 we fail to reject the null hypothesis at a significance level of <span class="math inline">\(\alpha =.05\)</span> . The data do not provide compelling evidence that the true proportion is greater than .5.</p></li>
<li><p><strong>One-Sided Lower Hypothesis test:</strong> Our hypothesis test is</p>
<p><span class="math display">\[\begin{array}{cc}
H_0: p = .5 \\
H_a: p &lt; .5
\end{array}
\]</span></p>
<p>and the image below helps us visualize the <span class="math inline">\(p\)</span>-value of our sample with <span class="math inline">\(\hat{p} = .55\)</span></p>
<p><img src="_main_files/figure-html/unnamed-chunk-218-1.png" width="672" /></p>
<p>The <span class="math inline">\(p\)</span>-value is the shaded llower tail area and can be calculated as follows.</p>
<div class="sourceCode" id="cb391"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb391-1"><a href="inferential-statistics-take-1.html#cb391-1" aria-hidden="true" tabindex="-1"></a>pNull <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb391-2"><a href="inferential-statistics-take-1.html#cb391-2" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(pNull<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> pNull)<span class="sc">/</span><span class="dv">200</span>)</span>
<span id="cb391-3"><a href="inferential-statistics-take-1.html#cb391-3" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> .<span class="dv">55</span></span>
<span id="cb391-4"><a href="inferential-statistics-take-1.html#cb391-4" aria-hidden="true" tabindex="-1"></a>pVal <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(.<span class="dv">55</span>, <span class="at">mean =</span> pNull, <span class="at">sd =</span> SE)</span>
<span id="cb391-5"><a href="inferential-statistics-take-1.html#cb391-5" aria-hidden="true" tabindex="-1"></a>pVal</span></code></pre></div>
<pre><code>## [1] 0.9213504</code></pre>
<p>No suprise here. With a <span class="math inline">\(p\)</span>-value of approximately .92 we fail to reject the null hypothesis at a significance level of <span class="math inline">\(\alpha =.05\)</span> . The data do not provide compelling evidence that the true proportion is less than .5.</p></li>
</ol>
<p>We close this section using the <code>gss2010</code> data to perform a hypothesis test to see if more than 10% of Americans in 2010 did not have a high school diploma. Again we’ll use a significance level of <span class="math inline">\(\alpha = .05\)</span>. Recall the following sample size and statistic from earlier.</p>
<div class="sourceCode" id="cb393"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb393-1"><a href="inferential-statistics-take-1.html#cb393-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(gss2010<span class="sc">$</span>degree)</span>
<span id="cb393-2"><a href="inferential-statistics-take-1.html#cb393-2" aria-hidden="true" tabindex="-1"></a>n</span></code></pre></div>
<pre><code>## [1] 2044</code></pre>
<div class="sourceCode" id="cb395"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb395-1"><a href="inferential-statistics-take-1.html#cb395-1" aria-hidden="true" tabindex="-1"></a>pHat <span class="ot">&lt;-</span> <span class="fu">sum</span>(gss2010<span class="sc">$</span>degree <span class="sc">==</span> <span class="st">&quot;LT HIGH SCHOOL&quot;</span>)<span class="sc">/</span>n</span>
<span id="cb395-2"><a href="inferential-statistics-take-1.html#cb395-2" aria-hidden="true" tabindex="-1"></a>pHat</span></code></pre></div>
<pre><code>## [1] 0.1492172</code></pre>
<p>We reformulate our question as a hypothesis test.</p>
<p><span class="math display">\[\begin{array}{cc}
H_0: p = .1 \\
H_a: p &gt; .1
\end{array}
\]</span></p>
<p>We assume the observations in the <code>gss2010</code> data set are independent. Before preceding with the hypothesis test, we need to make sure our sample size is sufficiently large so that we can use the Central Limit Theorem.</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="inferential-statistics-take-1.html#cb397-1" aria-hidden="true" tabindex="-1"></a>pNull <span class="ot">&lt;-</span> .<span class="dv">1</span></span>
<span id="cb397-2"><a href="inferential-statistics-take-1.html#cb397-2" aria-hidden="true" tabindex="-1"></a><span class="co"># will return a logical, T/F </span></span>
<span id="cb397-3"><a href="inferential-statistics-take-1.html#cb397-3" aria-hidden="true" tabindex="-1"></a>n<span class="sc">*</span>pNull <span class="sc">&gt;=</span> <span class="dv">10</span> </span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="inferential-statistics-take-1.html#cb399-1" aria-hidden="true" tabindex="-1"></a>n<span class="sc">*</span>(<span class="dv">1</span> <span class="sc">-</span> pNull) <span class="sc">&gt;=</span> <span class="dv">10</span></span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Since the calculation above returned two <code>TRUE</code>’s we’re good to go! The CLT implies.</p>
<p><span class="math display">\[ \hat{p} \sim N \left(\text{mean} = .1, SE = \sqrt{\frac{.1\cdot.9}{2044}} \right)\]</span></p>
<p>We are performing a one-sided upper hypothesis test, so our <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb401"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb401-1"><a href="inferential-statistics-take-1.html#cb401-1" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(pNull<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span> pNull)<span class="sc">/</span>n)</span>
<span id="cb401-2"><a href="inferential-statistics-take-1.html#cb401-2" aria-hidden="true" tabindex="-1"></a>pValue <span class="ot">&lt;-</span> <span class="fu">pnorm</span>(pHat, <span class="at">mean =</span> pNull, <span class="at">sd =</span> SE, <span class="at">lower.tail =</span> F)</span>
<span id="cb401-3"><a href="inferential-statistics-take-1.html#cb401-3" aria-hidden="true" tabindex="-1"></a>pValue </span></code></pre></div>
<pre><code>## [1] 5.983973e-14</code></pre>
<p>Since <span class="math inline">\(0 &lt; .05 = \alpha\)</span>, we reject the null and accept the research. Our data provide sufficient evidence to believe that more than 10% of Americans in 2010 did not have a high school diploma.</p>
<p>This <span class="math inline">\(p\)</span>-value is the same as the one we calculated in the previous section! This is no surprise: you can either think of this example as a check on simulation techniques or as the simulation techniques as a check on the central limit theorem.</p>
</div>
</div>
<div id="two-sample-proportion" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Two sample proportion<a href="inferential-statistics-take-1.html#two-sample-proportion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We often want to compare the same measurement or parameter for two different populations. As usual, we almost never know any single population parameter, so the best we can do is take a sample from each population and compare the sample statistics. When analyzing a categorical variable, we end up comparing sample proportions to make inferences about population proportions.</p>
<p>To make this more precise, suppose you’re investigating some proportion for two populations.</p>
<ul>
<li><p>Our population parameter of interest is <span class="math inline">\(p_1 - p2\)</span></p></li>
<li><p>Our sample statistic is <span class="math inline">\(\hat{p}_1- \hat{p}_2\)</span></p></li>
</ul>
<p>To make an inference about the population parameter, we need to learn about the sampling distribution of <span class="math inline">\(\hat{p}_1- \hat{p}_2\)</span>. Fortunately, if we can apply the central limit theorem for each sample individually, a version of the central limit theorem applies again! The formula for the standard error looks more complicated, but note that the variance of the sampling distrubtion for <span class="math inline">\(\hat{p}_1- \hat{p}_2\)</span> is just the sum of the variances for each each of the individual sampling distributions.</p>
<p><strong>CLT for two sample proportions</strong>: Suppose you are analyzing the difference between two population proportions <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span>, take two samples of size <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> from each population respectively. If</p>
<ul>
<li><p>the observations within each sample are independent from one another,</p></li>
<li><p>the observations between the samples are independent, and</p></li>
<li><p>each sample has at least ten expected or observed success and failures (ie <span class="math inline">\(n_ip_i \geq 10\)</span> and <span class="math inline">\(n_i(1 - p_i) \geq 10\)</span> for <span class="math inline">\(i= 1,2\)</span>)</p></li>
</ul>
<p>Then the sampling distribution for <span class="math inline">\(\hat{p}_1- \hat{p}_2\)</span> is approximately normally distributed. In particular,</p>
<p><span class="math display">\[ \hat{p}_1- \hat{p}_2 \sim N\left( \text{mean} = p_1 - p_2 \text{ and } SE = \sqrt{\frac{p_1(1-p_1)}{n_1}+ \frac{p_2(1-p_2)}{n_2}} \right) \]</span></p>
<p>The details on how we employ the CLT for two sample proportions changes depending on whether we’d like to calculate a confidence interval or perform a hypothesis test. As above, this difference arise because <em>always assume the null hypothesis is true</em> when performing a hypothesis test. The following subsections highlight exactly what these differences entail, but, briefly, it only changes the standard error calculation.</p>
<div id="confidence-intervals-2" class="section level4 hasAnchor" number="6.2.2.1">
<h4><span class="header-section-number">6.2.2.1</span> Confidence intervals<a href="inferential-statistics-take-1.html#confidence-intervals-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As a reminder, the general formula for a confidence interval is</p>
<p><span class="math display">\[\text{point est.} \pm (\text{crit. val.})\cdot SE  \]</span></p>
<p>so to calculate and interpret a confidence interval for two sample proportions, we simply need to figure out each of these three pieces. We’ll do so by example using the <code>cancer_in_dogs</code> data set from the <code>openintro</code> R package.</p>
<p>This data set is the result of a 1994 study from 1994 designed to determine if exposure to the herbicide herbicide 2,4-Dichlorophenoxyacetic acid (2,4-D) increases dogs’ risk of developing cancer. Let’s calculate a 95% confidence interval estimating the difference in cancer rates between dogs exposed to the herbicide and those who are not.</p>
<p>There are 491 dogs in the data set who were exposed and 945 dogs that were not. The data set has two columns: <code>order</code> and <code>response</code>; the first indicates whether or not the dogs were exposed to the herbicide and the second indicates whether or not the dog developed cancer. Below we display 4 randomly selected rows from the dataset.</p>
<div class="sourceCode" id="cb403"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb403-1"><a href="inferential-statistics-take-1.html#cb403-1" aria-hidden="true" tabindex="-1"></a><span class="fu">slice_sample</span>(cancer_in_dogs, <span class="at">n =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## # A tibble: 4 × 2
##   order    response 
##   &lt;fct&gt;    &lt;fct&gt;    
## 1 no 2,4-D no cancer
## 2 2,4-D    cancer   
## 3 2,4-D    cancer   
## 4 2,4-D    cancer</code></pre>
<p>Let <span class="math inline">\(p_1\)</span> denote the true proportion of dogs who are exposed to the herbicide and develop cancer; let <span class="math inline">\(p_2\)</span> denote the proportion of dogs who are not exposed but still develop cancer. We need to determine our sample proportions <span class="math inline">\(\hat{p}_1\)</span> and <span class="math inline">\(\hat{p}_2\)</span>. There are many ways to do this in R. A quick way is to use the <code>table(...)</code> function. Since the table only has two columns we can run the following to create a simple two-way table and store it in memory for future use.</p>
<div class="sourceCode" id="cb405"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb405-1"><a href="inferential-statistics-take-1.html#cb405-1" aria-hidden="true" tabindex="-1"></a>sumTab <span class="ot">&lt;-</span> <span class="fu">table</span>(cancer_in_dogs<span class="sc">$</span>response, cancer_in_dogs<span class="sc">$</span>order)</span>
<span id="cb405-2"><a href="inferential-statistics-take-1.html#cb405-2" aria-hidden="true" tabindex="-1"></a>sumTab <span class="sc">%&gt;%</span> <span class="fu">kable</span>()</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">2,4-D</th>
<th align="right">no 2,4-D</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">cancer</td>
<td align="right">191</td>
<td align="right">300</td>
</tr>
<tr class="even">
<td align="left">no cancer</td>
<td align="right">304</td>
<td align="right">641</td>
</tr>
</tbody>
</table>
<p>The columns in this two way table correspond to the samples of interest (ie the explanatory variable) and the rows the response variable. Thus, the denominator of our sample proportions will be the column sums and the numerators will be the entries in the <code>cancer</code> row.</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="inferential-statistics-take-1.html#cb406-1" aria-hidden="true" tabindex="-1"></a>n1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(sumTab[,<span class="dv">1</span>])</span>
<span id="cb406-2"><a href="inferential-statistics-take-1.html#cb406-2" aria-hidden="true" tabindex="-1"></a>n2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(sumTab[,<span class="dv">2</span>])</span>
<span id="cb406-3"><a href="inferential-statistics-take-1.html#cb406-3" aria-hidden="true" tabindex="-1"></a>pHat1 <span class="ot">&lt;-</span> sumTab[<span class="dv">1</span>,<span class="dv">1</span>] <span class="sc">/</span> n1</span>
<span id="cb406-4"><a href="inferential-statistics-take-1.html#cb406-4" aria-hidden="true" tabindex="-1"></a>pHat2 <span class="ot">&lt;-</span> sumTab[<span class="dv">1</span>,<span class="dv">2</span>] <span class="sc">/</span> n2 </span>
<span id="cb406-5"><a href="inferential-statistics-take-1.html#cb406-5" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(pHat1, pHat2)</span></code></pre></div>
<pre><code>## [1] 0.3858586 0.3188098</code></pre>
<p>When we calculate a confidence interval for two sample proportions, we’re estimating the <em>difference between the two proportions</em>, so our point estimate is</p>
<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb408-1"><a href="inferential-statistics-take-1.html#cb408-1" aria-hidden="true" tabindex="-1"></a>pDiff <span class="ot">&lt;-</span> pHat1 <span class="sc">-</span> pHat2</span>
<span id="cb408-2"><a href="inferential-statistics-take-1.html#cb408-2" aria-hidden="true" tabindex="-1"></a>pDiff</span></code></pre></div>
<pre><code>## [1] 0.06704881</code></pre>
<p>This means that roughly 6.7% <em>more</em> dogs exposed to the herbicide developed cancer in the sample data. Before we calculate a confidence interval, we need to make sure our samples meet the conditions needed to apply to Central Limit theorem.</p>
<ul>
<li><p>Independence within samples and between samples. The study employed random sampling for both groups and each sample size is less than 10% of its corresponding population. Moreover, the samples were taken independently of one another. We’re thus good to go here.</p></li>
<li><p>Sample size requirements. From the two-way table <code>sumTab</code> above we see that we have more than 10 successes (cancer) and failures (no cancer) in each group of dogs. Good to go here too!</p></li>
</ul>
<p>The CLT applies, so the sampling distribution of <span class="math inline">\(\hat{p}_1 - \hat{p}_2\)</span> is approximately normally distributed. Moreover, the standard error is</p>
<p><span class="math display">\[ SE \approx \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1}+ \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}} \]</span></p>
<p>which we can now easily calculate!</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="inferential-statistics-take-1.html#cb410-1" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(pHat1<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span> pHat1)<span class="sc">/</span>n1 <span class="sc">+</span>  pHat2<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span> pHat2)<span class="sc">/</span>n2)</span>
<span id="cb410-2"><a href="inferential-statistics-take-1.html#cb410-2" aria-hidden="true" tabindex="-1"></a>SE</span></code></pre></div>
<pre><code>## [1] 0.02663677</code></pre>
<p>This means that if we repeated the study over and over we should expect about changes of about 2.7% between the various differences in sample proportions. We’re finally ready to calculate our confidence interval! Since the sampling distribution is approximately normal, our the critical value for our 95% confidence interval is</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="inferential-statistics-take-1.html#cb412-1" aria-hidden="true" tabindex="-1"></a>zCrit <span class="ot">&lt;-</span> <span class="fu">qnorm</span>( (<span class="dv">1</span><span class="fl">-.95</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="at">lower.tail =</span> F)</span></code></pre></div>
<p>and our confidence interval is</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="inferential-statistics-take-1.html#cb413-1" aria-hidden="true" tabindex="-1"></a>ciDiff <span class="ot">&lt;-</span> pDiff <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>)<span class="sc">*</span>zCrit<span class="sc">*</span>SE</span>
<span id="cb413-2"><a href="inferential-statistics-take-1.html#cb413-2" aria-hidden="true" tabindex="-1"></a>ciDiff</span></code></pre></div>
<pre><code>## [1] 0.01484171 0.11925591</code></pre>
<p>How should we interpret this confidence interval? Remember, we’re estimating the difference in cancer rates between dogs who were exposed to the herbicide and those who were not. Thus:</p>
<p>With 95% confidence, the cancer rate of dogs who are exposed to 2,4-Dichlorophenoxyacetic acid is between 1.5% and 11.9% <em>higher</em> than dogs who were not exposed to the herbicide.</p>
<p>The next section will go through hypothesis testing using the same data.</p>
</div>
<div id="hypothesis-testing-1" class="section level4 hasAnchor" number="6.2.2.2">
<h4><span class="header-section-number">6.2.2.2</span> Hypothesis testing<a href="inferential-statistics-take-1.html#hypothesis-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Out in the wild you may want to simply check for a difference between two population proportions. The statistical inference for this is, of course, a hypothesis test. When you’re investigating a question like this your null and research hypotheses are</p>
<p><span class="math display">\[
\begin{array}{cl}
H_0: &amp; p_1 - p_2 = 0 \\
H_a: &amp; p_1 - p_2 \neq 0.
\end{array}
\]</span></p>
<p>The alternative hypothesis can by one-sided if you’d like; if it were <span class="math inline">\(p_1 - p_2 &gt; 0\)</span> you’d be asking if the first proportion is greater than the second and the opposite if it were <span class="math inline">\(p_1 - p_2 &lt; 0\)</span>. You’d could use a one sided hypothesis test to answer a question like “are there proportionally more Republicans in Montana than in Wyoming?”</p>
<p>As usual the CLT is the main tool for performing a 2-sample proportion hypothesis test. In this case we need to make sure that</p>
<ul>
<li><p>our observations are independent within and between groups</p></li>
<li><p>each sample has at least 10 <em>expected</em> success and failures.</p></li>
</ul>
<p>There’s a small difficulty we need to deal with here. Recall that we <strong>always assume the null hypothesis is true when performing a hypothesis test.</strong> In this case that only means <span class="math inline">\(p_1 = p_2\)</span>, but it doesn’t make any claim about what these proportions are actually equal to. The remedy is to estimate this value using a <em>pooled proportion</em>. Suppose your first and second samples have <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> observations respectively and <span class="math inline">\(suc_1\)</span> and <span class="math inline">\(suc_2\)</span> successes. The <em>pooled sample proportion</em> is</p>
<p><span class="math display">\[
\begin{array}{ccc}
\hat{p}_{pool} &amp; = &amp; \frac{suc_1 + suc_2}{n_1 + n_2} \\
&amp; = &amp; \frac{ \hat{p}_1 n_1 + \hat{p}_2 n_2}{n_1 + n_2}
\end{array}
\]</span>
or, in words, the pooled proportion is the total number of successes in both samples divided by the total number of observations. We use <span class="math inline">\(\hat{p}_{pool}\)</span> to assess the sample size requirements and in the formula for the standard error if our samples meet the conditions to apply the CLT. Let’s see how this plays out using the herbicide and cancer in dogs example from the last section. A two-way table summarizing the results of the experiment is below.</p>
<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb415-1"><a href="inferential-statistics-take-1.html#cb415-1" aria-hidden="true" tabindex="-1"></a>sumTab <span class="sc">%&gt;%</span> <span class="fu">kable</span>()</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">2,4-D</th>
<th align="right">no 2,4-D</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">cancer</td>
<td align="right">191</td>
<td align="right">300</td>
</tr>
<tr class="even">
<td align="left">no cancer</td>
<td align="right">304</td>
<td align="right">641</td>
</tr>
</tbody>
</table>
<p>The pooled proportion is the sum of the first row of this table divided by the sum of all entries in this table.</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="inferential-statistics-take-1.html#cb416-1" aria-hidden="true" tabindex="-1"></a>pPool <span class="ot">&lt;-</span> <span class="fu">sum</span>(sumTab[<span class="dv">1</span>,])<span class="sc">/</span><span class="fu">sum</span>(sumTab)</span>
<span id="cb416-2"><a href="inferential-statistics-take-1.html#cb416-2" aria-hidden="true" tabindex="-1"></a>pPool </span></code></pre></div>
<pre><code>## [1] 0.341922</code></pre>
<p>In other words, if we ignored herbicide exposure about 34% of the dogs in our sample developed cancer. We use this proportion to ensure that our samples are both large enough to apply the central limit theorem.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="inferential-statistics-take-1.html#cb418-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>( n1<span class="sc">*</span>pPool, n1<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>pPool))</span></code></pre></div>
<pre><code>## [1] 169.2514 325.7486</code></pre>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="inferential-statistics-take-1.html#cb420-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>( n2<span class="sc">*</span>pPool, n2<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>pPool))</span></code></pre></div>
<pre><code>## [1] 321.7486 619.2514</code></pre>
<p>Thus both of our samples have at least 10 expected success and expected failures, so we can apply the CLT! Remember we’re assuming that the null hypothesis is true and in turn that <span class="math display">\[p_1 = p_2 \approx \hat{p}_{pool}\]</span>. Thus</p>
<p><span class="math display">\[
\begin{array}{ccc}
SE &amp; = &amp; \sqrt{\frac{p_1(1-p_1)}{n_1}+ \frac{p_2(1-p_2)}{n_2}} \\ &amp; \approx &amp; \sqrt{\frac{\hat{p}_{pool}(1-\hat{p}_{pool})}{n_1} + \frac{\hat{p}_{pool}(1-\hat{p}_{pool})}{n_2}}
\end{array}
\]</span></p>
<p>The calculation for our current working example is below.</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="inferential-statistics-take-1.html#cb422-1" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>( pPool<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>pPool)<span class="sc">/</span>n1 <span class="sc">+</span> pPool<span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span>pPool)<span class="sc">/</span>n2) </span>
<span id="cb422-2"><a href="inferential-statistics-take-1.html#cb422-2" aria-hidden="true" tabindex="-1"></a>SE</span></code></pre></div>
<pre><code>## [1] 0.02633795</code></pre>
<p>We’re finally ready to calculate the <span class="math inline">\(p\)</span>-value for this experiment. Let’s perform this hypothesis test with a significance level of <span class="math inline">\(\alpha = .05\)</span>.</p>
<p>Our point estimate of <span class="math inline">\(p_1 - p2\)</span> is <code>pDiff</code> below.</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="inferential-statistics-take-1.html#cb424-1" aria-hidden="true" tabindex="-1"></a>pHat1 <span class="ot">&lt;-</span> sumTab[<span class="dv">1</span>,<span class="dv">1</span>] <span class="sc">/</span> n1</span>
<span id="cb424-2"><a href="inferential-statistics-take-1.html#cb424-2" aria-hidden="true" tabindex="-1"></a>pHat2 <span class="ot">&lt;-</span> sumTab[<span class="dv">1</span>,<span class="dv">2</span>] <span class="sc">/</span> n2 </span>
<span id="cb424-3"><a href="inferential-statistics-take-1.html#cb424-3" aria-hidden="true" tabindex="-1"></a>pDiff <span class="ot">&lt;-</span> pHat1 <span class="sc">-</span> pHat2</span>
<span id="cb424-4"><a href="inferential-statistics-take-1.html#cb424-4" aria-hidden="true" tabindex="-1"></a>pDiff</span></code></pre></div>
<pre><code>## [1] 0.06704881</code></pre>
<p>The test statistic, which in this case is a <span class="math inline">\(Z\)</span>-score, of our data is</p>
<p><span class="math display">\[
\begin{array}{ccc}
Z &amp; = &amp;  \frac{\text{point est.} - \text{null value}}{SE} \\
&amp; = &amp; \frac{ \hat{p}_1 - \hat{p}_2}{SE}
\end{array}
\]</span></p>
<p>or as follows in R.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="inferential-statistics-take-1.html#cb426-1" aria-hidden="true" tabindex="-1"></a>Z <span class="ot">&lt;-</span> pDiff <span class="sc">/</span> SE </span>
<span id="cb426-2"><a href="inferential-statistics-take-1.html#cb426-2" aria-hidden="true" tabindex="-1"></a>Z</span></code></pre></div>
<pre><code>## [1] 2.545711</code></pre>
<p>Since <span class="math inline">\(Z &gt;0\)</span> the <span class="math inline">\(p\)</span>-value of our hypothesis test is</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="inferential-statistics-take-1.html#cb428-1" aria-hidden="true" tabindex="-1"></a>pVal <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">pnorm</span>(Z,<span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb428-2"><a href="inferential-statistics-take-1.html#cb428-2" aria-hidden="true" tabindex="-1"></a>pVal</span></code></pre></div>
<pre><code>## [1] 0.01090555</code></pre>
<p>In other words, if there were no difference between cancer rates in dogs exposed to the herbicide and those who were not there would be a 1.1% chance of observing data like our own or more extreme.</p>
<p>Since <span class="math inline">\(p &lt; \alpha = .05\)</span>, we reject the null and accept the research: The data provide compelling evidence to believe that there the cancer rate of dogs exposed to the herbicide is different from the rate of dogs who were not exposed.</p>
<p>Note that this may have been a good candidate for a one-sided hypothesis test since we’re not simply interested in detecting a difference. We may be more interested in seeing if dogs exposed to the herbicide developed cancer at greater rates than those who were not exposed. In this situation, it is common to perform a two-sided hypothesis test, but also report a confidence interval to indicate which side of the null value the true difference falls on if your tests results in a rejection of the null.</p>
</div>
</div>
<div id="chi-squared-tests" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Chi-squared tests<a href="inferential-statistics-take-1.html#chi-squared-tests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can get a lot of mileage making inferences about categorical variables using single and two sample proportion tests and confidence intervals, but they don’t do everything we need. A single proportion may not always capture the subtleties we’re interested in. For example, if you want to the test whether or not the outcomes of a die toss really are equally likely, it isn’t enough to check if a 1 occurs 1/6 of the time using a single sample proportion test because say 4’s could be less likely and 6’s more likely. Similarly, when we’re testing for an association between two categorical variables a 2-sample proportion test may miss some differences or subtleties we’re interested in. For instance, if you’re testing for an association between smoking and socio-economic status, it is possible that lower and upper class individuals smoke at the same rate, but that middle class people smoke at a different rate. We thus need to develop a tool that can capture these differences that our previous techniques miss. This is where chi-squared (or <span class="math inline">\(\chi^2\)</span>) tests come in.</p>
<p>These tests are different from our the previous ones because you analyze <em>counts</em> instead of <em>proportions</em>. Counts are summarized in either <strong>one-way or two-way tables</strong>; one-way tables display the counts of the values a single categorical variable in a sample while two-way tables display the counts of the values of two categorical variables.</p>
<p>The data set <code>sp500_seq</code> from the <code>openintro</code> package is a summary of the all S&amp;P500 trading day outcomes from 1950 to 2018. To get to the data set in question, first label each day as a down day or an up day, then count the number of days between two up days. The outcomes in this data set are 1, 2, 3, …, 7+ and we treat these values as an ordered categorical variables because of the 7+. Below we see a <strong>one-way table</strong> for the counts of lag times between up days. We will analyze this data set in the next section, but we will use it to test to see if the direction of the S&amp;P500 on one day is independent from its direction on the previous day.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="inferential-statistics-take-1.html#cb430-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(sp500_seq<span class="sc">$</span>race)</span></code></pre></div>
<pre><code>## 
##    1    2    3    4    5    6   7+ 
## 1532  760  338  194   74   33   17</code></pre>
<p>For an example of a two-way table, we’ll use the <code>smoking</code> data set from the <code>openintro</code> package. This data set presents some survey data on smoking habits in the UK. You might wonder if there’s an association between marital status and smoking in the UK. This data set could provide some insight on this question and you could attempt to answer it by analyzing the following <strong>two-way table</strong>.</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="inferential-statistics-take-1.html#cb432-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(smoking<span class="sc">$</span>smoke, smoking<span class="sc">$</span>marital_status)</span></code></pre></div>
<pre><code>##      
##       Divorced Married Separated Single Widowed
##   No       103     669        46    269     183
##   Yes       58     143        22    158      40</code></pre>
<p>The basic idea behind all chi-squared tests is that under the null hypothesis each cell of your tabular data has an expected count; the goal is then is to measure the deviation between the <em>observed</em> counts and the <em>expected</em> counts. Moreover, we want a single test statistic that summarizes this.</p>
<p>You might start by simply looking at the difference between the observed <span class="math inline">\(O\)</span> and expected <span class="math inline">\(E\)</span> counts, then adding these differences up. Here we run into a dilemma that we’ve encountered before: positive and negative differences will possibly cancel each other out and make it look like there are no differences when in fact the differences are extreme. Instead, we square the difference between the observed and expected counts: <span class="math inline">\((O - E)^2\)</span>.</p>
<p>We still have a small dilemma. Suppose you have two cells with observed counts 11 and 101. The expected counts for these cells are 10 and 100 respectively; <span class="math inline">\((O- E)^2 = 1\)</span> for both of these cells, but the first observed count is 10% different from the expected while the second is only 1% different from the expected. We thus need to measure the squared deviation from the expected counts <em>relative to the expected counts</em> in each cell. For each cell in our summary table, we calculate</p>
<p><span class="math display">\[ \frac{(O-E)^2}{E}\]</span></p>
<p>This value ends up being a squared <span class="math inline">\(Z\)</span>-score! The single test statistic we calculate from our sample is the sum of these squared <span class="math inline">\(Z\)</span>-scores and is called a <span class="math inline">\(\chi^2\)</span>-test statistic (or chi-squared test statistic).</p>
<p><span class="math display">\[ \chi^2 = \sum_{all \, cells \\ in \, table } \frac{(O-E)^2}{E}\]</span></p>
<p>Since the test statistics are sums of squared <span class="math inline">\(Z\)</span>-scores, the corresponding sampling distribution is a sum of squared standard normal distributions. The number of squared normal distributions is a parameter called <strong>degrees of freedom</strong>, <span class="math inline">\(df\)</span>. Thus you completely determine a chi-squared distribution with <span class="math inline">\(df\)</span>.</p>
<p>Once you’ve calculated your <span class="math inline">\(\chi^2\)</span>-test statistic and have determined <span class="math inline">\(df\)</span>, you can calculate a <span class="math inline">\(p\)</span>-value and execute your hypothesis test. For any chi-squared test, “more favorable for the alternative hypothesis” will always mean roughly more deviation from the expected counts, so <span class="math inline">\(p\)</span>-values are always upper tail areas. For example, suppose you’re performing some hypothesis test and find</p>
<p><span class="math display">\[
\begin{array}{ccl}
df &amp; = &amp; 5 \text{ and } \\
\chi^2 &amp; = &amp; 6.2.
\end{array}
\]</span></p>
<p>The image below shows a plot of the chi-squared distribtuion with <span class="math inline">\(df = 5\)</span> and shades the <span class="math inline">\(p\)</span>-value of our imaginary hypothesis test.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="inferential-statistics-take-1.html#cb434-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">18</span>)), <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span> </span>
<span id="cb434-2"><a href="inferential-statistics-take-1.html#cb434-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_function</span>(<span class="at">fun =</span> dchisq,</span>
<span id="cb434-3"><a href="inferential-statistics-take-1.html#cb434-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> <span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb434-4"><a href="inferential-statistics-take-1.html#cb434-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dchisq,</span>
<span id="cb434-5"><a href="inferential-statistics-take-1.html#cb434-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> <span class="dv">5</span>),</span>
<span id="cb434-6"><a href="inferential-statistics-take-1.html#cb434-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">geom =</span> <span class="st">&quot;area&quot;</span>,</span>
<span id="cb434-7"><a href="inferential-statistics-take-1.html#cb434-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">xlim =</span> <span class="fu">c</span>(<span class="fl">6.2</span>, <span class="dv">18</span>),</span>
<span id="cb434-8"><a href="inferential-statistics-take-1.html#cb434-8" aria-hidden="true" tabindex="-1"></a>                <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb434-9"><a href="inferential-statistics-take-1.html#cb434-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">x =</span> <span class="st">&quot;chi-squared&quot;</span>, <span class="at">y =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb434-10"><a href="inferential-statistics-take-1.html#cb434-10" aria-hidden="true" tabindex="-1"></a>        <span class="at">title =</span> <span class="st">&quot;Chi-squared distribution with df = 5&quot;</span>,</span>
<span id="cb434-11"><a href="inferential-statistics-take-1.html#cb434-11" aria-hidden="true" tabindex="-1"></a>        <span class="at">subtitle =</span> <span class="st">&quot;p-value for chi-squared = 6.2 shaded&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-239-1.png" width="672" /></p>
<p>From this we see that we can calculate <span class="math inline">\(p\)</span>-values for chi-squared tests using the function <code>pchisq(test_stat, df = df, lower.tail = FALSE)</code>. For example, the <span class="math inline">\(p\)</span>-value for our imaginary hypothesis test would be</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="inferential-statistics-take-1.html#cb435-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(<span class="fl">6.2</span>, <span class="at">df =</span> <span class="dv">5</span>, <span class="at">lower.tail=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.2872417</code></pre>
<p>For more information about R’s functions for chi-squared distributions, see Chapter 4’s section on these distributions.</p>
<p>This section summarized the basics of all chi-squared tests from a big picture. From this summary note that we essentially only need to figure out two things.</p>
<ol style="list-style-type: decimal">
<li><p>How does one determine the expected counts for each cell of the table?</p></li>
<li><p>What are the appropriate numbers of degrees of freedom for the hypothesis test?</p></li>
</ol>
<p>The answers to these questions depends on the type of chi-squared test you’re performing. The next sections provide these answers for the two types of chi-squared tests.</p>
<div id="chi-squared-goodness-of-fit" class="section level4 hasAnchor" number="6.2.3.1">
<h4><span class="header-section-number">6.2.3.1</span> Chi-squared goodness of fit<a href="inferential-statistics-take-1.html#chi-squared-goodness-of-fit" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>These hypothesis tests are used to see if your sample data deviates from its expected distribution. For example, we expect the outcomes of a single die to be equally likely and could use a chi-squared goodness of fit test to check this.</p>
<p>The null and alternative hypothesis for a goodness of fit test are</p>
<p><span class="math display">\[
\begin{array}{cl}
H_0 &amp; \text{the data follow the expected distribution} H_a &amp;  \text{the data do not follow the expected distribution}
\end{array}
\]</span></p>
<p>and rejecting the hypothesis implies that the population distribution differs from the expected distribution.No surprise, the expected distribution determines the expected counts for each cell in your table. For example, if you toss a single die 120 times and record the outcome, you’d expect to get <span class="math inline">\(120/6 = 20\)</span> of each number 1-6.</p>
<p>To perform these hypothesis tests we need to know the number of degrees of freedom. You can think of this as the minimum number of cell counts in your table you need to completely determine the remaining cells. For example, if you toss a die 120 times and know the counts of the outcomes 1 through 5, you can figure out how many 6’s you got (120 minus the sum of 1-5 counts). But if you removed any of the other counts, you wouldn’t have enough information to complete the table. Thus you have 5 degrees of freedom in the die toss example. For one-way tables and goodness of fit tests</p>
<p><span class="math display">\[ df = k -1\]</span></p>
<p>where <span class="math inline">\(k\)</span> is the number of cells in your table.</p>
<p>Determining the expected counts can be slightly more involved than the die toss example. Let’s return to the S&amp;P500 example mentioned in the previous section. We want to test to see if the direction of the stock market on one day is independent from the direction of the stock market on the previous day. If we assume that the directions are independent, then the probability of the market going up on any day is the same. This might be tough to test on its own, but notice that we can count the number of days between two consecutive up days. For instance, “up up” would give a count of one, “up down up” a count of two, etc. The table below tallies all such runs from 1990 to 2011.</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="inferential-statistics-take-1.html#cb437-1" aria-hidden="true" tabindex="-1"></a>spTable <span class="ot">&lt;-</span> <span class="fu">table</span>(sp500_seq<span class="sc">$</span>race)</span>
<span id="cb437-2"><a href="inferential-statistics-take-1.html#cb437-2" aria-hidden="true" tabindex="-1"></a>spTable</span></code></pre></div>
<pre><code>## 
##    1    2    3    4    5    6   7+ 
## 1532  760  338  194   74   33   17</code></pre>
<p>If the daily directions are independent, what would the table above look like? The number of days between consecutive “up” days should follow a <strong>geometric distribution</strong>! To determine a geometric distribution, we need to know the probability of a success, which is the probability that the S&amp;P500 goes up on a given day. We can estimate this probability using the following code chunk looks at the raw S&amp;P500 data from 1990 to 2011.</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="inferential-statistics-take-1.html#cb439-1" aria-hidden="true" tabindex="-1"></a>sp500_1950_2018 <span class="sc">%&gt;%</span> </span>
<span id="cb439-2"><a href="inferential-statistics-take-1.html#cb439-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>( <span class="fu">as.character</span>(Date) <span class="sc">&gt;=</span> <span class="st">&#39;1990&#39;</span> <span class="sc">&amp;</span> </span>
<span id="cb439-3"><a href="inferential-statistics-take-1.html#cb439-3" aria-hidden="true" tabindex="-1"></a>            <span class="fu">as.character</span>(Date) <span class="sc">&lt;=</span> <span class="st">&#39;2012&#39;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb439-4"><a href="inferential-statistics-take-1.html#cb439-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">dir =</span> <span class="fu">if_else</span>( Close <span class="sc">&gt;</span> Open, <span class="cn">TRUE</span>, <span class="cn">FALSE</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb439-5"><a href="inferential-statistics-take-1.html#cb439-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>( <span class="at">upProb =</span> <span class="fu">sum</span>(dir)<span class="sc">/</span><span class="fu">length</span>(dir))</span></code></pre></div>
<pre><code>## # A tibble: 1 × 1
##   upProb
##    &lt;dbl&gt;
## 1  0.531</code></pre>
<p>So the probability that the S&amp;P500 goes up on a given day is rough 53.1%. Using this, we can generate a table of expected counts.</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="inferential-statistics-take-1.html#cb441-1" aria-hidden="true" tabindex="-1"></a><span class="co"># total number of up-to-up runs</span></span>
<span id="cb441-2"><a href="inferential-statistics-take-1.html#cb441-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span><span class="fu">sum</span>(spTable)</span>
<span id="cb441-3"><a href="inferential-statistics-take-1.html#cb441-3" aria-hidden="true" tabindex="-1"></a><span class="co">#list of factors</span></span>
<span id="cb441-4"><a href="inferential-statistics-take-1.html#cb441-4" aria-hidden="true" tabindex="-1"></a>days <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="st">&quot;7+&quot;</span>)</span>
<span id="cb441-5"><a href="inferential-statistics-take-1.html#cb441-5" aria-hidden="true" tabindex="-1"></a><span class="co">#probability/proportion of each factor level</span></span>
<span id="cb441-6"><a href="inferential-statistics-take-1.html#cb441-6" aria-hidden="true" tabindex="-1"></a>probs <span class="ot">&lt;-</span> <span class="fu">c</span>( <span class="fu">dgeom</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">5</span>,.<span class="dv">531</span>), <span class="fu">pgeom</span>(<span class="dv">5</span>, .<span class="dv">531</span>, <span class="at">lower.tail =</span> F))</span>
<span id="cb441-7"><a href="inferential-statistics-take-1.html#cb441-7" aria-hidden="true" tabindex="-1"></a><span class="co"># expected counts </span></span>
<span id="cb441-8"><a href="inferential-statistics-take-1.html#cb441-8" aria-hidden="true" tabindex="-1"></a>expected <span class="ot">&lt;-</span> n<span class="sc">*</span>probs</span>
<span id="cb441-9"><a href="inferential-statistics-take-1.html#cb441-9" aria-hidden="true" tabindex="-1"></a>expected</span></code></pre></div>
<pre><code>## [1] 1565.38800  734.16697  344.32431  161.48810   75.73792   35.52108   31.37361</code></pre>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="inferential-statistics-take-1.html#cb443-1" aria-hidden="true" tabindex="-1"></a><span class="co"># display all of the above in a horizontally organized table</span></span>
<span id="cb443-2"><a href="inferential-statistics-take-1.html#cb443-2" aria-hidden="true" tabindex="-1"></a><span class="fu">t</span>( <span class="fu">tibble</span>(days, <span class="at">prob =</span> <span class="fu">round</span>(probs, <span class="dv">3</span>), <span class="at">expected =</span> <span class="fu">round</span>(expected, <span class="dv">3</span>))) <span class="sc">%&gt;%</span> <span class="fu">kable</span>()</span></code></pre></div>
<table>
<tbody>
<tr class="odd">
<td align="left">days</td>
<td align="left">1</td>
<td align="left">2</td>
<td align="left">3</td>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">6</td>
<td align="left">7+</td>
</tr>
<tr class="even">
<td align="left">prob</td>
<td align="left">0.531</td>
<td align="left">0.249</td>
<td align="left">0.117</td>
<td align="left">0.055</td>
<td align="left">0.026</td>
<td align="left">0.012</td>
<td align="left">0.011</td>
</tr>
<tr class="odd">
<td align="left">expected</td>
<td align="left">1565.388</td>
<td align="left">734.167</td>
<td align="left">344.324</td>
<td align="left">161.488</td>
<td align="left">75.738</td>
<td align="left">35.521</td>
<td align="left">31.374</td>
</tr>
</tbody>
</table>
<p>We’re finally ready to calculate our test statistic and execute a hypothesis test. Let’s use a significance level of <span class="math inline">\(\alpha = .05\)</span>. We’re specifically testing</p>
<p><span class="math display">\[
\begin{array}{cl}
H_0: &amp; \text{the number of days between consecutive &quot;up&quot; days} \\
&amp; \text{on the S&amp;P500 follows a geometric distribution} \\
H_a: &amp; \text{the count does not follow a geometric distribution.}
\end{array}
\]</span></p>
<p>Note that our table has 7 cells in it, so</p>
<p><span class="math display">\[ df = 7 - 1 = 6. \]</span></p>
<p>We will calculate our test statistic in a few steps. Recall that it is</p>
<p><span class="math display">\[ \chi^2 = \sum_{all \\cells} \frac{(O-E)^2}{E} \]</span></p>
<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb444-1"><a href="inferential-statistics-take-1.html#cb444-1" aria-hidden="true" tabindex="-1"></a><span class="co"># each individual squared z-score</span></span>
<span id="cb444-2"><a href="inferential-statistics-take-1.html#cb444-2" aria-hidden="true" tabindex="-1"></a>(spTable <span class="sc">-</span> expected)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>expected </span></code></pre></div>
<pre><code>## 
##          1          2          3          4          5          6         7+ 
## 0.71212923 0.90898305 0.11616053 6.54551974 0.03987916 0.17893220 6.58517561</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb446-1"><a href="inferential-statistics-take-1.html#cb446-1" aria-hidden="true" tabindex="-1"></a><span class="co"># test stat</span></span>
<span id="cb446-2"><a href="inferential-statistics-take-1.html#cb446-2" aria-hidden="true" tabindex="-1"></a>chi.sq <span class="ot">&lt;-</span> <span class="fu">sum</span>((spTable <span class="sc">-</span> expected)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>expected)</span>
<span id="cb446-3"><a href="inferential-statistics-take-1.html#cb446-3" aria-hidden="true" tabindex="-1"></a>chi.sq</span></code></pre></div>
<pre><code>## [1] 15.08678</code></pre>
<p>Now that we have our test statistic in hand we can calculate a <span class="math inline">\(p\)</span>-value.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="inferential-statistics-take-1.html#cb448-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(chi.sq, <span class="at">df =</span> <span class="dv">6</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.0195924</code></pre>
<p>Since our <span class="math inline">\(p\)</span>-value is less than .05, we reject the null and accept the alternative. In other words, the counts of runs of different lengths do <em>not</em> appear to follow a geometric distribution. This implies that the direction of the S&amp;P500 on one day <strong>depends</strong> on the direction of the previous day.</p>
<p>This seems like it could be useful information! But how could one use it? This is a harder question and for the most part beyond the scope of this course. For fun, let’s look at the z-scores of each cell instead of the squared z-scores.</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="inferential-statistics-take-1.html#cb450-1" aria-hidden="true" tabindex="-1"></a>(spTable <span class="sc">-</span> expected)<span class="sc">/</span><span class="fu">sqrt</span>(expected)</span></code></pre></div>
<pre><code>## 
##          1          2          3          4          5          6         7+ 
## -0.8438775  0.9534060 -0.3408233  2.5584213 -0.1996977 -0.4230038 -2.5661597</code></pre>
<p>The only two run lengths that seem particularly unusual are runs of length 4 and 7 or more. In particular there are more runs of length 4 than we’d expect and fewer runs of length seven or more. Does that mean you should bet on the market going up if there are three down days in a row? Not without more analysis! At a bare minimum you’d want to calculate a conditional probability: what’s the probability the market goes up after three down days?</p>
<p><strong>Note:</strong> This is, of course, far from anything related to actual financial advice! This example is merely to demonstrate one of the myriad of things you can investigate with goodness of fit tests.</p>
</div>
<div id="chi-squared-test-of-independence" class="section level4 hasAnchor" number="6.2.3.2">
<h4><span class="header-section-number">6.2.3.2</span> Chi-squared test of independence<a href="inferential-statistics-take-1.html#chi-squared-test-of-independence" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>These tests are used to check for an association between two categorical variables. Because of this you will likely see chi-squared tests of independence all over the place. For example, the results of clinical trials in medicine are often analyzed using chi-squared tests of independence since you have two groups of individuals (those in the treatment group and those in the placebo group) and there are often two or more outcomes (symptoms improve, symptoms don’t improve, etc.). Recall from above that one applies these tests to two-way tables. We’ll work through the example testing for an association between marital status and smoking in the United Kingdom (the data comes from the <code>smoking</code> data set from the <code>openintro</code> package). The observed counts from a survey are displayed below.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="inferential-statistics-take-1.html#cb452-1" aria-hidden="true" tabindex="-1"></a>observed <span class="ot">&lt;-</span> <span class="fu">table</span>(smoking<span class="sc">$</span>smoke, smoking<span class="sc">$</span>marital_status)</span>
<span id="cb452-2"><a href="inferential-statistics-take-1.html#cb452-2" aria-hidden="true" tabindex="-1"></a>observed </span></code></pre></div>
<pre><code>##      
##       Divorced Married Separated Single Widowed
##   No       103     669        46    269     183
##   Yes       58     143        22    158      40</code></pre>
<p>The null and research hypotheses for a test of independence are</p>
<p><span class="math display">\[
\begin{array}{cl}
H_0: &amp; \text{the variables are independent} \\
H_a: &amp;  \text{the variables are dependent}
\end{array}
\]</span></p>
<p>For our example, the null hypothesis is that smoking and marital status are not associated; the research hypothesis is that there is an association between smoking and marital status.</p>
<p>As with goodness of fit tests, we need to determine the <em>expected</em> counts for our two-way table assuming the null hypothesis is true. Recall from probability that if two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, then <span class="math inline">\(P(A \&amp; B) = P(A)P(B)\)</span> (<strong>caution! This is only true when the events are independent</strong>). Suppose event <span class="math inline">\(A\)</span> corresponds to row <span class="math inline">\(i\)</span> in your two way table and event <span class="math inline">\(B\)</span> to column <span class="math inline">\(j\)</span>. Let <span class="math inline">\(n\)</span> denote the sum of all entries in your table; <span class="math inline">\(n\)</span> is sometimes called the grand total. Then</p>
<p><span class="math display">\[
P(A) = \frac{\text{row i total }}{n} \quad \text{and} \quad  P(B) = \frac{\text{column j total }}{n}
\]</span></p>
<p>so if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent</p>
<p><span class="math display">\[ P( A \&amp; B ) = \frac{\text{row i total }}{n} \frac{\text{column j total }}{n} = \frac{\text{row i total}\cdot \text{column j total }}{n^2}  \]</span></p>
<p>Thus the expected count in the <span class="math inline">\((i,j)\)</span>-cell of our two-way table is</p>
<p><span class="math display">\[ n \cdot P( A \&amp; B ) =  \frac{\text{row i total}\cdot \text{column j total }}{n}  \]</span></p>
<p>To make this more concrete, let’s find the expected count for the <span class="math inline">\((1,1)\)</span>-cell in our <code>observed</code> two-way table above. This cell corresponds to divorced people who do not smoke. The code chunk below calculates the corresponding row and column totals along with the grand total.</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="inferential-statistics-take-1.html#cb454-1" aria-hidden="true" tabindex="-1"></a> n <span class="ot">&lt;-</span> <span class="fu">sum</span>(observed)</span>
<span id="cb454-2"><a href="inferential-statistics-take-1.html#cb454-2" aria-hidden="true" tabindex="-1"></a>row1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(observed[<span class="dv">1</span>,])</span>
<span id="cb454-3"><a href="inferential-statistics-take-1.html#cb454-3" aria-hidden="true" tabindex="-1"></a>column1 <span class="ot">&lt;-</span> <span class="fu">sum</span>(observed[,<span class="dv">1</span>])</span>
<span id="cb454-4"><a href="inferential-statistics-take-1.html#cb454-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(n, row1, column1)</span></code></pre></div>
<pre><code>## [1] 1691 1270  161</code></pre>
<p>So if the smoking and marital status are independent, we’d expect to see</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="inferential-statistics-take-1.html#cb456-1" aria-hidden="true" tabindex="-1"></a>exp11 <span class="ot">&lt;-</span> row1<span class="sc">*</span>column1<span class="sc">/</span>n</span>
<span id="cb456-2"><a href="inferential-statistics-take-1.html#cb456-2" aria-hidden="true" tabindex="-1"></a>exp11 </span></code></pre></div>
<pre><code>## [1] 120.9166</code></pre>
<p>divorced non-smokers in our survey results. The squared <span class="math inline">\(z\)</span>-score for this cell would then be</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="inferential-statistics-take-1.html#cb458-1" aria-hidden="true" tabindex="-1"></a>(observed[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">-</span> exp11)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>exp11</span></code></pre></div>
<pre><code>## [1] 2.654765</code></pre>
<p>Now let’s make a table of expected counts. In practice, you’d use one of <code>R</code>’s in-built functions to perform this test, but here we’re demonstrating what’s going on “beneath the hood.” We use two nested “for” loops to make the table of expected counts.</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="inferential-statistics-take-1.html#cb460-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define an matrix with the right dimensions of all zeroes</span></span>
<span id="cb460-2"><a href="inferential-statistics-take-1.html#cb460-2" aria-hidden="true" tabindex="-1"></a>expected <span class="ot">&lt;-</span> <span class="fu">matrix</span>( <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">prod</span>(<span class="fu">dim</span>(observed))),</span>
<span id="cb460-3"><a href="inferential-statistics-take-1.html#cb460-3" aria-hidden="true" tabindex="-1"></a>                    <span class="at">nrow =</span> <span class="fu">dim</span>(observed)[<span class="dv">1</span>],</span>
<span id="cb460-4"><a href="inferential-statistics-take-1.html#cb460-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">ncol =</span> <span class="fu">dim</span>(observed)[<span class="dv">2</span>])</span>
<span id="cb460-5"><a href="inferential-statistics-take-1.html#cb460-5" aria-hidden="true" tabindex="-1"></a><span class="co"># iterate over all rows</span></span>
<span id="cb460-6"><a href="inferential-statistics-take-1.html#cb460-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(observed)[<span class="dv">1</span>]){</span>
<span id="cb460-7"><a href="inferential-statistics-take-1.html#cb460-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">#iterage over all columns</span></span>
<span id="cb460-8"><a href="inferential-statistics-take-1.html#cb460-8" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">dim</span>(observed)[<span class="dv">2</span>]){</span>
<span id="cb460-9"><a href="inferential-statistics-take-1.html#cb460-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">#update i,j-th entry with expected count </span></span>
<span id="cb460-10"><a href="inferential-statistics-take-1.html#cb460-10" aria-hidden="true" tabindex="-1"></a>    expected[i,j] <span class="ot">&lt;-</span> <span class="fu">sum</span>(observed[i,])<span class="sc">*</span><span class="fu">sum</span>(observed[,j])<span class="sc">/</span><span class="fu">sum</span>(observed)</span>
<span id="cb460-11"><a href="inferential-statistics-take-1.html#cb460-11" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb460-12"><a href="inferential-statistics-take-1.html#cb460-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb460-13"><a href="inferential-statistics-take-1.html#cb460-13" aria-hidden="true" tabindex="-1"></a>expected</span></code></pre></div>
<pre><code>##           [,1]     [,2]     [,3]     [,4]      [,5]
## [1,] 120.91662 609.8403 51.07037 320.6919 167.48078
## [2,]  40.08338 202.1597 16.92963 106.3081  55.51922</code></pre>
<p>Is it a problem that none of the entries in the table above are integers? No! We should think of the expected counts as average outcomes.</p>
<p>We can now easily calculate our table of squared <span class="math inline">\(z\)</span>-scores and our chi-squared test statistic. After that, to perform out hypothesis test, we simply need to determine the appropriate number of degrees of freedom <span class="math inline">\(df\)</span>.</p>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb462-1"><a href="inferential-statistics-take-1.html#cb462-1" aria-hidden="true" tabindex="-1"></a>(observed <span class="sc">-</span> expected)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>expected</span></code></pre></div>
<pre><code>##      
##         Divorced    Married  Separated     Single    Widowed
##   No   2.6547648  5.7389881  0.5033971  8.3321480  1.4380526
##   Yes  8.0084354 17.3123870  1.5185614 25.1349832  4.3380684</code></pre>
<p>The chi-squared test statistic is the sum of the entries in the table above.</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="inferential-statistics-take-1.html#cb464-1" aria-hidden="true" tabindex="-1"></a>chi <span class="ot">&lt;-</span> <span class="fu">sum</span>((observed <span class="sc">-</span> expected)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>expected)</span>
<span id="cb464-2"><a href="inferential-statistics-take-1.html#cb464-2" aria-hidden="true" tabindex="-1"></a>chi</span></code></pre></div>
<pre><code>## [1] 74.97979</code></pre>
<p>To calculate a <span class="math inline">\(p\)</span>-value and perform this hypothesis test, we simply need to determine <span class="math inline">\(df\)</span>. As with goodness of fit tests, <span class="math inline">\(df\)</span> is the minimum number of cell counts in your table you need to completely determine the remaining cells. If <span class="math inline">\(R\)</span> and <span class="math inline">\(C\)</span> are the number of rows and columns in your two-way table, then</p>
<p><span class="math display">\[ df = (R-1)\times (C-1) \]</span></p>
<p>so for our example</p>
<p><span class="math display">\[ df = (2-1)\times (5 -1 ) = 4. \]</span><br />
</p>
<p>Thus the <span class="math inline">\(p\)</span>-value for our hypothesis test is</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="inferential-statistics-take-1.html#cb466-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pchisq</span>(chi, <span class="at">df =</span> <span class="dv">4</span>, <span class="at">lower.tail =</span> F)</span></code></pre></div>
<pre><code>## [1] 2.012302e-15</code></pre>
<p>This <span class="math inline">\(p\)</span>-value is smaller than all typical significance levels, so we reject the null and accept the research. In particular, the data provide compelling evidence to believe that there is an association between marital status and smoking. What might this association look like? Let’s look at the <span class="math inline">\(z\)</span>-scores of our table counts instead of the squared <span class="math inline">\(z\)</span>-scores.</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb468-1"><a href="inferential-statistics-take-1.html#cb468-1" aria-hidden="true" tabindex="-1"></a>(observed <span class="sc">-</span> expected)<span class="sc">/</span><span class="fu">sqrt</span>(expected)</span></code></pre></div>
<pre><code>##      
##         Divorced    Married  Separated     Single    Widowed
##   No  -1.6293449  2.3956185 -0.7095048 -2.8865460  1.1991883
##   Yes  2.8299179 -4.1608157  1.2322992  5.0134801 -2.0828030</code></pre>
<p>It appears that married people are way less likely to smoke and single people are way more likely to smoke than expected. As in the previous example for goodness of fit, we could do a more in-depth analysis to describe the association we detected.</p>
</div>
</div>
</div>
<div id="analyzing-numerical-variables" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Analyzing numerical variables<a href="inferential-statistics-take-1.html#analyzing-numerical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We’ve finished our discussion of analyzing categorical variables and now turn our attention to analyzing numerical variables. There are many different numerical variables one can analyze but we will focus on making inferences about population averages. We use the Greek letter mu <span class="math inline">\(\mu\)</span> to denote a population average and <span class="math inline">\(\overline{x}\)</span> to denote a sample average. Similarly, we use the Greek letter <span class="math inline">\(\sigma\)</span> to denote the population standard deviation and <span class="math inline">\(s\)</span> to denote sample standard deviations. We will use sample averages to make inferences about population averages, but in the process we will have to estimate <span class="math inline">\(\sigma\)</span> using <span class="math inline">\(s\)</span>. This creates a small dilemma that we’ll define and address below.</p>
<p>One distinction that becomes more important and sometimes difficult to make when analyzing numerical variables is the difference between <strong>population distributions</strong> and <strong>sampling distributions</strong>. This was an easy distinction to make when we were dealing with categorical variables, because the population distributions were discrete things (proportion of success and proportion of failure), but sampling distributions were continuous. When dealing with numerical variables, however, the population and sampling distributions are often both continuous.</p>
<p>As an example, suppose you are trying to investigate the average income of residents in Flathead County. You take a sample of 100 adult county residents. You sample <em>from</em> a population and then <em>use</em> the sampling distribution of <span class="math inline">\(\overline{x}\)</span> to make an inference about the county average population. The plot below helps us visualize the difference between these two distributions. The distributions below are made up and serve merely to illustrate the differences between the two distributions.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-256-1.png" width="672" /></p>
<p>The plots above demonstrate that population distributions can be skewed (or in practice have a totally unknown shape), but under fairly mild hypotheses, the <strong>sampling distribution of <span class="math inline">\(\overline{x}\)</span> </strong> will be symmetric and unimodal. This is the central limit theorem in action again. As with categorical variables, this will be the main driver of inference about numerical variables for us.</p>
<p><strong>Central Limit Theorem (for a single sample average)</strong>: Suppose we have <span class="math inline">\(n\)</span> independent observations of a numerical variable. If either</p>
<ul>
<li><p>the population we’re sampling from is approximately normal or</p></li>
<li><p><span class="math inline">\(n\)</span> is sufficiently large or</p></li>
</ul>
<p>then the sampling distribution of <span class="math inline">\(\overline{x}\)</span> is approximately normal with</p>
<p><span class="math display">\[ E(\overline{x}) = \mu \quad \text{and} \quad SE( \overline{x}) = \frac{\sigma}{\sqrt{n}}\]</span></p>
<p>No surprise, the standard error of the sampling distribution of <span class="math inline">\(\overline{x}\)</span> depends on the standard deviation of the population. Unfortunately, we almost never know <span class="math inline">\(\sigma\)</span> and almost never know if the population we’re sampling from is approximately normally distributed. For the latter dilemma we have a few rules of thumb to guide us.</p>
<p><strong>Normality conditions:</strong></p>
<ul>
<li><p><span class="math inline">\(n &lt;30\)</span> <strong>and</strong> there are no clear outliers in your data, it is common to <em>assume</em> your sample comes from an approximately normal population. Note that this is simply a rule of thumb! To check for outliers, use <span class="math inline">\(z\)</span>-scores and look for the maximum in absolute value.</p></li>
<li><p><span class="math inline">\(n \geq 30\)</span> and there are no extreme outliers in your data.</p></li>
</ul>
<p>If either of these conditions holds and your observations are independent, then the we <em>assume</em> our sample meets the criteria in order to apply the CLT as described above. There is, however, one more small obstacle: we almost never know <span class="math inline">\(\sigma\)</span>, the population standard deviation, which determines the standard error of <span class="math inline">\(\overline{x}\)</span>. We do what appears to be the most logical thing, and estimate <span class="math inline">\(\sigma\)</span> with <span class="math inline">\(s\)</span>, our sample standard deviation so that</p>
<p><span class="math display">\[ SE = \frac{\sigma}{\sqrt{m}} \approx \frac{s}{\sqrt{n}}\]</span></p>
<p>but (because of technical reasons that we’ll skip) it turns our that <span class="math inline">\(s\)</span> is a <em>biased</em> estimator of <span class="math inline">\(\sigma\)</span>. Moreover, it consistently underestimates <span class="math inline">\(sigma\)</span>. Thus, our estimate of <span class="math inline">\(SE\)</span> above is also an underestimate, which means that we should expect <em>more</em> variability among our sample statistics. This bias is especially pronounced when your sample size is small, and tends to zero as the sample size increases. The key to remedying this problem, at least for our purposes, is to model the sampling distribution of <span class="math inline">\(\overline{x}\)</span> using <span class="math inline">\(t\)</span>-distribution.</p>
<p>Recall from <a href="distribtuion-calculations.html">Chapter 4</a> that a <span class="math inline">\(t\)</span>-distribution is a symmetric and unimodal distribution determined by a single parameter, <span class="math inline">\(df\)</span>, or degrees of freedom. Each <span class="math inline">\(t\)</span>-distribution has “fatter tails” than the standard normal distribution, meaning that a higher proportion of observations will fall more than 2 standard deviations from the mean. As <span class="math inline">\(df\)</span> gets larger, <span class="math inline">\(t\)</span>-distributions approach the standard normal distribution, so the fat tails get skinnier. Notice that this helps solve the dilemma of using <span class="math inline">\(s\)</span> to estimate <span class="math inline">\(\sigma\)</span>!</p>
<p>In the next sections we’ll see how to use <span class="math inline">\(t\)</span>-distributions to make inferences about population averages. Everything in the following two sections should feel very similar from a big picture perspective because the overall strategy for statistical inference remains the same.</p>
<div id="single-sample-mean" class="section level3 hasAnchor" number="6.3.1">
<h3><span class="header-section-number">6.3.1</span> Single sample mean<a href="inferential-statistics-take-1.html#single-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin our exploration of numerical variables by learning how to make inferences about a single population average. We will see how to calculate and interpret confidence intervals, then proceed to hypothesis testing.</p>
<p>To illustrate the techniques, we will use two examples in what follows. One example will be based on the <code>faithful</code> data set that is loaded by default in R. It gives the duration of 272 eruptions at Old Faithful along with the wait time between eruptions. We will also look the <code>women</code> data set which gives the height and weight of 15 American women from 1975. For both examples we will need to verify that we can apply the Central Limit Theorem, so we need to to check the normality conditions listed above.</p>
<p><strong>faithful</strong>: We’ll investigate the wait time between eruptions, the <code>waiting</code> variable in the <code>faithful</code> data set. Since the number of observations <span class="math inline">\(n = 272\)</span>, we simply need to check that the data set has no extreme outliers. We can do this in two ways, visually and numerically. First, we’ll look at a histogram of the data.</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="inferential-statistics-take-1.html#cb470-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(faithful, <span class="fu">aes</span>(<span class="at">x =</span> waiting)) <span class="sc">+</span></span>
<span id="cb470-2"><a href="inferential-statistics-take-1.html#cb470-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">&quot;black&quot;</span></span>
<span id="cb470-3"><a href="inferential-statistics-take-1.html#cb470-3" aria-hidden="true" tabindex="-1"></a>                 , <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span></span>
<span id="cb470-4"><a href="inferential-statistics-take-1.html#cb470-4" aria-hidden="true" tabindex="-1"></a>                 , <span class="at">bins =</span> <span class="dv">20</span>) <span class="sc">+</span> </span>
<span id="cb470-5"><a href="inferential-statistics-take-1.html#cb470-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Wait time between Yellowstone eruptions&quot;</span></span>
<span id="cb470-6"><a href="inferential-statistics-take-1.html#cb470-6" aria-hidden="true" tabindex="-1"></a>        , <span class="at">subtitle =</span> <span class="st">&quot;n = 272&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-257-1.png" width="672" /></p>
<p>The wait time between eruptions appears to be bimodal, but this is no problem for us. From the histogram above, no observations seem particularly extreme. Let’s double check this with some calculations. We can look at a summary of the wait times:</p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="inferential-statistics-take-1.html#cb471-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(faithful<span class="sc">$</span>waiting)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    43.0    58.0    76.0    70.9    82.0    96.0</code></pre>
<p>or translate this summary into <span class="math inline">\(z\)</span>-scores</p>
<div class="sourceCode" id="cb473"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb473-1"><a href="inferential-statistics-take-1.html#cb473-1" aria-hidden="true" tabindex="-1"></a>(<span class="fu">summary</span>(faithful<span class="sc">$</span>waiting) <span class="sc">-</span> <span class="fu">mean</span>(faithful<span class="sc">$</span>waiting))<span class="sc">/</span><span class="fu">sd</span>(faithful<span class="sc">$</span>waiting)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -2.0520 -0.9487  0.3754  0.0000  0.8167  1.8465</code></pre>
<p>These calculations confirm our observation: since no measurement is more than roughly 2 standard deviations away from the mean, our sample contains no outliers.</p>
<p>We also need to check independence of observations here. Does the wait time from one eruption to another influence the wait time until the next eruption? Quite possibly, but without more information it is hard to say. To keep things simple, we point out that independence is difficult to verify in this case, but we <em>assume</em> the observations are independent.</p>
<p><strong>women</strong>: We’ll look at the <code>height</code> variable. In this case the number of observations is small as <span class="math inline">\(n = 15\)</span>. But not all is lost! If we can safely <em>assume</em> the population is normally distributed, we can apply the central limit theorem. As above, we first look at a histogram</p>
<div class="sourceCode" id="cb475"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb475-1"><a href="inferential-statistics-take-1.html#cb475-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(women, <span class="fu">aes</span>(<span class="at">x =</span> height)) <span class="sc">+</span></span>
<span id="cb475-2"><a href="inferential-statistics-take-1.html#cb475-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">color =</span> <span class="st">&quot;black&quot;</span></span>
<span id="cb475-3"><a href="inferential-statistics-take-1.html#cb475-3" aria-hidden="true" tabindex="-1"></a>                 , <span class="at">fill =</span> <span class="st">&quot;steelblue&quot;</span></span>
<span id="cb475-4"><a href="inferential-statistics-take-1.html#cb475-4" aria-hidden="true" tabindex="-1"></a>                 , <span class="at">bins =</span> <span class="dv">7</span>) <span class="sc">+</span> </span>
<span id="cb475-5"><a href="inferential-statistics-take-1.html#cb475-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>( <span class="at">title =</span> <span class="st">&quot;Height of American women in 1975&quot;</span></span>
<span id="cb475-6"><a href="inferential-statistics-take-1.html#cb475-6" aria-hidden="true" tabindex="-1"></a>        , <span class="at">subtitle =</span> <span class="st">&quot;n = 15&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-260-1.png" width="672" /></p>
<p>No surprise, the distribution of heights in this sample is fairly uniform and there are no clear outliers in the data. We’ll check this using a summary and <span class="math inline">\(z\)</span>-scores again.</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="inferential-statistics-take-1.html#cb476-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(women<span class="sc">$</span>height)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    58.0    61.5    65.0    65.0    68.5    72.0</code></pre>
<p>and <span class="math inline">\(z\)</span>-scores:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="inferential-statistics-take-1.html#cb478-1" aria-hidden="true" tabindex="-1"></a>(<span class="fu">summary</span>(women<span class="sc">$</span>height) <span class="sc">-</span> <span class="fu">mean</span>(women<span class="sc">$</span>height))<span class="sc">/</span><span class="fu">sd</span>(women<span class="sc">$</span>height)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
## -1.5652 -0.7826  0.0000  0.0000  0.7826  1.5652</code></pre>
<p>Since all observations are less than 2 standard deviations from the mean, there are no outliers whatsoever in this data set. In this case it is safe to <em>assume</em> that the population is normally distributed, which means that we can apply the central limit theorem to this example as well!</p>
<p>We will use the CLT to estimate the population averages and also make inferences about them. Before we can proceed, we need to establish one more fact. Recall from above that we will model the sampling distribution of <span class="math inline">\(\overline{x}\)</span> with a <span class="math inline">\(t\)</span>-distribution. Unlike normal distributions that are determined by a mean <span class="math inline">\(\mu\)</span> and a standard deviation <span class="math inline">\(\sigma\)</span>, a <span class="math inline">\(t\)</span>-distribution is determined by a single parameter, <strong>degrees of freedom</strong> <span class="math inline">\(df\)</span>.</p>
<p>If your sample has <span class="math inline">\(n\)</span> measurements, then the sampling distribution is best modeled by a <span class="math inline">\(t\)</span>-distribution with
<span class="math display">\[ df = n - 1\]</span></p>
<div id="confidence-intervals-3" class="section level4 hasAnchor" number="6.3.1.1">
<h4><span class="header-section-number">6.3.1.1</span> Confidence intervals<a href="inferential-statistics-take-1.html#confidence-intervals-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Confidence intervals for single sample means are calculated using the typical pattern:</p>
<p><span class="math display">\[ \text{point est.} \pm \text{critical value}\cdot SE \]</span></p>
<p>where the point estimate is the sample average <span class="math inline">\(\overline{x}\)</span> and the standard error <span class="math inline">\(SE\)</span> is given by the CLT <span class="math inline">\(SE = \sigma / \sqrt{n} \approx s / \sqrt{n}\)</span>. If your sample meets the criteria for applying the central limit theorem, the only new datum we need to calculate is the critical value. Since the sampling distribution of <span class="math inline">\(\overline{x}\)</span> is best modeled with a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = n-1\)</span>, we should use this distribution to find the critical value.</p>
<p>You can calculate these <span class="math inline">\(t\)</span>-critical values using the <code>R</code> function <code>qt</code> in the exact same way you found <span class="math inline">\(z\)</span>-critical values in the previous section! In particular, if calculating an <span class="math inline">\(\alpha\)</span>% confidence interval, the critical value is <span class="math inline">\(t^\star_{df} =\)</span> <code>qt( (1-alpha)/2, df = n-1, lower.tail = FALSE )</code> where <code>alpha</code> is the confidence level as a decimal rather than a percentage.</p>
<p>Let’s now calculate and interpret 95% confidence intervals for the examples discussed in the section above, <code>women</code> and <code>faithful</code>.</p>
<p><strong>faithful</strong>: We want to calculate a 95% confidence interval for the average wait time between eruptions at the Old Faithful geyser in Yellowstone (in minutes). To do so, we need four numbers: <span class="math inline">\(\overline{x}\)</span> the sample average, <span class="math inline">\(n\)</span> the sample size, <span class="math inline">\(s\)</span> the sample standard deviation, and <span class="math inline">\(t^\star_df\)</span> the <span class="math inline">\(t\)</span>-critical value. The code chunk below calculates all of these numbers along with the standard error, then prints the <span class="math inline">\(t\)</span>-critical value.</p>
<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb480-1"><a href="inferential-statistics-take-1.html#cb480-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(faithful<span class="sc">$</span>waiting)</span>
<span id="cb480-2"><a href="inferential-statistics-take-1.html#cb480-2" aria-hidden="true" tabindex="-1"></a>xBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(faithful<span class="sc">$</span>waiting)</span>
<span id="cb480-3"><a href="inferential-statistics-take-1.html#cb480-3" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(faithful<span class="sc">$</span>waiting)</span>
<span id="cb480-4"><a href="inferential-statistics-take-1.html#cb480-4" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> s <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb480-5"><a href="inferential-statistics-take-1.html#cb480-5" aria-hidden="true" tabindex="-1"></a>tStar <span class="ot">&lt;-</span> <span class="fu">qt</span>( (<span class="dv">1</span> <span class="sc">-</span> .<span class="dv">95</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="at">df =</span> n<span class="dv">-1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb480-6"><a href="inferential-statistics-take-1.html#cb480-6" aria-hidden="true" tabindex="-1"></a>tStar</span></code></pre></div>
<pre><code>## [1] 1.968756</code></pre>
<p>Notice that <span class="math inline">\(t^\star_df\)</span> is very close to the <span class="math inline">\(z\)</span>-critical value for a 95% confidence interval. This is because 272 is large, so the associated <span class="math inline">\(t\)</span>-distribution is very close to the standard normal curve.</p>
<p>Now we can combine the above into a confidence interval.</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="inferential-statistics-take-1.html#cb482-1" aria-hidden="true" tabindex="-1"></a>cI <span class="ot">&lt;-</span> xBar <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span>tStar<span class="sc">*</span>SE</span>
<span id="cb482-2"><a href="inferential-statistics-take-1.html#cb482-2" aria-hidden="true" tabindex="-1"></a>cI</span></code></pre></div>
<pre><code>## [1] 69.27418 72.51994</code></pre>
<p>Thus, with 95% confidence, the average wait time between two eruptions at Old Faithful is between roughly 69.3 and 72.5 minutes.</p>
<p><strong>women</strong>: We want to calculate a 95% confidence interval for the average height of American women in 1975. We need the same numbers as above to make this calculation and the code chunk below does exactly that.</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="inferential-statistics-take-1.html#cb484-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(women<span class="sc">$</span>height)</span>
<span id="cb484-2"><a href="inferential-statistics-take-1.html#cb484-2" aria-hidden="true" tabindex="-1"></a>xBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(women<span class="sc">$</span>height)</span>
<span id="cb484-3"><a href="inferential-statistics-take-1.html#cb484-3" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(women<span class="sc">$</span>height)</span>
<span id="cb484-4"><a href="inferential-statistics-take-1.html#cb484-4" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> s <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb484-5"><a href="inferential-statistics-take-1.html#cb484-5" aria-hidden="true" tabindex="-1"></a>tStar <span class="ot">&lt;-</span> <span class="fu">qt</span>( (<span class="dv">1</span> <span class="sc">-</span> .<span class="dv">95</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="at">df =</span> n<span class="dv">-1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb484-6"><a href="inferential-statistics-take-1.html#cb484-6" aria-hidden="true" tabindex="-1"></a>tStar</span></code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<p>This time around, notice that <span class="math inline">\(t^\star_df \approx 2.144\)</span> which is larger than the <span class="math inline">\(z\)</span>-critical value for a 95% confidence interval. This is because the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(14\)</span> degrees of freedom has fatter tails than the standard normal distribution. This is exactly what we want because we are more uncertain about our use of <span class="math inline">\(s\)</span>, the sample standard deviation, as an estimate of <span class="math inline">\(\sigma\)</span>, the population standard deviation.</p>
<p>Now, calculating the confidence interval:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="inferential-statistics-take-1.html#cb486-1" aria-hidden="true" tabindex="-1"></a>cI <span class="ot">&lt;-</span> xBar <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span>tStar<span class="sc">*</span>SE</span>
<span id="cb486-2"><a href="inferential-statistics-take-1.html#cb486-2" aria-hidden="true" tabindex="-1"></a>cI</span></code></pre></div>
<pre><code>## [1] 62.52341 67.47659</code></pre>
<p>Our calculations imply that the average height of American women in 1975 was between about 62.52 and 67.48 inches, with 95% confidence.</p>
</div>
<div id="hypothesis-testing-2" class="section level4 hasAnchor" number="6.3.1.2">
<h4><span class="header-section-number">6.3.1.2</span> Hypothesis testing<a href="inferential-statistics-take-1.html#hypothesis-testing-2" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Now that we have confidence intervals under our belt and understand the details of using <span class="math inline">\(t\)</span>-distributions for making inferences about means, we can move through hypothesis testing fairly quickly. When making inferences about an average, the null and research hypotheses are statements about the value of the true population mean.</p>
<p><span class="math display">\[ \begin{array}{cc}
H_0: &amp; \mu = \mu_0 \\
H_a: &amp; \mu \neq \mu_0 \text{ or } \mu &gt; \mu_0 \text{ or } \mu &lt; \mu_0
\end{array}
\]</span></p>
<p>Remember that one assumes the null hypothesis is true when performing a hypothesis test. Note: unlike analyzing proportions, the standard error depends only on your sample and not the null mean! This is because
<span class="math display">\[ SE = \frac{\sigma}{\sqrt{n}} \approx \frac{s}{\sqrt{n}} \]</span>
whenever we can apply the Central Limit Theorem.</p>
<p>To execute this hypothesis test, we perform the usual steps.</p>
<ol start="0" style="list-style-type: decimal">
<li><p>Set a significance level <span class="math inline">\(\alpha\)</span></p></li>
<li><p>Calculate all appropriate sample statistics. In this case the sample average <span class="math inline">\(\overline{x}\)</span> and standard deviation <span class="math inline">\(s\)</span>.</p></li>
<li><p>Calculate the standard error <span class="math inline">\(SE\)</span>.</p></li>
<li><p>Calculate your test statistic. In this case, it is called a <span class="math inline">\(T\)</span>-statistic since the sampling distribution is best modeled with a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(df = n-1\)</span>. These statistics are the same as <span class="math inline">\(z\)</span>-scores.</p></li>
</ol>
<p><span class="math display">\[ T = \frac{\overline{x} - \mu_0}{SE} \]</span></p>
<p>Note that this is where we’re using the null hypothesis! The <span class="math inline">\(T\)</span>-statistic measures how far our sample average is from the null average in terms of standard error.</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Calculate the <span class="math inline">\(p\)</span>-value of your test statistic. This depends on the type of test you’re performing. In this case, <span class="math inline">\(p\)</span>-value calculations are exactly like they are for single sample proportions, except you use the function <code>pt</code> in R instead of <code>pnorm</code>.</p></li>
<li><p>Compare your <span class="math inline">\(p\)</span>-value to the significance level <span class="math inline">\(\alpha\)</span> and make a conclusion.</p></li>
</ol>
<p>We’ll use the two data sets above to go through two different hypothesis test examples.</p>
<p><strong>women</strong>: As usual, we start with a question: “Were American women taller in 1975 than they were in 2022?” This question is actually a bit more complicated than it seems at first pass since the category “adult” spans a wide range of ages and average height varies with time. Our data set <code>women</code> looks at the average height of women aged between 30 and 39 in 1975, so really we should ask “Were American women aged 30-39 taller in 1975 than they were in 2022?”. We’ll use a significance level of <span class="math inline">\(\alpha = .1\)</span>.</p>
<p>Pulling data from <a href="https://ourworldindata.org/grapher/average-height-of-women?tab=chart&amp;country=~USA">Our World in Data</a>, the average height of an woman aged between 30 and 39 in 2022 was roughly 64.4 inches. If <span class="math inline">\(\mu\)</span> denotes the average height of women in 1975, then we set up our hypothesis test as</p>
<p><span class="math display">\[\begin{array}{cc}
H_0: &amp; \mu = 64.4 \\
H_1 &amp; \mu &gt; 64.4
\end{array}\]</span></p>
<p>The our sample mean, standard deviation, sample size, and standard error are calculated below.</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="inferential-statistics-take-1.html#cb488-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(women<span class="sc">$</span>height)</span>
<span id="cb488-2"><a href="inferential-statistics-take-1.html#cb488-2" aria-hidden="true" tabindex="-1"></a>xBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(women<span class="sc">$</span>height)</span>
<span id="cb488-3"><a href="inferential-statistics-take-1.html#cb488-3" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(women<span class="sc">$</span>height)</span>
<span id="cb488-4"><a href="inferential-statistics-take-1.html#cb488-4" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> s <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb488-5"><a href="inferential-statistics-take-1.html#cb488-5" aria-hidden="true" tabindex="-1"></a><span class="co">#printing the calculated values </span></span>
<span id="cb488-6"><a href="inferential-statistics-take-1.html#cb488-6" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">n =</span> n, <span class="at">xBar =</span> xBar, <span class="at">s =</span> s, <span class="at">SE =</span> SE)</span></code></pre></div>
<pre><code>##         n      xBar         s        SE 
## 15.000000 65.000000  4.472136  1.154701</code></pre>
<p>We now use these values to calculate the <span class="math inline">\(T\)</span> statistic. Note: we label this test statistic <code>t</code> because <code>T</code> is R shorthand for <code>TRUE</code>.</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="inferential-statistics-take-1.html#cb490-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> (xBar <span class="sc">-</span> <span class="fl">64.4</span>)<span class="sc">/</span>SE</span>
<span id="cb490-2"><a href="inferential-statistics-take-1.html#cb490-2" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>## [1] 0.5196152</code></pre>
<p>so our sample lies about half a standard deviation above the null average. We calculate the <span class="math inline">\(p\)</span>-value for this test using the function <code>pt</code> with <span class="math inline">\(df = n -1 = 15 -1 = 14\)</span>. Recall that we are performing a one-sided upper hypothesis test.</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="inferential-statistics-take-1.html#cb492-1" aria-hidden="true" tabindex="-1"></a>pVal <span class="ot">&lt;-</span> <span class="fu">pt</span>(t, <span class="at">df =</span> <span class="dv">15</span> <span class="sc">-</span> <span class="dv">1</span>, <span class="at">lower.tail =</span> F)</span>
<span id="cb492-2"><a href="inferential-statistics-take-1.html#cb492-2" aria-hidden="true" tabindex="-1"></a>pVal</span></code></pre></div>
<pre><code>## [1] 0.3057246</code></pre>
<p>This means that if women aged between 30-39 were the same height in 1975 as they were in 2022 there would be about a 30% chance of observing a sample with an average height of 65 inches or more.</p>
<p>Of course, since <span class="math inline">\(p \approx .306 \geq .1\)</span> we fail to reject the null hypothesis.</p>
<p><strong>faithful:</strong> Now imagine you’re a program director at Yellowstone National Park and in charge of the Old Faithful eruption presentations. You currently have presentations every 75 minutes, but for some reason, your scheduling seems off. You decide to to check to see if the average wait time between eruptions is different from 75 minutes using the data we have available in <code>faithful</code> and we will perform out hypothesis test using a significance level of <span class="math inline">\(\alpha = .01\)</span>. Re-framing your question as a hypothesis test yields</p>
<p><span class="math display">\[\begin{array}{cc}
H_0: &amp; \mu = 75 \\
H_1 &amp; \mu \neq  75
\end{array}\]</span></p>
<p>We derive the necessary numbers for this hypothesis test in the code chunk below.</p>
<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb494-1"><a href="inferential-statistics-take-1.html#cb494-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(faithful<span class="sc">$</span>waiting)</span>
<span id="cb494-2"><a href="inferential-statistics-take-1.html#cb494-2" aria-hidden="true" tabindex="-1"></a>xBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(faithful<span class="sc">$</span>waiting)</span>
<span id="cb494-3"><a href="inferential-statistics-take-1.html#cb494-3" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(faithful<span class="sc">$</span>waiting)</span>
<span id="cb494-4"><a href="inferential-statistics-take-1.html#cb494-4" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> s <span class="sc">/</span> <span class="fu">sqrt</span>(n)</span>
<span id="cb494-5"><a href="inferential-statistics-take-1.html#cb494-5" aria-hidden="true" tabindex="-1"></a><span class="co">#printing the calculated values </span></span>
<span id="cb494-6"><a href="inferential-statistics-take-1.html#cb494-6" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">n =</span> n, <span class="at">xBar =</span> xBar, <span class="at">s =</span> s, <span class="at">SE =</span> SE)</span></code></pre></div>
<pre><code>##           n        xBar           s          SE 
## 272.0000000  70.8970588  13.5949738   0.8243164</code></pre>
<p>We use these values to calculate the <span class="math inline">\(T\)</span>-test statistic.</p>
<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb496-1"><a href="inferential-statistics-take-1.html#cb496-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> (xBar <span class="sc">-</span> <span class="dv">75</span>)<span class="sc">/</span>SE</span>
<span id="cb496-2"><a href="inferential-statistics-take-1.html#cb496-2" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>## [1] -4.977387</code></pre>
<p>This shows our sample average is almost 5 standard deviations below the mean in the null sampling distribution. With this in mind, we expect to reject the null and accept the alternative hypothesis, but we’ll calculate the <span class="math inline">\(p\)</span>-value for our sample just to be sure.</p>
<p>Since we’re performing a two-sided hypothesis test and <span class="math inline">\(T &lt; 0\)</span>, the <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb498-1"><a href="inferential-statistics-take-1.html#cb498-1" aria-hidden="true" tabindex="-1"></a>pVal <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(t, <span class="at">df =</span> n<span class="dv">-1</span>)</span></code></pre></div>
<p>In other words, if the average wait time between eruptions were actually 75 minutes, there’s about a 0.00011% a sample of 272 eruptions with an average wait time as different from 75 minutes as our own. Since <span class="math inline">\(p &lt; \alpha\)</span>, we reject the null and accept the research: the data lead us to conclude that the average wait time between eruptions at Old Faithful is different from 75 minutes.</p>
<p>Now, it’s up to you as the imaginary program manager to figure out how to use this information.</p>
</div>
</div>
<div id="two-sample-mean" class="section level3 hasAnchor" number="6.3.2">
<h3><span class="header-section-number">6.3.2</span> Two sample mean<a href="inferential-statistics-take-1.html#two-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Just as in the previous section concerning numerical variables, we can soup up our work with a single sample mean so that we can use essentially the same two to compare two population averages using two independent samples. You will typically find yourself in this situation when you are testing for an association between a 2-valued categorical explanatory variable and a numerical response variable. One possible example is in studying the outcome of a randomized control trial concerning a particular drug’s affect on, say, reaction time. Give one group the drug, the other group a placebo, then measure the time it takes everyone to complete a particular reaction time oriented task. In this case, you’d want to compare the average reaction time of the treatment group (sample 1) to the average reaction time of the control group (sample 2). In this case, the explanatory variable is group membership (ie received the drug or did not) and the response variable is reaction time. An association between the variables in this case simply manifests itself as a difference between the group averages. In what follows we’ll learn how to analyze data that comes about from such a study.</p>
<p>The population parameter of interest is the difference in population means</p>
<p><span class="math display">\[ \mu_1 - \mu_2 \]</span></p>
<p>so the appropriate point estimate, no surprise, is the difference in sample averages</p>
<p><span class="math display">\[ \overline{x}_1 - \overline{x}_2 \]</span></p>
<p>Our hope is to use the tool we derived in the last section, but tweak the parameters a bit so that it can be applied to two samples instead of just one. As a first step, we hope to be able to apply the Central Limit Theorem, so, of course, both of our samples need to meet certain criteria. These are:</p>
<ul>
<li><p>Independent observations within groups. Observations in one group should not influence the measurement of any other observation in the same group.</p></li>
<li><p>Independence between groups. Observations in one group should not influence the measurement of any other observation in the other group.</p></li>
<li><p>The sampling distribution for each group should be approximately normal on its own. Check this using the sample size and/or normality check from the previous section.</p></li>
</ul>
<p>If your samples meet these criteria, the sampling distribution of <span class="math inline">\(\overline{x}_1 - \overline{x}_2\)</span> is approximately normal with</p>
<ul>
<li><p><span class="math inline">\(E(\overline{x}_1 - \overline{x}_2 ) = \mu_1 - \mu_2\)</span> and</p></li>
<li><p><span class="math inline">\(SE\left(\overline{x}_1 - \overline{x}_2 \right) = \sqrt{ \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2} } \approx \sqrt{ \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} }\)</span></p></li>
</ul>
<p>where <span class="math inline">\(s_i^2\)</span> and <span class="math inline">\(\sigma_i^2\)</span> are the sample and population standard deviations of group <span class="math inline">\(i\)</span> and <span class="math inline">\(n_i\)</span> is the sample size.</p>
<p>As in the single sample case, whenever we do not know the population standard deviation (ie almost always!) we must approximate the sampling distribution with a <span class="math inline">\(t\)</span>-distribution. This means we need to determine an appropriate number of degrees of freedom. We’ll skip thinking about why the following is true, but some smart people (Welch and Satterthwaite) have determined that</p>
<p><span class="math display">\[ df \approx \frac{\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{s_1^4}{n_1^2(n_1 - 1)} + \frac{s_2^4}{n_2^2(n_2 - 1)}}  \]</span></p>
<p>which is a terrible formula to try to use. It turns out that</p>
<p><span class="math display">\[ \min( n_1 -1, n_2 - 1 ) \leq \frac{\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\frac{s_1^4}{n_1^2(n_1 - 1)} + \frac{s_2^4}{n_2^2(n_2 - 1)}} \]</span></p>
<p>and using fewer degrees of freedom results in more conservative decision making. Thus, especially when making calculations “by hand,” it is common to set <span class="math inline">\(df\)</span> equal to the smaller of <span class="math inline">\(n_1 -1\)</span> and <span class="math inline">\(n_2 -1\)</span>, ie</p>
<p><span class="math display">\[ df \approx min(n_1 - 1, n_2 - 1).\]</span></p>
<p><strong>A note about equal variance two-sample tests:</strong> If you look around the internet or search other resources, you may stumble across a more simple formula for the degrees of freedom and/or standard error. In particular, you may see <span class="math inline">\(df = n_1 + n_2 - 2\)</span>. This degree of freedom calculation and the associated standard error calculation <em>are only valid if you {<strong>know</strong>} the variances of both populations are equal.</em> This is a “dangerous” assumption to make in that it is often hard or impossible to know this fact. We recommend instead to default the one of the two degree of freedom calculations along with the standard error formula described above as these will hold all of the time, even when the population variances are equal.</p>
<p>With these details ironed out, we’re ready to move on to some examples of inference. For our examples, we’ll use the <code>gss2010</code> data set from the <code>openintro</code> package which gives some data from the 2010 General Society Survey. We want to use this data set to explore the relationship between the respondents’ hours worked per week and their opinion about whether or not marijuana should be legalized. These variables are represented in the <code>hrs1</code> and <code>grass</code> column. The <code>grass</code> column has the values</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="inferential-statistics-take-1.html#cb499-1" aria-hidden="true" tabindex="-1"></a><span class="fu">unique</span>(gss2010<span class="sc">$</span>grass)</span></code></pre></div>
<pre><code>## [1] &lt;NA&gt;      LEGAL     NOT LEGAL
## Levels: LEGAL NOT LEGAL</code></pre>
<p>and the <code>hrs1</code> variable records the hours per week a respondent works. Some values are missing from both columns, so the code below subsets the data for respondents who answered both questions, then selects only the columns we’re interested in.</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="inferential-statistics-take-1.html#cb501-1" aria-hidden="true" tabindex="-1"></a>mj <span class="ot">&lt;-</span> gss2010 <span class="sc">%&gt;%</span></span>
<span id="cb501-2"><a href="inferential-statistics-take-1.html#cb501-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span><span class="fu">is.na</span>(hrs1) <span class="sc">&amp;</span> <span class="sc">!</span><span class="fu">is.na</span>(grass)) <span class="sc">%&gt;%</span> <span class="co">#select only rows with non-null values in hrs1 and grass</span></span>
<span id="cb501-3"><a href="inferential-statistics-take-1.html#cb501-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(grass, hrs1)</span></code></pre></div>
<p>Before moving on to making inference, let’s look at a summary of data.</p>
<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb502-1"><a href="inferential-statistics-take-1.html#cb502-1" aria-hidden="true" tabindex="-1"></a>mj <span class="sc">%&gt;%</span> </span>
<span id="cb502-2"><a href="inferential-statistics-take-1.html#cb502-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(grass) <span class="sc">%&gt;%</span></span>
<span id="cb502-3"><a href="inferential-statistics-take-1.html#cb502-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarise</span>(</span>
<span id="cb502-4"><a href="inferential-statistics-take-1.html#cb502-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">n</span>() <span class="co">#sample size for each stance on mj legalization</span></span>
<span id="cb502-5"><a href="inferential-statistics-take-1.html#cb502-5" aria-hidden="true" tabindex="-1"></a>    , <span class="at">xBar =</span> <span class="fu">mean</span>(hrs1) <span class="co">#sample average for stance</span></span>
<span id="cb502-6"><a href="inferential-statistics-take-1.html#cb502-6" aria-hidden="true" tabindex="-1"></a>    , <span class="at">s =</span> <span class="fu">sd</span>(hrs1) <span class="co">#sample standard deviation for each stance</span></span>
<span id="cb502-7"><a href="inferential-statistics-take-1.html#cb502-7" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## # A tibble: 2 × 4
##   grass         n  xBar     s
##   &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 LEGAL       357  41.5  14.5
## 2 NOT LEGAL   354  40.0  15.5</code></pre>
<div id="hypothesis-testing-3" class="section level4 hasAnchor" number="6.3.2.1">
<h4><span class="header-section-number">6.3.2.1</span> Hypothesis testing<a href="inferential-statistics-take-1.html#hypothesis-testing-3" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We’ll begin with a question: Is stance on marijuana legalization associated with weekly hours worked? In other words, is there a difference between the average number of hours worked by people who believe marijuana should be legalized and those who don’t? Let <span class="math inline">\(\mu_L\)</span> and <span class="math inline">\(\mu_N\)</span> denote the average number of hours worked weekly by those who believe marijuana should be legalized and those who think it should be illegal. Then our question, as a hypothesis test, is</p>
<ul>
<li><p><span class="math inline">\(H_0: \mu_L - \mu_N = 0\)</span></p></li>
<li><p><span class="math inline">\(H_a: \mu_L - \mu_N \neq 0\)</span></p></li>
</ul>
<p>Let’s perform this hypothesis test with a significance level of <span class="math inline">\(\alpha = .05\)</span>. First, we should think about the conditions needed to ensure an approximately normal sampling distribution:</p>
<ul>
<li><p>Independence between and within groups: this seems safe to assume because the General Society Survey, the source of this data, is a high quality randomized survey.</p></li>
<li><p>Normality of each individual sampling distribution: the sample sizes are both farily large (<span class="math inline">\(n_i \geq 350\)</span>), so the CLT would apply to each group individually.</p></li>
</ul>
<p>Now we need to calculate a bunch of sample statistics. In particular, the sample size, sample mean, and sample standard deviation for each group. The code below first breaks our data set into two smaller sets, one for each group.</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="inferential-statistics-take-1.html#cb504-1" aria-hidden="true" tabindex="-1"></a>legal <span class="ot">&lt;-</span> mj <span class="sc">%&gt;%</span> </span>
<span id="cb504-2"><a href="inferential-statistics-take-1.html#cb504-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(grass <span class="sc">==</span> <span class="st">&#39;LEGAL&#39;</span>)</span>
<span id="cb504-3"><a href="inferential-statistics-take-1.html#cb504-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb504-4"><a href="inferential-statistics-take-1.html#cb504-4" aria-hidden="true" tabindex="-1"></a>illegal <span class="ot">&lt;-</span> mj <span class="sc">%&gt;%</span> </span>
<span id="cb504-5"><a href="inferential-statistics-take-1.html#cb504-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(grass <span class="sc">==</span> <span class="st">&#39;NOT LEGAL&#39;</span>)</span></code></pre></div>
<p>Now we calculate all of the sample statistics (note that these are all in the summary table above…).</p>
<div class="sourceCode" id="cb505"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb505-1"><a href="inferential-statistics-take-1.html#cb505-1" aria-hidden="true" tabindex="-1"></a><span class="co">#legal</span></span>
<span id="cb505-2"><a href="inferential-statistics-take-1.html#cb505-2" aria-hidden="true" tabindex="-1"></a>xBarL <span class="ot">&lt;-</span> <span class="fu">mean</span>(legal<span class="sc">$</span>hrs1)</span>
<span id="cb505-3"><a href="inferential-statistics-take-1.html#cb505-3" aria-hidden="true" tabindex="-1"></a>sL <span class="ot">&lt;-</span> <span class="fu">sd</span>(legal<span class="sc">$</span>hrs1)</span>
<span id="cb505-4"><a href="inferential-statistics-take-1.html#cb505-4" aria-hidden="true" tabindex="-1"></a>nL <span class="ot">&lt;-</span> <span class="fu">length</span>(legal<span class="sc">$</span>hrs1)</span>
<span id="cb505-5"><a href="inferential-statistics-take-1.html#cb505-5" aria-hidden="true" tabindex="-1"></a><span class="co">#illegal</span></span>
<span id="cb505-6"><a href="inferential-statistics-take-1.html#cb505-6" aria-hidden="true" tabindex="-1"></a>xBarN <span class="ot">&lt;-</span> <span class="fu">mean</span>(illegal<span class="sc">$</span>hrs1)</span>
<span id="cb505-7"><a href="inferential-statistics-take-1.html#cb505-7" aria-hidden="true" tabindex="-1"></a>sN <span class="ot">&lt;-</span> <span class="fu">sd</span>(illegal<span class="sc">$</span>hrs1)</span>
<span id="cb505-8"><a href="inferential-statistics-take-1.html#cb505-8" aria-hidden="true" tabindex="-1"></a>nN <span class="ot">&lt;-</span> <span class="fu">length</span>(illegal<span class="sc">$</span>hrs1)</span>
<span id="cb505-9"><a href="inferential-statistics-take-1.html#cb505-9" aria-hidden="true" tabindex="-1"></a><span class="co">#standard error</span></span>
<span id="cb505-10"><a href="inferential-statistics-take-1.html#cb505-10" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> <span class="fu">sqrt</span>( sL<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>nL <span class="sc">+</span> sN<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>nN)</span></code></pre></div>
<p>We’re angling to calculate a <span class="math inline">\(p\)</span>-value, so need to determine our <span class="math inline">\(t\)</span>-test statistic and the number of degrees of freedom. The test statistic is easier, so we’ll do that first. As usual it is the point estimate minus the null value all divided by the standard error.</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="inferential-statistics-take-1.html#cb506-1" aria-hidden="true" tabindex="-1"></a><span class="co">#note that the null difference is 0</span></span>
<span id="cb506-2"><a href="inferential-statistics-take-1.html#cb506-2" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> (xBarL <span class="sc">-</span> xBarN)<span class="sc">/</span>SE</span>
<span id="cb506-3"><a href="inferential-statistics-take-1.html#cb506-3" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>## [1] 1.352633</code></pre>
<p>As a demonstration of the claims about degrees of freedom mentioned above, we will calculate this quantity both ways. The long way gives</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="inferential-statistics-take-1.html#cb508-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> (sL<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>nL <span class="sc">+</span> sN<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>nN)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span> ( sL<span class="sc">^</span><span class="dv">4</span><span class="sc">/</span>(nL<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(nL<span class="dv">-1</span>)) <span class="sc">+</span> sN<span class="sc">^</span><span class="dv">4</span><span class="sc">/</span>(nN<span class="sc">^</span><span class="dv">2</span><span class="sc">*</span>(nN<span class="dv">-1</span>)))</span>
<span id="cb508-2"><a href="inferential-statistics-take-1.html#cb508-2" aria-hidden="true" tabindex="-1"></a>df1</span></code></pre></div>
<pre><code>## [1] 705.1106</code></pre>
<p>which isn’t a whole number. This isn’t a problem even though it’s a pain to calculate ‘by hand.’</p>
<p>The easier approximation is</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb510-1"><a href="inferential-statistics-take-1.html#cb510-1" aria-hidden="true" tabindex="-1"></a>df2 <span class="ot">&lt;-</span> <span class="fu">min</span>(nL<span class="dv">-1</span>, nN<span class="dv">-1</span>)</span>
<span id="cb510-2"><a href="inferential-statistics-take-1.html#cb510-2" aria-hidden="true" tabindex="-1"></a>df2</span></code></pre></div>
<pre><code>## [1] 353</code></pre>
<p>which is quite a bit smaller than the “official” number of degrees of freedom, yielding slightly more conservative decisions in hypothesis testing and wider confidence intervals. Now that we have our test statistic and degrees of freedom, we can calculate our <span class="math inline">\(p\)</span>-value. Our test statistic <span class="math inline">\(t\)</span> is positive, so we can calculate our <span class="math inline">\(p\)</span>-value as</p>
<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb512-1"><a href="inferential-statistics-take-1.html#cb512-1" aria-hidden="true" tabindex="-1"></a><span class="co">#using df1, the &#39;official&#39; degrees of freedom</span></span>
<span id="cb512-2"><a href="inferential-statistics-take-1.html#cb512-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(t, <span class="at">df =</span> df1, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.1766066</code></pre>
<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb514-1"><a href="inferential-statistics-take-1.html#cb514-1" aria-hidden="true" tabindex="-1"></a><span class="co">#using df2, the &#39;approximate&#39; degrees of freedom</span></span>
<span id="cb514-2"><a href="inferential-statistics-take-1.html#cb514-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(t, <span class="at">df =</span> df2, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.1770387</code></pre>
<p>Notice that the 2 <span class="math inline">\(p\)</span>-value calculations are not all that different, but that the second value is ever so slightly greater than the first. This is reflecting the more conservative degree of freedom choice.</p>
<p>To finish up, our <span class="math inline">\(p\)</span>-value is greater than the significance level <span class="math inline">\(\alpha = .05\)</span>, so we fail to reject the null. In other words, our data do not provide compelling evidence of a difference in average weekly hours worked based on marijuana legalization stance.</p>
<p>In the next section we’ll use the calculations from this section to calculate and interpret confidence intervals for a difference in two averages.</p>
</div>
<div id="confidence-intervals-4" class="section level4 hasAnchor" number="6.3.2.2">
<h4><span class="header-section-number">6.3.2.2</span> Confidence intervals<a href="inferential-statistics-take-1.html#confidence-intervals-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When you calculate a confidence interval for a difference between two means it is important to remember that that is exactly what your interval is estimating. In others words, you’re estimating <span class="math inline">\(\mu_1 - \mu-2\)</span>. The procedure and basic ingredients needed for calculating these confidence intervals is essentially the same as always:</p>
<p><span class="math display">\[ \text{point est.} \pm \text{crit. val.}\cdot SE \]</span></p>
<p>where in this case</p>
<ul>
<li><p><span class="math inline">\(\text{point est.} = \overline{x}_1 - \overline{x}_2\)</span>;</p></li>
<li><p><span class="math inline">\(\text{crit. val.} = t^\star_df\)</span>, the <span class="math inline">\(t\)</span>-critical value that corresponds to your confidence level and the appropriate number of degrees of freedom, approximately <span class="math inline">\(df = \min \left( n_1 -1, n_2 - 2) \right)\)</span>; and</p></li>
<li><p><span class="math inline">\(SE \approx \sqrt{ \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} }\)</span></p></li>
</ul>
<p>Continuing the example from the previous section, let’s estimate the difference in average weekly hours worked by people who believe marijuana should be legalized and those who don’t at the 90% confidence level. In the previous section we calculated our point estimate, the standard error, and the number of degrees of freedom needed for this calculation. They are</p>
<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb516-1"><a href="inferential-statistics-take-1.html#cb516-1" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="at">point.est =</span> xBarL <span class="sc">-</span> xBarN, <span class="at">SE =</span> SE, <span class="at">df =</span> df2)</span></code></pre></div>
<pre><code>##  point.est         SE         df 
##   1.523643   1.126428 353.000000</code></pre>
<p>so the only ingredient we need to calculate is the <span class="math inline">\(t\)</span>-critical value <span class="math inline">\(t_{df = 353}^\star\)</span>. We’re using a 90% confidence interval, so this value is</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="inferential-statistics-take-1.html#cb518-1" aria-hidden="true" tabindex="-1"></a>t.crit <span class="ot">&lt;-</span> <span class="fu">qt</span>( (<span class="dv">1</span><span class="fl">-.9</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="at">df =</span> df2, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb518-2"><a href="inferential-statistics-take-1.html#cb518-2" aria-hidden="true" tabindex="-1"></a>t.crit</span></code></pre></div>
<pre><code>## [1] 1.649182</code></pre>
<p>Our 90% confidence interval is</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="inferential-statistics-take-1.html#cb520-1" aria-hidden="true" tabindex="-1"></a>(xBarL <span class="sc">-</span> xBarN) <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span>t.crit<span class="sc">*</span>SE</span></code></pre></div>
<pre><code>## [1] -0.3340409  3.3813276</code></pre>
<p>Interpreting this confidence interval is maybe the newest thing in this section. Since we’re estimating <span class="math inline">\(\mu_{legal} - \mu_{not\,legal}\)</span>, the negative end point gives an estimate where <span class="math inline">\(\mu_{not\, legal} &gt; \mu_{legal}\)</span> and while the positive endpoint gives an estimate where <span class="math inline">\(\mu_{legal} &gt; \mu_{not\, legal}\)</span>.</p>
<p>Thus, with 90% confidence, we estimate that people who believe that marijuana should be legal work, on average, between .33 hours <em>less</em> and 3.38 hours <em>more</em> than people who think the drug should be illegal.</p>
<p>Note that this confidence interval calculation jives with our hypothesis test in the previous section since 0 is in the interval. This means the null value from the hypothesis test is a plausible value for the true difference in means, so we better have failed to reject the null (which we did!).</p>
</div>
</div>
<div id="paired-data" class="section level3 hasAnchor" number="6.3.3">
<h3><span class="header-section-number">6.3.3</span> Paired data<a href="inferential-statistics-take-1.html#paired-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Think back to making inferences about two-sample proportions. When performing our analysis, we had either verify or assume that the observations in our samples were independent from one another, both within and between groups. It turns out that some studies and experiments produce numerical data that do not satisfy this condition, but the way they fail to meet the independence condition is a feature instead of a bug from an analysis perspective. Some examples of data where we witness this type of failure show up when</p>
<ul>
<li><p>Comparing textbook prices from the FVCC Bookstore and Amazon.</p></li>
<li><p>Comparing the volume of bikes tire on two different rims.</p></li>
<li><p>Comparing individuals’ pre-course test score with their post-course test score.</p></li>
</ul>
<p>Notice that in each of these cases, we’re taking two measurements for the same observational unit. For example, the book <em>OpenIntro Statistics</em> costs $20 from Amazon but the same book costs $30 from the FVCC book store. The observational unit in this case is the textbook and there are two different price measurements for it. Because it is the same book, the Amazon and FVCC bookstore prices are <em>dependent</em>!</p>
<p>The examples above all generate what we call <strong>paired data</strong> where you have two collections of measurements, but each observation in one collection has a “special correspondence/correlation” with an observation in the other collection. Think for a second about the types of questions that one might try to answer with paired data. Mimicking the examples of paired data above, we might ask the following:</p>
<ul>
<li><p>Does the FVCC Bookstore charge more than Amazon, on average?</p></li>
<li><p>Does rim width affect bike tire volume?</p></li>
<li><p>Does a course improve students’ knowledge and skillet in a particular subject?</p></li>
</ul>
<p>Each of these questions can be answered by looking at the <em>average difference</em> in the paired data sets. We use <span class="math inline">\(\mu_{diff}\)</span> to denote the average difference in a paired data set. If you take the difference between the paired measurements, you have a single measurement for each observational unit. This implies that one can analyze <span class="math inline">\(\mu_{diff}\)</span> just like one would analyze a single-sample numerical variable! In particular, the central limit theorem, as stated in the previous section applies in the exact same way, you just calculate and use the sample average and standard deviation <em>of the differences</em>.</p>
<p>In the subsections below we will go through an example of calculating and interpreting a confidence interval and a hypothesis test for some paired data. While it would be nice to actually analyze textbook price differences for FVCC as it compares to Amazon, gathering that information is somewhat tedious. Fortunately, the <code>openintro</code> package has textbook data from UCLA from Fall 2018. This is stored in the data set <code>ucla_textbooks_f18</code>. Below we select the course name along with the course’s new textbook price from the UCLA bookstore and Amazon.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="inferential-statistics-take-1.html#cb522-1" aria-hidden="true" tabindex="-1"></a>books <span class="ot">&lt;-</span> ucla_textbooks_f18 <span class="sc">%&gt;%</span></span>
<span id="cb522-2"><a href="inferential-statistics-take-1.html#cb522-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(course, bookstore_new, amazon_new) <span class="sc">%&gt;%</span></span>
<span id="cb522-3"><a href="inferential-statistics-take-1.html#cb522-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">drop_na</span>() <span class="co">#this omits all rows with an NA or unknown price</span></span>
<span id="cb522-4"><a href="inferential-statistics-take-1.html#cb522-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb522-5"><a href="inferential-statistics-take-1.html#cb522-5" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(books)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##   course                                                   bookstore_new amazon_new
##   &lt;fct&gt;                                                            &lt;dbl&gt;      &lt;dbl&gt;
## 1 Introduction to American Indian Studies                           48.0       47.4
## 2 Archaeology: Introduction                                         14.3       13.6
## 3 Arts Encounters: Exploring Arts Literacy in 21st Century          13.5       12.5
## 4 Introduction to Buddhism                                          49.3       55.0
## 5 Black Holes and Cosmic Catastrophes                              120.       125. 
## 6 Introduction to Communication Studies                             17.0       11.8</code></pre>
<p>We want to investigate the average difference in cost between Amazon and UCLA’s bookstore. Let’s add a difference column to the <code>books</code> data set. Note that we will look at the bookstore’s cost minus Amazon’s cost for each book.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="inferential-statistics-take-1.html#cb524-1" aria-hidden="true" tabindex="-1"></a>books <span class="ot">&lt;-</span> books <span class="sc">%&gt;%</span></span>
<span id="cb524-2"><a href="inferential-statistics-take-1.html#cb524-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">diff =</span> bookstore_new <span class="sc">-</span> amazon_new)</span>
<span id="cb524-3"><a href="inferential-statistics-take-1.html#cb524-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb524-4"><a href="inferential-statistics-take-1.html#cb524-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(books)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 4
##   course                                                bookstore_new amazon_new   diff
##   &lt;fct&gt;                                                         &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;
## 1 Introduction to American Indian Studies                        48.0       47.4  0.520
## 2 Archaeology: Introduction                                      14.3       13.6  0.710
## 3 Arts Encounters: Exploring Arts Literacy in 21st Cen…          13.5       12.5  0.97 
## 4 Introduction to Buddhism                                       49.3       55.0 -5.69 
## 5 Black Holes and Cosmic Catastrophes                           120.       125.  -4.83 
## 6 Introduction to Communication Studies                          17.0       11.8  5.18</code></pre>
<p>Let’s compare the average difference in price to the difference between average prices (recall this is what we looked at in the previous section).</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="inferential-statistics-take-1.html#cb526-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(books<span class="sc">$</span>diff)</span></code></pre></div>
<pre><code>## [1] 3.583235</code></pre>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="inferential-statistics-take-1.html#cb528-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(books<span class="sc">$</span>bookstore_new) <span class="sc">-</span> <span class="fu">mean</span>(books<span class="sc">$</span>amazon_new)</span></code></pre></div>
<pre><code>## [1] 3.583235</code></pre>
<p>They are the same! So what’s different here from the last section? Well, in the previous section we had two <em>independent</em> samples, but now, while we are looking at two columns of data, we essentially only have a <em>single</em> sample of independent measurements. This changes two things, one small and one large:</p>
<ul>
<li><p>The relatively small change is the number of degrees of freedom. If we were performing a two-sample <span class="math inline">\(t\)</span>-test with the data in <code>books</code> we’d use $ df = 133.59$, but with a single sample of differences we use <span class="math inline">\(df = n-1 = 67\)</span>.</p></li>
<li><p>The larger change is the standard error used in inference. Recall that two-sample <span class="math inline">\(t\)</span>-test uses
<span class="math display">\[ SE = \sqrt{ \frac{ \sigma_1^2}{n_1}  + \frac{ \sigma_2^2}{n_2} } \approx \sqrt{ \frac{ \sigma_1^2}{n_1}  + \frac{ \sigma_2^2}{n_2} } \]</span>
precisely because we have two independent samples. When we only have a single independent sample of differences, we use
<span class="math display">\[ SE = \sigma_{diff}/\sqrt{n} \]</span>
just as you would for a single sample mean analysis. Let’s compare these values for our working example.</p></li>
</ul>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="inferential-statistics-take-1.html#cb530-1" aria-hidden="true" tabindex="-1"></a><span class="co"># If the book samples were independent, the SE would be</span></span>
<span id="cb530-2"><a href="inferential-statistics-take-1.html#cb530-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>( <span class="fu">var</span>(books<span class="sc">$</span>bookstore_new)<span class="sc">/</span><span class="fu">length</span>(books<span class="sc">$</span>bookstore_new) </span>
<span id="cb530-3"><a href="inferential-statistics-take-1.html#cb530-3" aria-hidden="true" tabindex="-1"></a>      <span class="sc">+</span> <span class="fu">var</span>(books<span class="sc">$</span>amazon_new)<span class="sc">/</span><span class="fu">length</span>(books<span class="sc">$</span>amazon_new))</span></code></pre></div>
<pre><code>## [1] 9.518939</code></pre>
<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb532-1"><a href="inferential-statistics-take-1.html#cb532-1" aria-hidden="true" tabindex="-1"></a><span class="co"># But in reality the standard error should be</span></span>
<span id="cb532-2"><a href="inferential-statistics-take-1.html#cb532-2" aria-hidden="true" tabindex="-1"></a><span class="fu">sd</span>(books<span class="sc">$</span>diff)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">length</span>(books<span class="sc">$</span>diff))</span></code></pre></div>
<pre><code>## [1] 1.627834</code></pre>
<p>When you have two independent samples you expect <em>more</em> variability between samples! This should be somewhat intuitive: since the two samples have no sway on one another, they can be a lot more different.</p>
<p>Next up we’ll see how to use these details to perform inference, starting with confidence intervals, then performing a hypothesis test.</p>
<div id="confidence-intervals-5" class="section level4 hasAnchor" number="6.3.3.1">
<h4><span class="header-section-number">6.3.3.1</span> Confidence intervals<a href="inferential-statistics-take-1.html#confidence-intervals-5" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As always, the formula for a <span class="math inline">\(c\)</span>% confidence interval starts with
<span class="math display">\[ \text{point est.} \pm \text{crit val.}\cdot SE\]</span>
Filling in the details, we have</p>
<ul>
<li><p>Point estimate: the average difference, <span class="math inline">\(\overline{x}_{diff}\)</span>.</p></li>
<li><p>$ SE = <em>{diff} / s</em>{diff} / $</p></li>
<li><p>critical value: the <span class="math inline">\(t\)</span>-critical value with <span class="math inline">\(df = n -1\)</span>.</p></li>
</ul>
<p>Let’s use this to calculate a 95% confidence interval for the average difference between bookstore and Amazon textbook prices for UCLA in 2018.</p>
<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb534-1"><a href="inferential-statistics-take-1.html#cb534-1" aria-hidden="true" tabindex="-1"></a><span class="co">#first we calculate all the necessary pieces</span></span>
<span id="cb534-2"><a href="inferential-statistics-take-1.html#cb534-2" aria-hidden="true" tabindex="-1"></a>xBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(books<span class="sc">$</span>diff)</span>
<span id="cb534-3"><a href="inferential-statistics-take-1.html#cb534-3" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(books<span class="sc">$</span>diff)</span>
<span id="cb534-4"><a href="inferential-statistics-take-1.html#cb534-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(books<span class="sc">$</span>diff)</span>
<span id="cb534-5"><a href="inferential-statistics-take-1.html#cb534-5" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> s <span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb534-6"><a href="inferential-statistics-take-1.html#cb534-6" aria-hidden="true" tabindex="-1"></a>crit.val <span class="ot">&lt;-</span> <span class="fu">qt</span>( (<span class="dv">1</span><span class="fl">-.95</span>)<span class="sc">/</span><span class="dv">2</span>, <span class="at">df =</span> n <span class="sc">-</span><span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span>
<span id="cb534-7"><a href="inferential-statistics-take-1.html#cb534-7" aria-hidden="true" tabindex="-1"></a><span class="co">#then calculate the confidence interval</span></span>
<span id="cb534-8"><a href="inferential-statistics-take-1.html#cb534-8" aria-hidden="true" tabindex="-1"></a>xBar <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span>crit.val<span class="sc">*</span>SE</span></code></pre></div>
<pre><code>## [1] 0.3340641 6.8324064</code></pre>
<p>What does this mean? Remember, we’re estimating the average difference in book cost between the UCLA bookstore and Amazon. Thus, with 95% confidence, new textbooks from the UCLA book store cost, on average between $0.33 and $6.83 <em>more</em> than the same new textbook on Amazon.</p>
</div>
<div id="hypothesis-testing-4" class="section level4 hasAnchor" number="6.3.3.2">
<h4><span class="header-section-number">6.3.3.2</span> Hypothesis testing<a href="inferential-statistics-take-1.html#hypothesis-testing-4" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Just as with calculating and interpreting confidence intervals, the key to performing hypothesis tests on paired data is to remember that pairing yields a single sample of differences, so the details of inference are essentially the same as they are for a single sample mean.</p>
<p>Returning to our <code>books</code> example, let’s try to answer the following question: “Do new textbooks from the UCLA bookstore cost more, on average, than new textbooks from Amazon?” Turning this question into a hypothesis test.</p>
<ul>
<li><p><span class="math inline">\(H_0: \mu_{diff} = 0\)</span></p></li>
<li><p><span class="math inline">\(H_a: \mu_{diff} &gt; 0\)</span></p></li>
</ul>
<p>Here note that <span class="math inline">\(\mu_{diff}\)</span> corresponds to bookstore price minus Amazon price. Let’s perform this hypothesis test with a significance level of <span class="math inline">\(\alpha = .1\)</span>. The sample statistics we need are calculated below.</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="inferential-statistics-take-1.html#cb536-1" aria-hidden="true" tabindex="-1"></a>xBar <span class="ot">&lt;-</span> <span class="fu">mean</span>(books<span class="sc">$</span>diff)</span>
<span id="cb536-2"><a href="inferential-statistics-take-1.html#cb536-2" aria-hidden="true" tabindex="-1"></a>s <span class="ot">&lt;-</span> <span class="fu">sd</span>(books<span class="sc">$</span>diff)</span>
<span id="cb536-3"><a href="inferential-statistics-take-1.html#cb536-3" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(books<span class="sc">$</span>diff)</span>
<span id="cb536-4"><a href="inferential-statistics-take-1.html#cb536-4" aria-hidden="true" tabindex="-1"></a>SE <span class="ot">&lt;-</span> s <span class="sc">/</span><span class="fu">sqrt</span>(n)</span>
<span id="cb536-5"><a href="inferential-statistics-take-1.html#cb536-5" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(xBar, s, n, SE)</span></code></pre></div>
<pre><code>## [1]  3.583235 13.423467 68.000000  1.627834</code></pre>
<p>The test statistic is a <span class="math inline">\(t\)</span>-statistic and is</p>
<p><span class="math display">\[ t =  \frac{\overline{x}_{diff} - \mu_{diff, null}}{SE} \]</span></p>
<p>where <span class="math inline">\(\mu_{diff,null}\)</span> is the null value of the mean difference; this value is typically zero. Thus our test statistic is</p>
<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb538-1"><a href="inferential-statistics-take-1.html#cb538-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> xBar<span class="sc">/</span>SE</span>
<span id="cb538-2"><a href="inferential-statistics-take-1.html#cb538-2" aria-hidden="true" tabindex="-1"></a>t</span></code></pre></div>
<pre><code>## [1] 2.201228</code></pre>
<p>which means that if there were no difference in price, our sample falls about 2.2 standard deviations above the null value <em>in the sampling distribution</em>.</p>
<p>We use our <span class="math inline">\(t\)</span>-statistic to calculate the <span class="math inline">\(p\)</span>-value for this test. Since we are performing a one sided, upper hypothesis test the <span class="math inline">\(p\)</span>-value is</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb540-1"><a href="inferential-statistics-take-1.html#cb540-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pt</span>(t, <span class="at">df =</span> n <span class="sc">-</span><span class="dv">1</span>, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## [1] 0.01558447</code></pre>
<p>Thus, if there were no difference in textbook costs, there would be about a 1.6% chance of observing a sample with differences at least as extreme as those observed in our sample.</p>
<p>The <span class="math inline">\(p\)</span>-value <span class="math inline">\(.0156 &lt; \alpha = .05\)</span>, so we reject the null and accept the research. In a sentence: the data provide compelling evidence to believe that, on average, new textbooks cost more from the UCLA bookstore than from Amazon.</p>
</div>
</div>
<div id="analysis-of-variance-anova" class="section level3 hasAnchor" number="6.3.4">
<h3><span class="header-section-number">6.3.4</span> Analysis of variance (ANOVA)<a href="inferential-statistics-take-1.html#analysis-of-variance-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>From a calculation standpoint, the <strong>analysis of variance tests</strong>, or <strong>ANOVA tests</strong> may feel like the most technical type of hypothesis test we learn about this semester. With that in mind, I encourage you to rely on technology (ie R) maybe even more for this type of test than all of the rest. In this section, we will go through the nitty gritty details of ANOVA, *<strong>but I suggest you use the <code>aov</code> function described in a future section for all of your own calculations</strong>. This function does all the work for you, which helps minimize error on your part.</p>
<p>With that disclaimer out of the way, what is the purpose of ANOVA? In the most direct sense, it is an extension of a two sample mean comparison. Recall that you can think of a two-sample mean comparison as a check for an association between a categorical <em>explanatory variable</em> with two values and a numerical <em>response variable</em>. For instance, think back to our example where we checked to see if belief about marijuana legalization had any association with average weekly hours worked (it appears not to). In this case, stance on legalization was the categorical explanatory variable and weekly hours worked was the numerical response variable.</p>
<p>Well, what would you do if your categorical explanatory variable had more than two possible values? Let’s set up a simulated example that we’ll use to demonstrate the ideas at play here. Suppose we run an experiment testing the efficacy of some new drug and we suspect the effect of the drug might be dose dependent. We randomly sample 45 people to participate in the trial and assign them to one of three groups:</p>
<ul>
<li><p>A: control group receiving only a placebo;</p></li>
<li><p>B: treatment group receiving 1mg of the drug;</p></li>
<li><p>C: treatment group receiving 2mg of the drug.</p></li>
</ul>
<p>Measure some health index <span class="math inline">\(H\)</span> for each participant after 2 weeks.The code below simulates data for two different trials of the same experiment. Group A0 is the group A from the first trial while group A1 is the A group from the second trial.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-293-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>There is a very clear difference between the two trials; you can see that there’s much more “spread” in trial 0 than in trial 1 which seems to make the differences between the group averages more apparent in the second trial. More on this soon.</p>
<p>Let’s return to the original question: how could we analyze the results of only one of these trials? One thought is to try to make an inference about difference between the group averages two at a time. The idea behind this is that if there is no association between the group and outcome, then the group averages would all be the same. Thus, an association would rear its head as a difference between group averages.</p>
<p>To test two groups at a time for trial 0 you’d have to test A0 vs B0, A0 vs C0, and B0 vs C0. This might not seem like too much of an issue, but a major problem rears its head when you perform many test: the probability of making at least one Type I error (ie a false positive) grows exponentially. Suppose you have <span class="math inline">\(k\)</span> groups or <span class="math inline">\(k\)</span> values of a categorical variable. To test for a difference in a numerical variable for two values at a time, you would have to perform<br />
<span class="math display">\[  m =  {k \choose 2}   \]</span>
distinct hypotheses tests. Remember that the significance level <span class="math inline">\(\alpha\)</span> of a hypothesis test is the probability of a false positive if the null is true. Thus, if all group averages are the same, the probability of making at least one Type I error out of all <span class="math inline">\(m\)</span> 2-sample <span class="math inline">\(t\)</span>-tests is</p>
<p><span class="math display">\[ 1 - (1 - \alpha)^m\]</span></p>
<p>The table below gives this probability for a few different group numbers <span class="math inline">\(k\)</span> and <span class="math inline">\(\alpha = .05\)</span></p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="inferential-statistics-take-1.html#cb542-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb542-2"><a href="inferential-statistics-take-1.html#cb542-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">k =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span><span class="sc">*</span><span class="dv">5</span></span>
<span id="cb542-3"><a href="inferential-statistics-take-1.html#cb542-3" aria-hidden="true" tabindex="-1"></a>  , <span class="at">probOneTypeOne =</span> <span class="fu">round</span>(<span class="dv">1</span> <span class="sc">-</span> .<span class="dv">95</span><span class="sc">^</span><span class="fu">choose</span>(k, <span class="dv">2</span>), <span class="dv">4</span>)</span>
<span id="cb542-4"><a href="inferential-statistics-take-1.html#cb542-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## # A tibble: 5 × 2
##       k probOneTypeOne
##   &lt;dbl&gt;          &lt;dbl&gt;
## 1     5          0.401
## 2    10          0.901
## 3    15          0.995
## 4    20          1.00 
## 5    25          1</code></pre>
<p>If you have 15 groups or 15 values for your categorical variable, at least one Type I error is virtually certain. This might seem outlandish at first glance - what categorical variables have more than 15 meaningful groups? It turns out that this becomes a major problem in genetics, as one example, where one can <a href="https://en.wikipedia.org/wiki/Multiple_comparisons_problem">“measure expression levels of tens of thousands of genes”</a>.</p>
<p>For our purposes we can get around this dilemma by understanding a single hypothesis test, <strong>the Analysis of Variance or ANOVA test</strong> that checks for an association between the categorical explanatory variable and the numerical response variable. Suppose we have <span class="math inline">\(k\)</span> different groups that we’d like to compare the average of for some numerical variable. Let <span class="math inline">\(\mu_i\)</span> denote the population mean of group <span class="math inline">\(i\)</span>. The null and alternative hypotheses for ANOVA are:</p>
<ul>
<li><p><span class="math inline">\(H_0: \mu_1 = \mu_2 = \cdots = \mu_k\)</span>. (the variables are independent)</p></li>
<li><p><span class="math inline">\(H_a: \text{at least one mean differs from the others}\)</span> (there is an association between the variables)</p></li>
</ul>
<p>Before digging in to the details of this test, we should understand the conditions required for making a valid inference. We need</p>
<ul>
<li><p>observations that are independent between and within groups;</p></li>
<li><p>the observations within each group to come from a normally distributed population; and</p></li>
<li><p>the variances of the groups are all approximately equal</p></li>
</ul>
<p>If you look back up to the code generating our simulated example, you’ll notice that these conditions are satisfied by our data:</p>
<ul>
<li><p>we randomly generated independent observations with <code>rnorm</code> to ensure independence and that we were sampling from normally distributed populations.</p></li>
<li><p>we chose the standard deviations of the normal distrubtions to be about the same in each trial to make sure we met the last condition.</p></li>
</ul>
<p>With these conditions satisfied, we’re ready to proceed in understanding ANOVA. We use the plot above as motivation. Again, notice that in trial zero, there is not very much of a difference between the centers of each group <em>relative</em> to the amount of variability within each group. On the other hand, in trial one, the difference between the group’s centers are large <em>relative</em> to the amount of variability within each group. The boxplots below help to emphasize this point.</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="inferential-statistics-take-1.html#cb544-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> Group, <span class="at">y =</span> Outcome, <span class="at">color =</span> Trial)) <span class="sc">+</span></span>
<span id="cb544-2"><a href="inferential-statistics-take-1.html#cb544-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb544-3"><a href="inferential-statistics-take-1.html#cb544-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="at">alpha =</span> .<span class="dv">5</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-295-1.png" width="672" /></p>
<p>This plot indicates that we should try to develop a single sample statistic that looks at the variability of the group averages relative to the variability within all groups. This will be the <span class="math inline">\(F\)</span>-statistics:</p>
<p><span class="math display">\[ F = \frac{\text{mean variability between groups}}{\text{mean variability within groups}}\]</span></p>
<p>We just need to figure out how to best measure the numerator and the denominator. We will make calculations for both trial 0 and trial 1 simultaneously, so subset <code>df</code> for each trial first.</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb545-1"><a href="inferential-statistics-take-1.html#cb545-1" aria-hidden="true" tabindex="-1"></a>df0 <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb545-2"><a href="inferential-statistics-take-1.html#cb545-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Trial <span class="sc">==</span> <span class="dv">0</span> )</span>
<span id="cb545-3"><a href="inferential-statistics-take-1.html#cb545-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb545-4"><a href="inferential-statistics-take-1.html#cb545-4" aria-hidden="true" tabindex="-1"></a>df1 <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span></span>
<span id="cb545-5"><a href="inferential-statistics-take-1.html#cb545-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Trial <span class="sc">==</span> <span class="dv">1</span>)</span></code></pre></div>
<p><strong>Numerator of F-statistic:</strong> When measuring any type of variance, you need to establish some point to measure that variability from. When looking at sample data, that point is the sample average. In our case, we’re trying to first measure the <em>variability between groups</em> and the way we decide to do this is by comparing the <strong>group averages</strong> <span class="math inline">\(\overline{x}_i\)</span> to the average of all measurements in our sample, called the <strong>grand average</strong> <span class="math inline">\(\overline{x}\)</span>. The grand average estimates the value all the group averages would be equal to if there were no differences between them. This brings us to our first component of ANOVA, the sum of squares between groups or SSG.</p>
<p><span class="math display">\[ \text{total variability between groups} = SSG = \sum_{i = 1}^k n_i \left( \overline{x}_i - \overline{x} \right)^2\]</span></p>
<p>Note that we weight the squared deviation by the group’s sample size <span class="math inline">\(n_i\)</span> since a large deviation from a small group should be weighted similarly to a small deviation from a large group. We have <span class="math inline">\(k\)</span> groups, so the average variability between groups (sometimes called <strong>mean square between groups</strong> or MSG) is</p>
<p><span class="math display">\[ \text{mean variability between groups} =  MSG =\frac{SSG}{k-1}\]</span></p>
<p>The code below makes these calculations for trial 0 and trial 1 from our motivating example.</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="inferential-statistics-take-1.html#cb546-1" aria-hidden="true" tabindex="-1"></a><span class="co">#find grand means, n, and k</span></span>
<span id="cb546-2"><a href="inferential-statistics-take-1.html#cb546-2" aria-hidden="true" tabindex="-1"></a>gm0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(df0<span class="sc">$</span>Outcome)</span>
<span id="cb546-3"><a href="inferential-statistics-take-1.html#cb546-3" aria-hidden="true" tabindex="-1"></a>gm1 <span class="ot">&lt;-</span> <span class="fu">mean</span>(df1<span class="sc">$</span>Outcome)</span>
<span id="cb546-4"><a href="inferential-statistics-take-1.html#cb546-4" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">dim</span>(df0)[<span class="dv">1</span>] <span class="co">#number of rows in df0 - the same for trial 1</span></span>
<span id="cb546-5"><a href="inferential-statistics-take-1.html#cb546-5" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">unique</span>(df0<span class="sc">$</span>Group)) <span class="co"># of groups in df0 - the same for trial 1</span></span>
<span id="cb546-6"><a href="inferential-statistics-take-1.html#cb546-6" aria-hidden="true" tabindex="-1"></a><span class="co">#create summary tables that find all of our needed summary statistics</span></span>
<span id="cb546-7"><a href="inferential-statistics-take-1.html#cb546-7" aria-hidden="true" tabindex="-1"></a>summary0 <span class="ot">&lt;-</span> df0 <span class="sc">%&gt;%</span></span>
<span id="cb546-8"><a href="inferential-statistics-take-1.html#cb546-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Group) <span class="sc">%&gt;%</span></span>
<span id="cb546-9"><a href="inferential-statistics-take-1.html#cb546-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb546-10"><a href="inferential-statistics-take-1.html#cb546-10" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">n</span>() </span>
<span id="cb546-11"><a href="inferential-statistics-take-1.html#cb546-11" aria-hidden="true" tabindex="-1"></a>    , <span class="at">xBar =</span> <span class="fu">mean</span>(Outcome)</span>
<span id="cb546-12"><a href="inferential-statistics-take-1.html#cb546-12" aria-hidden="true" tabindex="-1"></a>    , <span class="at">s =</span> <span class="fu">sd</span>(Outcome)</span>
<span id="cb546-13"><a href="inferential-statistics-take-1.html#cb546-13" aria-hidden="true" tabindex="-1"></a>    , <span class="at">s2 =</span> <span class="fu">var</span>(Outcome)</span>
<span id="cb546-14"><a href="inferential-statistics-take-1.html#cb546-14" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb546-15"><a href="inferential-statistics-take-1.html#cb546-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb546-16"><a href="inferential-statistics-take-1.html#cb546-16" aria-hidden="true" tabindex="-1"></a>summary0</span></code></pre></div>
<pre><code>## # A tibble: 3 × 5
##   Group     n  xBar     s    s2
##   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 A0       15  2.28  3.78  14.3
## 2 B0       15  1.02  3.67  13.5
## 3 C0       15  2.62  4.48  20.1</code></pre>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb548-1"><a href="inferential-statistics-take-1.html#cb548-1" aria-hidden="true" tabindex="-1"></a>summary1 <span class="ot">&lt;-</span> df1 <span class="sc">%&gt;%</span></span>
<span id="cb548-2"><a href="inferential-statistics-take-1.html#cb548-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Group) <span class="sc">%&gt;%</span></span>
<span id="cb548-3"><a href="inferential-statistics-take-1.html#cb548-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(</span>
<span id="cb548-4"><a href="inferential-statistics-take-1.html#cb548-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">n =</span> <span class="fu">n</span>() </span>
<span id="cb548-5"><a href="inferential-statistics-take-1.html#cb548-5" aria-hidden="true" tabindex="-1"></a>    , <span class="at">xBar =</span> <span class="fu">mean</span>(Outcome)</span>
<span id="cb548-6"><a href="inferential-statistics-take-1.html#cb548-6" aria-hidden="true" tabindex="-1"></a>    , <span class="at">s =</span> <span class="fu">sd</span>(Outcome)</span>
<span id="cb548-7"><a href="inferential-statistics-take-1.html#cb548-7" aria-hidden="true" tabindex="-1"></a>    , <span class="at">s2 =</span> <span class="fu">var</span>(Outcome)</span>
<span id="cb548-8"><a href="inferential-statistics-take-1.html#cb548-8" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb548-9"><a href="inferential-statistics-take-1.html#cb548-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb548-10"><a href="inferential-statistics-take-1.html#cb548-10" aria-hidden="true" tabindex="-1"></a>summary1</span></code></pre></div>
<pre><code>## # A tibble: 3 × 5
##   Group     n  xBar     s    s2
##   &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 A1       15 0.894 1.06  1.13 
## 2 B1       15 0.644 0.740 0.548
## 3 C1       15 1.83  1.03  1.07</code></pre>
<p>With these summary statistics in hand, finding the SSG and MSG is easy! We store the values for future use.</p>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb550-1"><a href="inferential-statistics-take-1.html#cb550-1" aria-hidden="true" tabindex="-1"></a><span class="co">#first find SSG</span></span>
<span id="cb550-2"><a href="inferential-statistics-take-1.html#cb550-2" aria-hidden="true" tabindex="-1"></a>SSG0 <span class="ot">&lt;-</span> <span class="fu">sum</span>( summary0<span class="sc">$</span>n<span class="sc">*</span>(summary0<span class="sc">$</span>xBar <span class="sc">-</span> gm0)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb550-3"><a href="inferential-statistics-take-1.html#cb550-3" aria-hidden="true" tabindex="-1"></a>SSG1 <span class="ot">&lt;-</span> <span class="fu">sum</span>( summary1<span class="sc">$</span>n<span class="sc">*</span>(summary1<span class="sc">$</span>xBar <span class="sc">-</span> gm1)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb550-4"><a href="inferential-statistics-take-1.html#cb550-4" aria-hidden="true" tabindex="-1"></a><span class="co">#print values</span></span>
<span id="cb550-5"><a href="inferential-statistics-take-1.html#cb550-5" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(SSG0, SSG1)</span></code></pre></div>
<pre><code>## [1] 21.35958 11.62261</code></pre>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="inferential-statistics-take-1.html#cb552-1" aria-hidden="true" tabindex="-1"></a><span class="co">#find MSG then print </span></span>
<span id="cb552-2"><a href="inferential-statistics-take-1.html#cb552-2" aria-hidden="true" tabindex="-1"></a>MSG0 <span class="ot">&lt;-</span> SSG0<span class="sc">/</span>(k<span class="dv">-1</span>)</span>
<span id="cb552-3"><a href="inferential-statistics-take-1.html#cb552-3" aria-hidden="true" tabindex="-1"></a>MSG1 <span class="ot">&lt;-</span> SSG1<span class="sc">/</span>(k<span class="dv">-1</span>)</span>
<span id="cb552-4"><a href="inferential-statistics-take-1.html#cb552-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(MSG0, MSG1)</span></code></pre></div>
<pre><code>## [1] 10.679792  5.811306</code></pre>
<p>So very roughly there is more variability between groups in trial 0 than trial 1. But this isn’t the whole story! We need to compare these quantities to the average variability within the groups.</p>
<p><strong>Denominator of F-statistic:</strong> This calculation is a bit easier to figure out since we already have a good way of measuring the variability within a group: simply that group’s sample variance! Thus,to calculate the total variability within groups we simply need to sum numerates of all the group’s variance. This is called the <strong>sum of squared error</strong>.</p>
<p><span class="math display">\[\text{total variability within groups} = SSE = \sum_{i = 1}^k (n_i - 1)s_i^2 \]</span></p>
<p>The **mean square error* or average variability within groups is then</p>
<p><span class="math display">\[ \text{mean variability within groups} = MSE = \frac{SSE}{n - k} \]</span></p>
<p>The denominator <span class="math inline">\(n-k\)</span> might be a surprise. Notice that the numerator, if you squint at it enough and think about the definition of sample variance, is really a sum of <span class="math inline">\(n\)</span> squared deviations from a mean. The calculation also involves <span class="math inline">\(k\)</span> distinct averages, so we have ‘used’ <span class="math inline">\(k\)</span> of our <span class="math inline">\(n\)</span> total degrees of freedom available, leaving <span class="math inline">\(n-k\)</span> degrees.</p>
<p>Once you’ve calculated the <span class="math inline">\(MSG\)</span> and the <span class="math inline">\(MSE\)</span> you have everything you for the <span class="math inline">\(F\)</span>-statistic:
<span class="math display">\[ F = \frac{MSG}{MSE}.\]</span></p>
<p>The code below uses our previous summary tables to calculate the SSE, MSE, and <span class="math inline">\(F\)</span> statistics for trials 0 and 1 from our motivating example. Notice that we calculated the variance and standard deviation in each group above.</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="inferential-statistics-take-1.html#cb554-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calc and store SSE first then print</span></span>
<span id="cb554-2"><a href="inferential-statistics-take-1.html#cb554-2" aria-hidden="true" tabindex="-1"></a>SSE0 <span class="ot">&lt;-</span> <span class="fu">sum</span>( (summary0<span class="sc">$</span>n<span class="sc">-</span> <span class="dv">1</span>)<span class="sc">*</span>summary0<span class="sc">$</span>s2 )</span>
<span id="cb554-3"><a href="inferential-statistics-take-1.html#cb554-3" aria-hidden="true" tabindex="-1"></a>SSE1 <span class="ot">&lt;-</span> <span class="fu">sum</span>( (summary1<span class="sc">$</span>n<span class="sc">-</span> <span class="dv">1</span>)<span class="sc">*</span>summary1<span class="sc">$</span>s2 )</span>
<span id="cb554-4"><a href="inferential-statistics-take-1.html#cb554-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(SSE0, SSE1)</span></code></pre></div>
<pre><code>## [1] 670.0917  38.4532</code></pre>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="inferential-statistics-take-1.html#cb556-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calc and store MSE</span></span>
<span id="cb556-2"><a href="inferential-statistics-take-1.html#cb556-2" aria-hidden="true" tabindex="-1"></a>MSE0 <span class="ot">&lt;-</span> SSE0<span class="sc">/</span>(n  <span class="sc">-</span> k)</span>
<span id="cb556-3"><a href="inferential-statistics-take-1.html#cb556-3" aria-hidden="true" tabindex="-1"></a>MSE1 <span class="ot">&lt;-</span> SSE1<span class="sc">/</span> (n<span class="sc">-</span>k)</span>
<span id="cb556-4"><a href="inferential-statistics-take-1.html#cb556-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(MSE0, MSE1)</span></code></pre></div>
<pre><code>## [1] 15.9545638  0.9155523</code></pre>
<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb558-1"><a href="inferential-statistics-take-1.html#cb558-1" aria-hidden="true" tabindex="-1"></a><span class="co"># F stats</span></span>
<span id="cb558-2"><a href="inferential-statistics-take-1.html#cb558-2" aria-hidden="true" tabindex="-1"></a>F0 <span class="ot">&lt;-</span> MSG0<span class="sc">/</span>MSE0</span>
<span id="cb558-3"><a href="inferential-statistics-take-1.html#cb558-3" aria-hidden="true" tabindex="-1"></a>F1 <span class="ot">&lt;-</span> MSG1<span class="sc">/</span>MSE1</span>
<span id="cb558-4"><a href="inferential-statistics-take-1.html#cb558-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(F0, F1)</span></code></pre></div>
<pre><code>## [1] 0.6693879 6.3473231</code></pre>
<p>We see from the last calculation what we expect: there is a lot more variation within the groups of trial 1 than of trial 0. The <span class="math inline">\(F\)</span>-statistic furthers this comparison: the variability between groups relative to the variability within groups is much more stark in trial 1 than trial 0.</p>
<p>Calculating a <span class="math inline">\(p\)</span>-value is final step in executing an ANOVA test. Recall that a <span class="math inline">\(p\)</span>-value is the probability of a sample at least as favorable for <span class="math inline">\(H_a\)</span> as our own, assuming <span class="math inline">\(H_0\)</span> is true. A sample that is more favorable for <span class="math inline">\(H_a\)</span> than our own would have more variability between groups relative to the variability within groups. Thus, the <span class="math inline">\(p\)</span>-value for an ANOVA test is <em>always</em> a one-sided upper tail area in the appropriate <span class="math inline">\(F\)</span>-distribution.</p>
<p>Recall from a previous section that an <span class="math inline">\(F\)</span>-distribution is the ratio of two <span class="math inline">\(\chi^2\)</span>-distributions. This implies we need to specify two degrees of freedom</p>
<ul>
<li><p><span class="math inline">\(df_1 = k-1\)</span> is the numerator degrees of freedom.</p></li>
<li><p><span class="math inline">\(df_2 = n - k\)</span> is the denominator degrees of freedom.</p></li>
</ul>
<p>to calculate a <span class="math inline">\(p\)</span>-value. Just as we use the <code>pnorm</code>, <code>pt</code>, and <code>pchisq</code> functions to find areas under the corresponding probability density function curves and thus calculate <span class="math inline">\(p\)</span>-values, we use the function <code>pf</code> when dealing with <span class="math inline">\(F\)</span>-distributions. Combining our comments above, the syntax for a <span class="math inline">\(p\)</span>-value for us is <code>pf(F, df1 = k -1 , df2 = n-k , lower.tail = FALSE)</code>. The <span class="math inline">\(p\)</span>-values for our two examples are calculated below.</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="inferential-statistics-take-1.html#cb560-1" aria-hidden="true" tabindex="-1"></a><span class="co">#calculate and store p-values</span></span>
<span id="cb560-2"><a href="inferential-statistics-take-1.html#cb560-2" aria-hidden="true" tabindex="-1"></a>p0 <span class="ot">&lt;-</span> <span class="fu">pf</span>(F0, <span class="at">df1 =</span> k <span class="sc">-</span><span class="dv">1</span>, <span class="at">df2 =</span> n <span class="sc">-</span>k, <span class="at">lower.tail =</span>  <span class="cn">FALSE</span>)</span>
<span id="cb560-3"><a href="inferential-statistics-take-1.html#cb560-3" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">pf</span>(F1, <span class="at">df1 =</span> k <span class="sc">-</span><span class="dv">1</span>, <span class="at">df2 =</span> n <span class="sc">-</span>k, <span class="at">lower.tail =</span>  <span class="cn">FALSE</span>)</span>
<span id="cb560-4"><a href="inferential-statistics-take-1.html#cb560-4" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(p0, p1)</span></code></pre></div>
<pre><code>## [1] 0.517399088 0.003902959</code></pre>
<p>We can finally conclude our hypothesis test for trial 0 and trial 1! We’ll use a significance level for trial 0 and trial 1.</p>
<ul>
<li><p><strong>Trial 0 Conclusion:</strong> We fail to reject the null since <span class="math inline">\(p \approx .52 &gt; .05\)</span>. The data do not provide evidence of any difference between the group averages. Alternatively, the data do not provide evidence of an association between Group and Outcome.</p></li>
<li><p><strong>Trial 1 Conclusion:</strong> We reject the null and accept the research since $ p .003 &lt; .05$. The data provide strong evidence that at least one group mean differs from the others. Alternatively, the data provide strong evidence of an association between group membership and outcome.</p></li>
</ul>
<p>Notice that these conclusions match what we expected from our original plot! This was by design, of course, to help us build our intuition behind ANOVA.</p>
<p>In practice, you will almost never go through an ANOVA calculation in the way that we have here. You’ll use software (hopefully R) to automate all of these calculations for you and the ANOVA section of the next chapter describes exactly how to do that.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distribution-calculations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inferential-statistics-take-2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/05-inference_pt1.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
