[["index.html", "R and RStudio for STAT216 Chapter 1 About 1.1 Organization 1.2 Getting started 1.3 Code in this book 1.4 Help 1.5 Debugging 1.6 Other resources", " R and RStudio for STAT216 Charles Katerba Last update: 2023-03-14 Chapter 1 About This book is intended to be a quick and dirty introduction to R (a statistical programming language) and RStudio (an integrated development environment, or IDE, for R) for students in the introductory statistics class at Flathead Valley Community College. This book aims to introduce you to all of the features youll need to use in R to be successful in the course. Moreover, we hope that it will help you develop the tools you need to use R in your future endeavors. This book is by no means a complete guide to everything one can do in R, but the aim is to cut down on content and theory in the name of practicality. This class will provide many students with their first interaction with computer programming. The coding required is not extensive and the coding-based activities are heavily scaffolded. The point of including R in this course is not to introduce another layer of difficulty; the opposite is true in fact! The goal is to give you a glimpse at how programming can make your life easier, at least when it comes to statistics and data science. In the process, you will hopefully add another useful and practical tool to your tool belt. Computers are not exiting our lives any time soon and being somewhat familiar with a little programming will only be an advantage. Some might say such familiarity will become essential. 1.1 Organization The chapters of this book will be organized to follow the flow of STAT216 so that reading the book sequentially teaches you the tools as you need them in class. This book will almost certainly be a work in progress, so if theres something that youd like to see or if you think something is missing, dont hesitate to reach out to the author (or your instructor if they are different people). 1.2 Getting started The programming language R is somewhat old, appearing first in 1993. That said, it is still quite useful and will remain so for a long time coming since it is an open-source programming language and software environment. This means that the development and maintenance of the language and software are in the hands of all users, not just a small, select group of developers. Fear not! You will not have to do any developing. For our purposes, the open-source nature of R implies that the community of users will continue to grow the features and power of the language to meet the needs of the times. There are two mains ways to use R: through a cloud computing service or through an installation directly to your computer. The easiest way to start is through a cloud computing service. In both cases, we recommend you use the IDE RStudio as it makes using R more intuitive and straightforward. Below we describe how to use RStudio on the cloud and on your own computer. RStudio (which will rebrand as Posit in October 2022) hosts its own cloud computing service called RStudio Cloud. To get started: Click the previous link to access their website. Create a new account, for free. Your free account gives 25 project hours per month. This should be enough for your work in STAT216, but theres a chance youll need more. There is a small fee in this case. Click Start a new project in the top right corner of your browser. This will open an RStudio session for you. Change the name of your project to something evocative to tell your future self what you were working on. Some ideas: STAT216 HW, STAT216 Activities, etc. You are now ready to rip! You can also download a desktop version of RStudio. Accessing R in this way is slightly more involved, but offers more flexibility and no computational limitations. It is also free. To install R to your own computer: First, you must install R. This link takes you to CRAN, the Comprehensive R Archive Network, where you can download the latest version of R. Be sure to select the download appropriate to your operating system and follow the instructions provided at the link above. Next, download and install RStudio Desktop following the directions at the link. Again, be sure to select the download appropriate to your operating sytem. You can now open RStudio by finding the application on your computer. You, too, are now ready to rip! No matter what version of RStudio you are using, you should now see a window that looks something like this on your computer. An RStudio terminal. Before proceeding, notice that your RStudio session has 3 windows open. The console: this is the most interactive window in your session. You can run quick calculations here. For example, type 2 + 2 then hit enter. Files/plots window: The files tab shows all of the files present in your current working directory. We dont need to talk much about file management here and will cover it as needed in the future. When you make quick plots, they will appear under the plot tab. For example, evaluating plot(rnorm(10), rnorm(10)) in the console will produce a plot similar to the one above in the Plots panel. This little bit of code generates 2 collections of 10 random numbers and makes a scatter plot of the results. Environment window: This window displays the names of the objects you have stored in your computers memory. As an example, if you evaluate x &lt;- rnorm(10), you will see the value x appear in the Environment tab. It is good to check the values you have stored when youre running into problems with your code. At this point, you can skim the rest of this chapter or move on to RStudio basics. 1.3 Code in this book You should be able to copy and paste all code chunks in this book into your own RStudio terminal and reproduce the results you see in the book. All data used in this book will be sourced from base R, other commonly used packages, or appropriate links will be given for downloaded data sets. The author will endeavor to be clear about what packages are loaded and where data sets are coming from throughout the book. 1.4 Help There are many avenues for finding help using R. Your instructor, the STEM tutoring center, and the internet are great places to start. For the latter, you have to carefully phrase what youre asking, but if you do so, you can usually find great results with some clear examples. R also has a lot of built in help documentation. You can access this documentation using a ? before the name of the function/object that youre intereted in. For example, R has a function called prop.test(...) that well learn how to use in this class. To see Rs documententation for prop.test(...), simply run ?prop.test in the console. The examples are usually helpful, but sometimes can create more confusion. In any case, this is a great place to start when you feel confused about a particular function or object. If you bring up the documentation for an in-built data set, it will describe what the variables of that data set are. For instance, try running ?mtcars in the console 1.5 Debugging In this class, you will very likely come to a point where some code youve written will not run. This experience can be very frustrating. Remember, it happens to everyone and it frustrates everyone. Hadley Wickham provides a a detailed guide on how to debug code in this book Advanced R, but here is my take with a few additions. Double and triple check your syntax. New coders are often stymied by small syntax mistakes. For instance, if you want to use a function called f in R, but you type F instead, the machine wont do the right thing. Similarly, be careful about commas; for example, elements of lists need to be separated by commas and they can be easy to forget. There are a too many syntax errors one can make to list here. Search the internet for the error codes youre seeing. You can simply copy and paste them into your web browser. Most likely someone before you has made the same mistake and asked the same question. Figure out exactly where the error is. This is harder than it seems like it should be sometimes. In short pieces of code it is usually easy to identify, but when you become more advanced and write longer code the problem can get tricky. Reach out to your instructor and/or the STEM tutoring center at FVCC. These people are invaluable resources. Be sure to not only share the error code, but the entire chunk of code that you think is problematic. Even better, share all of your code. The website CrossValidated is a stack exchange where users can ask and answers questions about statistics, especially R coding. You may be able to get questions answered here, but be warned: the community is very picky about how you ask questions and about the type of questions you ask. 1.6 Other resources As mentioned above, these notes are only going to give you a brief glimpse of R and RStudio. The resources linked below will either provide a much more in-depth look at R and its applications or provide a very quick summary of certain packages and/or features. RStudio cheatsheets. These are invaluable as they provide a lot of information in a small amount of space. A list of helpful R functions. Thomas Scofield made a super consice list of commonly used base R functions and provided examples on how to use them. R for Data Science. As the title suggests, this book focuses more on data science than on statistics, so it spends more time on data visualization, programming, data wrangling, and modeling. Advanced R. This book goes into the more nitty gritty details of programming in R. In writing this book the author has referenced many of the works above along with a collection of other resources. See the bibliography for a somewhat complete list. "],["rstudio-basics.html", "Chapter 2 RStudio basics 2.1 A big calculator 2.2 Naming things 2.3 Lists and vectors 2.4 Packages 2.5 Data frames and tibbles 2.6 R Markdown documents", " Chapter 2 RStudio basics At this point you should have an operational RStudio terminal at your disposal. If not, go back to Getting started. This chapter will focus some of the most basic tools and skills youll need to successfully use RStudio. Well start with using R as a calculator, learn about naming and storing values/objects, then progress to loading and using data frames (the bread and butter of R in some sense), and finish with R Markdown documents, which allow you to create fully reproducible documents that contain a mixture of text and code. Fun fact: these notes were written in a collection of RMarkdown documents and compiled with a package called bookdown. As you proceed, we encourage you to try to run any code you see in this book and to play around and do things on your own. The best way to learn how to code is to experiment. The immediate feedback of running some successful code can be quite gratifying! 2.1 A big calculator At its most basic, R is a big, fancy, and initially cumbersome calculator. Think of any calculation youd like to do and R can likely do it. R can handle more than just numbers! For our purposes, well need numbers, strings, and logicals. 2.1.1 Numbers R has three atomic types of numbers: real, integer, and complex, but we will only need to think of real numbers. You can do arithmetic and evaluate the standard elementary functions with numbers as you would expect. Addition, multiplication, subtraction, and division are +, *, -, and / respectively. For example: 2 + 2 ## [1] 4 3 * 1.5 ## [1] 4.5 9 - 4 ## [1] 5 5/3 ## [1] 1.666667 Exponentiation is either ^ or **, the latter is old school. You can raise a number to a power of 10 using scientific notation, for example 1.2e4 is 12000. 2^3 ## [1] 8 2**3 ## [1] 8 1.2e4 ## [1] 12000 All of your favorite elementary functions are built into R. Note that the log function is base \\(e\\), not base 10. The example below shows how to change base. exp(2) # e^2 ## [1] 7.389056 sin(pi/2) # note the trig functions are in radians ## [1] 1 log(10) # natural log of 10 ## [1] 2.302585 log(100, 10) # the log of 100, base 10 ## [1] 2 Reminder! Be careful and mindful about the order of operations; computers and calculators read your input very literally. For instance, \\(6 \\div 2(1 + 2)\\), is an annoying statement with respect to the order of operations. R will evaluate this expression correctly if you type 6/2*(1+2). Make sure you can evaluate it correctly as well so that you input what you mean. 2.1.2 Strings We want to be able to analyze both numerical and categorical variables in R. For this reason, along with many others, your can use and manipulate strings of character in R. We wont be doing much more than using strings for values of categorical variables, so we wont go into too much detail. To define a string, simply put the expression in question in quotation marks. &quot;R might be cool? I don&#39;t know.&quot; Base R has some handy functions for manipulating strings. We may not need all of them, but theyre good to be aware of. substr(string, start=n1, stop=n2) will return a subset of your string starting at the n1th character, ending at the n2 character. substr(&quot;R might be cool? I don&#39;t know.&quot;, start = 18, stop = 30) ## [1] &quot;I don&#39;t know.&quot; nchar(string) counts the number of characters in a string. nchar(&quot;R might be cool? I don&#39;t know.&quot;) ## [1] 30 toupper/tolower(string) converts all letters to either upper or lower case. toupper(&quot;R might be cool? I don&#39;t know.&quot;) ## [1] &quot;R MIGHT BE COOL? I DON&#39;T KNOW.&quot; paste(..., sep = \" \") will concatenate a collection of strings, separated by a space. You can change what separates the strings. This function is most useful when you have to repeatedly concatenate some strings. paste(&quot;R might be cool? I don&#39;t know.&quot;, &quot;Yes, it is!&quot;) ## [1] &quot;R might be cool? I don&#39;t know. Yes, it is!&quot; You can do a lot more with strings, but things start to get more complicated quickly. The package stringr has some really handy functions, but are unnecessary for this course. 2.1.3 Logicals and Logical operators Throughout this class you will need to compare various objects in R using standard logical operators like equals (==), less than &lt;, greater than or equal to &gt;= etc. When you compare objects using these operators, R returns a new type of object called a logical. Logicals are just TRUE and FALSE. You can check equality of numbers and strings using ==. To check if two objects are unequal, use !=. 4 == 5 ## [1] FALSE 2^3 == 8 ## [1] TRUE &quot;cat&quot; == &quot;dog&quot; ## [1] FALSE &quot;dog&quot; != &quot;cat&quot; ## [1] TRUE You can compare numbers with your favorite inequalities (&lt;, &gt;, &lt;=, &gt;=) as well. More interestingly, you can also compare strings with inequalities. R defaults to lexicographic (ie dictionary) ordering of strings. 10 &lt; 9 ## [1] FALSE exp(3) &gt;= exp(2) ## [1] TRUE &quot;a&quot; &lt; &quot;b&quot; ## [1] TRUE &quot;aa&quot; &gt; &quot;ab&quot; ## [1] FALSE For convenience, TRUE sometimes behaves like 1 and FALSE like 0. In particular, you can perform arithmetic with logicals. This makes counting the number of matches you have quite easy in some situations, as well see. TRUE + TRUE ## [1] 2 TRUE*FALSE ## [1] 0 Note: R may occasionally exhibit behavior that you find goofy when comparing objects. This is usually because of coercion. When comparing objects, R will try to make the objects into the same type, if possible. We dont want to get bogged down in these details, but it is something to be aware of. The following example illustrates coercion when trying to compare a string to a number. &quot;1&quot; == 1 #R converts the string to 1, as a number, then compares. ## [1] TRUE &quot;01&quot; == 1 # R leaves the string as is, so the objects are unequal. ## [1] FALSE Finally, you can combine logical into compound expressions using the operators AND (in R AND is &amp;) and OR (in R |). The expression P &amp; Q is TRUE if and only if P and Q are both true. The expression P | Q yields TRUEif and only if at least one of P or Q is true. Lets look at some examples in code. (5 &gt; 2) &amp; (4 == 2^2) # both statements are true, so compound is as well ## [1] TRUE (5 &lt; 2) &amp; (4 == 2^2) # first statement false, so compound is false ## [1] FALSE (5 &lt; 2) | (4 == 2^2) # second statement is true, so compound is true ## [1] TRUE (5 &lt; 2) | (4 != 2^2) # both false, so compound is false ## [1] FALSE This might seem odd right now, but this simple feature is very helpful when identifying subsets of a dataset with certain properties as well see later on in these notes. 2.2 Naming things Now that we know what types of objects we can use in R, we can talk about naming objects and storing them in your computers memory so that you can reference them easily later. You can name objects in two ways, either with the classic &lt;- or with =. These two methods of naming objects technically do different things, but the difference is subtle enough that we wont worry about it. Note that when you name an object, it will appear in your Environment window in RStudio. The example below will illustrates the basics of naming objects. x &lt;- 1.123e6 y &lt;- 5 x ## [1] 1123000 y ## [1] 5 x/y ## [1] 224600 Once youve named an object and stored it in memory, you can use its name anywhere else to call the value associated to the name. Read the following only if youve had some programming experience. If not, skip to the next section. R makes copies of objects in a different way from, say, Python (and many other languages).The difference lies in deep vs shallow copies. Basically, a deep copy creates an entirely new object in memory, but a shallow copy merely points to the original objects position in memory. R will actually create a shallow copy until the original object is modified, then it will create a deep copy in order to save on memory. To illustrate this point, if you were to run the following code in Python: x = [1,2,3] y = x #shallow copy created here x.append(5) y it would print [1,2,3,5]. However, the equivalent code in R is x = 1:3 y = x # shallow copy of x created here x &lt;- append(x, 5) #but deep copy of original x created here for y y ## [1] 1 2 3 Its good to know how R is behaving; deep copies can end up using more memory, but that wont be an issue in this class with modern computing power. 2.3 Lists and vectors One of the many advantages to using a computer for mathematics, statistics, and data analysis is their ability to store and organize large chunks of information. A list/vector is the most basic way to start organizing data in R. This section will guide you through creating and manipulating lists. 2.3.1 Creating lists The most basic tool for creating a list in R is the function c(..). According to the documentation c(...) is a generic function which combines its arguments. All that to say that it is a function that creates a list. Heres a few examples. You can make lists with any types of data in them. x &lt;- c(1,2,3) y &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;rabbit&quot;) You can combine lists easily with the c(...) function as well, but notice in the example below that something funny happens. z &lt;- c(x, y) z ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;cat&quot; &quot;dog&quot; &quot;rabbit&quot; R coerced the numbers from the list x into characters/strings because y contains strings. This is a subtlety that can sometimes cause headaches. If your lists all contain the same types of objects, you have nothing to worry about. Throughout this course youll often want to know how many objects are in a list and most times we dont want to manually count them. The length function does this for us. The examples above are easy to count by hand, but as a quick check: length(x) ## [1] 3 length(y) ## [1] 3 length(z) ## [1] 6 2.3.2 Subsetting lists After defining a list, sometimes we want to access only certain elements of that list; this is called subsetting. Well spend more time on fancier subsetting methods later in this course, but this will get us started with the basics. Technical note: R indexing starts at 1 and not 0, a difference between R and some other programming languages like python. To get started, well create a long list of randomly generated numbers. rando &lt;- runif(1000, min = 0, max = 100) length(rando) ## [1] 1000 This list has 1000 elements in it. We can find the 497th element using rando[497] ## [1] 25.83411 and in general, use listName[number]. We can also easily select ranges of indicies. For instance, suppose we want to know what the first 10 values in our list are. rando[1:10] ## [1] 7.602391 64.377289 96.515758 26.878043 71.645381 30.643242 2.955113 4.919082 8.650007 47.620461 Finally, we can pick out specific elements from our big list using a smaller list of our desired indices. For example, if we wanted the entries number 2,3,5,7,11, and 13 from our list, we could use the following. indicies &lt;- c(2,3,5,7,11,13) rando[indicies] ## [1] 64.377289 96.515758 71.645381 2.955113 45.281032 70.976481 2.3.3 Special lists The last section taught us how to make the most basic type of list. While useful, making lists in this way can be tedious because it requires one to type in all values. Often out in the wild, you will want to create lists that follow specific patterns. For instance: the letters c through p; a list of the integers from 1 to 100; a list of numbers from -10 to 10, spaced by .01; the letters d, g, and z repeated 1000 times each. It is easy to construct these types of lists in R with minimal typing. Lists of integers: to make a list of sequential integers in R, use the format start : end, for instance 2:5 will return the list 2,3,4,5. x &lt;- -8:12 x ## [1] -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 length(x) ## [1] 21 Lists of sequential numbers that are not integers: The function seq produces sequences of numbers that follow a particular pattern. The general syntax is seq( from = STARTING VALUE, to = ENDING VALUE, by = SPACE BETWEEN ENTRIES) So if we wanted a list of all numbers from -10 to 10, spaced by .01, wed use foo &lt;- seq(from = -10, to = 10, by = .01) We can access the first 5 elements of this list and count the number of elements in our list foo with the following. foo[1:5] ## [1] -10.00 -9.99 -9.98 -9.97 -9.96 length(foo) ## [1] 2001 Lists of sequential letters: R has two in-built lists of letters called letters and LETTERS. No surprise, one of them is a list of lower case letters, the other upper case. letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; &quot;w&quot; &quot;x&quot; &quot;y&quot; ## [26] &quot;z&quot; To select the 4th through 11th lowercase letters of the alphabet, use the following syntax (which will be explained in a subsequent section). letters[4:11] ## [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; Lists with a repeating patterns: If youre performing an experiment or reformatting a data set, you may want to create a list with a repeated pattern. For example, suppose you have an experiment with 1000 people in the treatment group and 1000 people in the control group. The group a research subject is in is a variable, so should be a column in a data set. If the outcome of the experiment will be recorded in the order treatment, control, treatment, control, etc. for all 2000 participants, we would like to easily make a list of length 2000 following this pattern. The function rep( x, times = n) will do exactly this for us! In this case x is a list and times specifies the number of times youd like that list to be repeated. As an example, to make the list described above, we would use the commands vals &lt;- c( &quot;treatment&quot;, &quot;control&quot;) group &lt;- rep( vals, times = 1000 ) To ensure this does what we want, lets look at the length of group and at the first 10 elements. length(group) ## [1] 2000 group[1:10] ## [1] &quot;treatment&quot; &quot;control&quot; &quot;treatment&quot; &quot;control&quot; &quot;treatment&quot; &quot;control&quot; &quot;treatment&quot; &quot;control&quot; ## [9] &quot;treatment&quot; &quot;control&quot; Success! 2.3.4 List arithmetic Lists are great for storing information, but the real power comes from being able to manipulate them. You can weak lists in a few ways: using summary functions, which well cover in the next chapter, and using usual arithmetic. Lets start by defining 3 lists. x &lt;- 1:3 y &lt;- c(9, 18, 27) z &lt;- 10:15 #note length(z) = 6 Well start by adding, subtracting, multiplying, dividing lists, and exponentiate lists. We can also scale all values in a list by a constant and raise all elements to a certain power (note that R performs all of these operations component wise, so it may not be what youd expect if youve had some experience with matrices). x + y ## [1] 10 20 30 x - y ## [1] -8 -16 -24 x*y ## [1] 9 36 81 y/x ## [1] 9 9 9 y^x ## [1] 9 324 19683 .5*x ## [1] 0.5 1.0 1.5 x^2 ## [1] 1 4 9 Many functions in R are vectorized, meaning that if you input a list, the function will be applied to every element of the list. For example: sin(x) ## [1] 0.8414710 0.9092974 0.1411200 log(y, 3) #log base 3 ## [1] 2.00000 2.63093 3.00000 We need to be careful when doing arithmetic with lists. If were not careful, seemingly mysterious behavior can crop up if the lists were working with have different length. For instance, adding the lists x and z above doesnt seem to make sense since they have different lengths. But running the following returns a list with length 6. x + z ## [1] 11 13 15 14 16 18 So whats going on? R will repeat the values from the shorter list until the length matches the longer one. In the example above, were really getting the same result as rep(x, 2) + z ## [1] 11 13 15 14 16 18 This feature can be annoying and confusing if youre not aware of it, but helpful and powerful if you are. The next silly example will hopefully illustrate this point. Suppose we want to generate a list of random numbers such that the odd numbers are between -1 and 0, but the even numbers are between 1 and 2. Heres our strategy: first, generate 10 random numbers between 0 and 1; next, add one to all the even indexed entries so that their possible range of values goes from 0 to 1 to 1 to 2; finally, multiply the odd entries by -1. In code, this looks like rando &lt;- runif(10, min = 0, max = 1) rando &lt;- c(0,1) + rando rando &lt;- c(-1,1)*rando rando ## [1] -0.65269685 1.27378410 -0.99526780 1.67204649 -0.34310413 1.33721215 -0.01131421 1.21188912 ## [9] -0.83996405 1.74160449 A more practical application of this vectorization feature is to use logical operators and use them to help subset lists. As a basic example, R will compare lists component wise or to a particular value. x &lt; y ## [1] TRUE TRUE TRUE x &lt; 2 ## [1] TRUE FALSE FALSE Remember that we can combine logicals using &amp; and | (AND and OR). This gives us a way to check more complicated conditions on values. The list rando from above should have numbers between -1 and 0 in the odd entries and between 1 and 2 in the even. Lets check that. #subsetting only odd indices (rando &lt;= 0 &amp; rando &gt;= -1)[seq(from = 1, to = 10, by = 2)] ## [1] TRUE TRUE TRUE TRUE TRUE #subsetting only even indices (rando &gt;=1 &amp; rando &lt;= 2)[seq(from = 2, to = 10, by = 2)] ## [1] TRUE TRUE TRUE TRUE TRUE So the even and odd index entries of rando have the properties we wanted them to! Yahoo. Hopefully you are starting to see how these simple functions and features can be combined and layered into much more sophisticated gadgets. 2.4 Packages This short section changes gears. Recall that R and RStudio are both open-source projects, so much of the development lies in the hands of the community of users. There are many features and functions built into R, but youll occasionally run into things that are not innate to it. When this happens, you have two options: you can either program the feature that you want on your own (which is hard), or try to figure out if someone else has already programmed the feature youre interested in. Very often youll find that they have and typically these features will be available in an R package. Packages are simply extensions of the base R programming language; they typically contain code, data, and documentation. In this class, well use only a handful of packages. Some of them are: tidyverse: actually a collection of packages that utilize the same underlying syntax, grammar, etc. In principle, the tidyverse makes data science more intuitive, cleaner, and faster than base R. openintro: a package that contains many of the data sets in our textbook, OpenIntro Statistics. infer: a package that implements the tidyverse syntax and grammar for statistical inference. BDSA: a package that has a handful of data sets and a few functions that will be useful. This package is associated with the book Basic Statistics and Data Analysis by Larry J. Kitchens. There are thousands of R packages available to help you accomplish almost any task you can imagine. Some fun examples: spotifyr: a package to streamline accessing Spotifys API (application programming interface), so you can pull data on song, artist, and playlist information. There are similar packages for many of your favorite websites and applications. caret: a consolidated machine learning library that makes creating, analyzing, and utilizing machine learning algorithms a breeze. leaflet: a package for creating interactive maps in R. You have very likely interacted with a map created in leaflet! This link will show you some code to create a heatmap from Strava data using leaflet. bookdown: a package for authoring books in RStudio. The book you are reading was written using bookdown! You probably get the point: once you start using packages, the capabilities of R and RStudio are limitless. But how do you actually use them? First, you have to install the package, then load it into your current working environment. Installing packages: to install a package, use the function install.packages(...) in the console of RStudio. Be sure to put the name of the package in quotation marks. For example, to install the tidyverse package, execute the following code in the console: install.packages(&quot;tidyverse&quot;) Once youve install a package, you have to load and attach the package to your current working environment. Use the library(...) function to do this. After installing the package, you no longer need to call it with quotation marks. Thus, to attach the tidyverse package, execute the following in your Rstudio console: library(tidyverse) Caution 1: You have to attach the packages youd like to use every time you start a new RStudio session! Alternatively, you have to load them at the beginning of your R script or R Markdown document. More on this in a coming section. Caution 2: In RStudio Cloud, packages need to be installed in every project you create. This means if you start a new project for every activity, you must re-install (and of course load) all of the packages required for the project. To avoid this difficulty, I recommend starting one project for activities and creating a new folder in this project for each individual directory. 2.5 Data frames and tibbles Data frames are the bread and butter of R and statistics in general. At their most basic, they are simply arrays of objects be they numbers, strings, or logicals with named columns (and sometimes named rows). If youre recording data, remember: Each column of a data frame represents a variable. Each row of a data frame represents an observation, so you record a value of each variable for each observation. There are two primary types of data frames in R, data.frames and tibbles. The former is an object from base R and the latter is a data frame class from the tidyverse package. There are differences between these two classes of data frames, but we wont worry about them too much. In this class, because they display in a nicer way and some operations are performed more quickly with them, we will primarily use tibbles. For more information on tibble vs data.frame, evaluate ?tibble in the console after loading the tidyverse package. 2.5.1 Creating data frames 2.5.1.1 From packages Many R packages contain test data sets that can be stored in your computers memory and used for exploration and demonstration of particular ideas. To use these data sets, you can either make a copy of them using the &lt;- operator or load them using the data function. As an example, tidyverse has a dataset labeled us_rent_income. You can load use this data set by calling df &lt;- us_rent_income #or data(&quot;us_rent_income&quot;) #note the quotation marks! These built-in data sets are great for examples, but they dont help you analyze your own data. Next, well learn how to input our own data into R. 2.5.1.2 Reading files The most common way to load your own data into R is by reading the data in from a file (eg csv, excel, pdf, etc files) stored on your computer. These other file formats are much easier and more flexible for entering data. Loading and analyzing the data in R also helps prevent the user from accidentally changing values in their data during their analysis; changing values in data sets in R is very difficult to do accidentally by design). We cant and wont try to cover all of the possible ways you can load a data set into R from a file on your computer, but we will cover some of the most common techniques. In general, you can figure out how to load a file of a particular type into R with a simple internet search. Below we list some functions and their use. In each case, the first input is file, which is possibly a the path of the file youre trying to load. Its easiest to put the file in the same directory/folder that youre working in. read.csv(file, ...): this function reads a CSV (comma separated value) file and loads the data into R. Note that sometimes you have to tweak the arguments of this function to load the data set in exactly the way you want. read_excel(file, ...): from the readxl package does what youd expect, reading and loading Microsoft Excel files. As above, you might have to play around with this to load the data appropriately. pdf_text(file, ...): from the pdftools reads all of the text from each page of a PDF document. This is handy, but requires quite a bit of tidying and is not something well cover in this course. The basic take away is that you can read data from PDF files into R. download.file(url, destfile, ...): this function downloads a file from from the specified url and stores it as destfile. The following example illustrates how you might use a combination of these functions in practice. The code downloads a data set from Montana State Universitys STAT216 GitHub page, loads it into R, then displays the results. The data set lists a handful of countries, the countrys max speed limit, and the average number of highway fatalities annually. #first we store the url and file name as strings url &lt;- &quot;https://raw.githubusercontent.com/MTstateIntroStats/IntroStatActivities/master/data/Highway.csv&quot; fileName &lt;- &quot;Highway.csv&quot; #the next command downloads the file if you haven&#39;t downloaded it already if(!file.exists(fileName)){ download.file(url, fileName) } #now load the file into R df &lt;- read.csv(fileName) #print data frame df ## limit death country ## 1 55 3.0 Norway ## 2 55 3.3 United States ## 3 55 3.4 Finland ## 4 70 3.5 Britain ## 5 55 4.1 Denmark ## 6 60 4.3 Canada ## 7 55 4.7 Japan ## 8 60 4.9 Australia ## 9 60 5.1 Netherlands ## 10 75 6.1 Italy 2.5.1.3 Manual entry When you are simulating data, working on homework, or simply have a small data set, it can be convenient to manually type your data into R. To make a data frame, you simply list the columns as named vectors. We will go through three examples of of how you can do this. First off, suppose you wanted to enter the following table as a data frame into R. animal weight length cat 5 24 dog 45 35 rabbit 2 10 turtle 15 15 To enter this table, we would use the following: df &lt;- tibble( animal = c(&quot;cat&quot;, &quot;dog&quot;, &quot;rabbit&quot;, &quot;turtle&quot;), weight = c(5, 45, 2, 15), length = c(24, 35, 10, 15)) df ## # A tibble: 4 x 3 ## animal weight length ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cat 5 24 ## 2 dog 45 35 ## 3 rabbit 2 10 ## 4 turtle 15 15 Now suppose we want to make a data set for the points on the line \\(y = 2x + 3 + \\text{some random noise}\\) with integer \\(x\\) values between 1 and 10. We can do this easily! df &lt;- tibble(x = 1:10, y = 2*x + 3 + rnorm(10)) df ## # A tibble: 10 x 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 4.97 ## 2 2 4.92 ## 3 3 8.54 ## 4 4 11.2 ## 5 5 10.9 ## 6 6 15.5 ## 7 7 17.9 ## 8 8 19.7 ## 9 9 20.3 ## 10 10 23.5 As a final example, lets start with a table that doesnt follow the convention of variables in columns, observations in rows. For this example, suppose you recorded the weight in pounds of 5 turtles, cats, and dogs as follows: Turtle Cat Dog 15 2 26 10 13 60 15 12 60 12 8 22 12 2 59 In this case, notice that we only actually have two variables around, not three: species of pet and pet weight. When dealing with categorical variables, it is common to see data organized as above, but be aware that this isnt the most effective way to organize your information for the purpose of analysis. Since the data set has only two variables, our data frame in R should only have two columns. We could enter this data as follows: df &lt;- tibble( species = c( rep(&quot;turtle&quot;, 5), rep(&quot;cat&quot;, 5), rep(&quot;dog&quot;, 5)), weight = c(15, 10, 15, 12, 12,2, 13, 12, 8 , 2,26, 60, 60, 22, 59)) df ## # A tibble: 15 x 2 ## species weight ## &lt;chr&gt; &lt;dbl&gt; ## 1 turtle 15 ## 2 turtle 10 ## 3 turtle 15 ## 4 turtle 12 ## 5 turtle 12 ## 6 cat 2 ## 7 cat 13 ## 8 cat 12 ## 9 cat 8 ## 10 cat 2 ## 11 dog 26 ## 12 dog 60 ## 13 dog 60 ## 14 dog 22 ## 15 dog 59 This format might not be as easy to see, but it will be substantially easier to analyze in R. 2.5.2 Using data frames You can think of the rest of this book as a quick summary on ways to use data frames, but this section will touch on a few basics we need before we can do anything else. In this section, well use the starwars data set from the tidyverse package. First off, once youve stored a data set, it is good to look at it to see what youre working with. There are a few different options that all have their places. view(df): from the tidyverse package and opens a spreadsheet-like view of your dataset. You cannot edit the data, only see it. head(df): returns the first 6 rows of a data set and may exclude some columns from view. tail(df): returns the last 6 rows of a data set and may exclude some of the columns from view. glimpse(df): from the tidyverse package and prints a few entries from every column of your data set. Lets see how all of these (except view) behave on the starwars data set. Try view(starwars) out on your own. head(starwars) ## # A tibble: 6 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species films ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; ## 1 Luke Skywa~ 172 77 blond fair blue 19 male mascu~ Tatooine Human &lt;chr&gt; ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu~ Tatooine Droid &lt;chr&gt; ## 3 R2-D2 96 32 &lt;NA&gt; white, bl~ red 33 none mascu~ Naboo Droid &lt;chr&gt; ## 4 Darth Vader 202 136 none white yellow 41.9 male mascu~ Tatooine Human &lt;chr&gt; ## 5 Leia Organa 150 49 brown light brown 19 fema~ femin~ Alderaan Human &lt;chr&gt; ## 6 Owen Lars 178 120 brown, gr~ light blue 52 male mascu~ Tatooine Human &lt;chr&gt; ## # ... with 2 more variables: vehicles &lt;list&gt;, starships &lt;list&gt; tail(starwars) ## # A tibble: 6 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species films ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lis&gt; ## 1 Finn NA NA black dark dark NA male mascu~ &lt;NA&gt; Human &lt;chr&gt; ## 2 Rey NA NA brown light hazel NA fema~ femin~ &lt;NA&gt; Human &lt;chr&gt; ## 3 Poe Dameron NA NA brown light brown NA male mascu~ &lt;NA&gt; Human &lt;chr&gt; ## 4 BB8 NA NA none none black NA none mascu~ &lt;NA&gt; Droid &lt;chr&gt; ## 5 Captain Ph~ NA NA unknown unknown unknown NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;chr&gt; ## 6 Padm√© Amid~ 165 45 brown light brown 46 fema~ femin~ Naboo Human &lt;chr&gt; ## # ... with 2 more variables: vehicles &lt;list&gt;, starships &lt;list&gt; glimpse(starwars) ## Rows: 87 ## Columns: 14 ## $ name &lt;chr&gt; &quot;Luke Skywalker&quot;, &quot;C-3PO&quot;, &quot;R2-D2&quot;, &quot;Darth Vader&quot;, &quot;Leia Organa&quot;, &quot;Owen Lars&quot;, &quot;Beru ~ ## $ height &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 228, 180, 173, 175, 170, 18~ ## $ mass &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.0, 84.0, NA, 112.0, 80.0, ~ ## $ hair_color &lt;chr&gt; &quot;blond&quot;, NA, NA, &quot;none&quot;, &quot;brown&quot;, &quot;brown, grey&quot;, &quot;brown&quot;, NA, &quot;black&quot;, &quot;auburn, white~ ## $ skin_color &lt;chr&gt; &quot;fair&quot;, &quot;gold&quot;, &quot;white, blue&quot;, &quot;white&quot;, &quot;light&quot;, &quot;light&quot;, &quot;light&quot;, &quot;white, red&quot;, &quot;lig~ ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;brown&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;brown&quot;, &quot;blue-gra~ ## $ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, 41.9, 64.0, 200.0, 29.0, 4~ ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;none&quot;, &quot;none&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;none&quot;, &quot;male&quot;, &quot;male&quot;, &quot;~ ## $ gender &lt;chr&gt; &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, &quot;feminine&quot;, &quot;masculine&quot;, &quot;feminin~ ## $ homeworld &lt;chr&gt; &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Naboo&quot;, &quot;Tatooine&quot;, &quot;Alderaan&quot;, &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Tat~ ## $ species &lt;chr&gt; &quot;Human&quot;, &quot;Droid&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Hum~ ## $ films &lt;list&gt; &lt;&quot;The Empire Strikes Back&quot;, &quot;Revenge of the Sith&quot;, &quot;Return of the Jedi&quot;, &quot;A New Hope~ ## $ vehicles &lt;list&gt; &lt;&quot;Snowspeeder&quot;, &quot;Imperial Speeder Bike&quot;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &quot;Imperial Speeder Bike&quot;, &lt;&gt;, &lt;~ ## $ starships &lt;list&gt; &lt;&quot;X-wing&quot;, &quot;Imperial shuttle&quot;&gt;, &lt;&gt;, &lt;&gt;, &quot;TIE Advanced x1&quot;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &quot;X-wing&quot;,~ IMPORTANT! Now that weve looked at our data, we might want to access certain columns in that data frame; this is what the $ operator does in R. Explicitly, df$columnName yields the column with name columnName as a list, as in the example below. Were using the head function because the list of characters is long. length(starwars$name) ## [1] 87 head( starwars$name ) ## [1] &quot;Luke Skywalker&quot; &quot;C-3PO&quot; &quot;R2-D2&quot; &quot;Darth Vader&quot; &quot;Leia Organa&quot; &quot;Owen Lars&quot; Just as with lists, you can pick out particular elements or ranges of elements in a data frame using bracket notation. In general, the syntax is df[rowRange , columnRange] where rowRange and columnRange can be lists or single numbers. Using starwars as an example: starwars[2, 5] #second row, 5th column ## # A tibble: 1 x 1 ## skin_color ## &lt;chr&gt; ## 1 gold starwars[2,] #all columns and only second row ## # A tibble: 1 x 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species films ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;list&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none masculine Tatooine Droid &lt;chr&gt; ## # ... with 2 more variables: vehicles &lt;list&gt;, starships &lt;list&gt; head(starwars[,5]) #head of 5th column ## # A tibble: 6 x 1 ## skin_color ## &lt;chr&gt; ## 1 fair ## 2 gold ## 3 white, blue ## 4 white ## 5 light ## 6 light We close this section with a discussion on the subset function, which allows one to select certain rows from a data set that have some desired features. The function returns a data set with all of the same columns as the original. The syntax is subset(data, condition on variables). As an example, suppose we wanted a data set of all Star Wars characters whose mass is less than 45 kilograms. smallCharacters &lt;- subset(starwars, mass &lt; 45) Since we reference the data set starwars inside the call to subset we dont need to call the mass column with a $. 2.6 R Markdown documents In this course you will complete a handful of activities using RStudio. Youll write up the results of these activities as lab reports in R Markdown documents. Markdown is an easy-to-learn markup language that converts a plain-text file into a nicely formatted document. R Markdown documents provide an interface between R and Markdown. In a markdown document, you can write R code along with markdown text. When you compile your R Markdown document, RStudio evaluates all code and prints the output. In this way, R Markdown provides a way to create fully reproducible data analysis reports and more (this book was written in R Markdown!). You can use Rmd documents to save and execute code in a notebook style environment and create beautiful reports. A lot of people have already written quite a bit about R Markdown, so well keep this brief. For more detailed information, check out this tutorial provided by RStudio. To play around, start with the following: Create a new Rmd file by clicking file -&gt; new file -&gt; R Markdown. Enter a title, your name, and the date, then click OK. This creates a new Rmd file. A document should open that looks like this. An R Markdown document Notice that your document has three different types of environments. The YAML is at the beginning and delineated by --- at the beginning and end. This section determines the overall formatting of your document. You can go wild in this section with some guidance and internet sleuthing. Code chunks which start with ```{r} and end with ```. Code chunks allow you to use R in a notebook-style environment by clicking the green triangle in the upper right-hand corner of the chunk to run the code. You can run any code you can imagine in a code chunk. Plain text and text formatted using markdown. Write whatever you want. Headings are indicated by pound signs, with the size of the heading determined by the number of pound signs, so # Title would be a big title, ## Section would be a section heading, a ### Subsection, and so on. You can make text italic by surrounding the text with * on both sides. You can bold face text with **. For example *foo* renders as foo and **bar** renders as bar. There are a lot more formatting hacks, but that should get you started. As youre writing a document, you should periodically knit the document Knitting starts at the beginning of your document and evaluates it from top to bottom, then renders the output in whatever format youve specified. RStudio executes code chunks in sequential order. RStudio ignores all values and data stored in memory and all loaded packages when knitting. In this way, knitting a document starts a new environment from scratch. This feature can lead to headaches if you dont think about it ahead of time. Some common pitfalls when using R Markdown: forgetting to load packages at the beginning of a document and loading them from the console. In this case, all code chunks will run, but your document may not knit. Notice that I included a few packages in the screenshot above Dont forget to load packages at the beginning! naming an object from the console (eg: x &lt;- 1:10) and using it in a code chunk, but forgetting to name the same object in a code chunk. In this case, code chunks will all evaluate, but your document will not knit. Some suggestions when using R Markdown: run your code chunks often. This helps ensure everything is working the way you think it should. knit your document early and often. It is a lot easier to find small mistakes as they arise than it is to debug an entire document. be patient. R Markdown is a powerful tool and you can do a lot with it, but it can take a little bit of getting used to. You can use it to do a lot more than write lap reports. For instance, you may be able to write a scientific paper in R Markdown and have all of your writing and data analysis in one package! Try new things; it is really easy to write code that doesnt run, but it is really hard to break R, RStudio, or a Markdown document. You can only improve by trying. If you want to take a deep dive into markdown, check out the book R Markdown: The Definitive Guide. "],["cross.html", "Chapter 3 Summary statistics and data visualization 3.1 Summary statistics in R 3.2 Data visualization", " Chapter 3 Summary statistics and data visualization At this point you should start feeling more comfortable using R. You know how to define lists, make calculations involving lists, define data frames, install and load packages, and work in and on R Markdown documents. Up to this point you may feel like R has made your life harder; the point of this chapter is to convince you that, at least for doing statistics, R can make your life much, much easier. In all of the examples below we will be using the teacher data set from the openintro package in R which contains salary data for 71 teachers in the St.¬†Louis Public Schools and other variables that may influence salary. For more information, evaluate ?teacher in the console. head(teacher) ## # A tibble: 6 x 8 ## id degree fte years base fica retirement total ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 BA 1 5 45388 3472. 7689. 56549. ## 2 02 MA 1 15 60649 4640. 10274. 75563. ## 3 03 MA 1 16 60649 4640. 10274. 75563. ## 4 04 BA 1 10 54466 4167. 9227. 67859. ## 5 05 BA 1 26 65360 5000. 11072. 81432. ## 6 06 BA 1 28.5 65360 5000. 11072. 81432. 3.1 Summary statistics in R R makes calculating summary statistics a breeze so that we can focus instead on appropriately interpreting and using these statistics. Of course, the software can handle both categorical and numerical variables equally as well. Remember, when analyzing numerical variables, we typically look at the mean/standard deviation or median/IQR as summary statistics; when analyzing categorical variables, we typically look at sample counts and/or proportions. 3.1.1 Numerical variables The commands we use to calculate all of your favorite summary statistics are fairly intuitive and straightforward in R. For example to calculate the mean of a data variable x, simply evaluate mean(x). The list below gives some common summary statistics and an example using the teacher data set. As usual, this is not a complete list. mean(x, ...) and median(x, ...): calculates the average and median of the data set. If x contains NA (unknown or not available) values, considering setting using the option na.rm = T which removes the NA values from your list. The average base salary for a teacher from our data set is mean(teacher$base) ## [1] 56415.96 median(teacher$base) ## [1] 59914 var(x, ...) and sd(x, ...): calculates the sample variance and standard deviation of the data set. For our base teacher salary: var(teacher$base) ## [1] 99684592 sd(teacher$base) ## [1] 9984.217 quantile(x, probs = # or list of percentiles ): returns the value in your data set corresponding to the given percentiles. For instance quantile(x, probs = .25) returns the first quartile of x. quantile(teacher$base, probs = .25) #first quartile ## 25% ## 51174 quantile(teacher$base, probs = .75) #third quartile ## 75% ## 65360 quantile(teacher$base, probs = c(.25, .5, .75)) #Q1, median, Q3 ## 25% 50% 75% ## 51174 59914 65360 IRQ(x,...): returns the interquartile range of the numerical variable x. IQR(teacher$base) ## [1] 14186 quantile(teacher$base, .75) - quantile(teacher$base, .25) ## 75% ## 14186 min(x,...) and max(x,...): returns the minimum and maximum values of the numerical varible x. min(teacher$base) ## [1] 19900 max(teacher$base) ## [1] 68230 summary(x,...): returns 6 summary statistics if x is a numerical variable: min, first quartile, median, mean, third quartile, and max. summary(teacher$base) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 19900 51174 59914 56416 65360 68230 fivenum(x, ...): returns a classic five number summary of the numerical variable x: min, Q1, median, Q3, and max. fivenum(teacher$base) ## [1] 19900 51174 59914 65360 68230 3.1.2 Categorical variables This section will focus on ways to create summary tables (ie frequency tables and contingency tables) for categorical variables. We will focus on using base R techniques for these purposes, but Chapter 3 will go into more detail about using the dplyr package to make the construction of more complicated summaries a bit easier. As above, the list below will help you summarize and analyze categorical variables. summary(x, ...): if x is a list of observations of a categorical variable, summary(x) will return a list counts for each value of the variable. Such a table is called a one-way frequency table. For example unique(teacher$degree) #unique values this variable takes on ## [1] BA MA ## Levels: BA MA summary(teacher$degree) #count the number of occurences of each value ## BA MA ## 39 32 table(...): this function is more flexible than summary in that will return either a one-way table (if only one categorical variable is specified) or a two-way/contingincy table (if two categorical variables are specified). #generates a one-way table table(teacher$degree) ## ## BA MA ## 39 32 #generates a two-way table table(teacher$degree, teacher$fte) ## ## 0.5 1 ## BA 0 39 ## MA 1 31 xtabs(~.): the output of this function is very similar to table(...), but the output labels the variables. The input syntax is slightly different because it uses Rs formula syntax. To illustrate the difference in input, well create the same tables as the last example. Variable names follow the ~ and are separated by +. #one-way table xtabs(~degree, data = teacher) ## degree ## BA MA ## 39 32 #two-way table xtabs(~degree + fte, data = teacher) ## fte ## degree 0.5 1 ## BA 0 39 ## MA 1 31 prop.table(...): the input of prop.table is a table itself. This function converts a table of frequencies into a table of proportions. #define one-way table, convert to proportion table tbl1 &lt;- table(teacher$degree) prop.table(tbl1) ## ## BA MA ## 0.5492958 0.4507042 #define two-way table, convert to proportion table tbl2 &lt;- xtabs(~degree + fte, data = teacher) prop.table(tbl2) ## fte ## degree 0.5 1 ## BA 0.00000000 0.54929577 ## MA 0.01408451 0.43661972 3.1.3 Combining it all You can combine these summary statistic functions with various subsetting techniques from the previous chapter to tease out relationships between variables. As an example, using the teacher data set still, wed expect teachers with more education to earn a higher salary on average. In the examples above we saw that there is only one part time teacher in the sample (the teacher with fte = 0.5) and this person has a lower salary simply from working less. Thus, to compare salaries by degree, we may want to exclude this observation. Doing this manually: #subset for only full time teachers fullTime &lt;- subset(teacher, fte == &quot;1&quot;) #subset by degree bach &lt;- subset(fullTime, degree == &quot;BA&quot;) mast &lt;- subset(fullTime, degree == &quot;MA&quot;) #finally calculate averages mean(bach$base) ## [1] 56257.1 mean(mast$base) ## [1] 57793.74 It would be nice if we could streamline this process. We can do so using the by function. The input is by(dataToSummarize, variableToGroupBy, summaryFunctionToApply). Thus, to recreate our work above we can use by(fullTime$base, fullTime$degree, mean) ## fullTime$degree: BA ## [1] 56257.1 ## ------------------------------------------------------------------------------- ## fullTime$degree: MA ## [1] 57793.74 As a final example, we will give a preview of creating the same table in using tidyversesyntax and grammar. Check out the next chapter for much more information, starting with the original teacher data set. fullTime %&gt;% filter( fte == &quot;1&quot;) %&gt;% #select only full time group_by(degree) %&gt;% #specify variables to group by summarize( n = n(), #create summary table, first column is a count avgSal = mean(base)) #second column is average salary ## # A tibble: 2 x 3 ## degree n avgSal ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BA 39 56257. ## 2 MA 31 57794. 3.2 Data visualization There are three main ways to create plots in R: base R, lattice, and ggplot2. We will only learn about base R and ggplot2 in this course. In practice, I use base R to make graphs quickly to get an idea of whats going on and ggplot2 to make more visually appealing and complicated graphics. In general, you should be able to make most graphs using either package, but sometimes it is easier to use one over the other. Caution: When creating data visualizations you may have to massage your data ahead of time to get it formatted correctly! 3.2.1 Base R The basic function for plotting in base R is simply plot. Often, R will correctly interpret what type of graph youre trying to make if you format the data correctly. To avoid possible confusion, its best to call the type of plot youre trying to make directly. Each of the functions below have a lot of options and arguments for customizing the visualizations. The best way to learn about these is to simply play around. Histograms: When trying to visualize the distribution of a single numerical variable you often want to start with a histogram. The basic command for this is hist(...). The first histogram below just gives you a rough idea of what the histograms look like, the second demonstrates the types of options you can specify. #no options specified hist(teacher$base) #vs many options specified hist(teacher$base, #variable to make a histogram of breaks = 11, #number of cells, can be left blank freq = FALSE, #displays relative frequency on y-axis, if true, displays frequency main = &quot;Histogram of Teacher Base Salary&quot;, #sets main title xlab = &quot;Base salary (USD)&quot;, #x-axis label ylab = &quot;Frequency&quot; #y-axis label, can be left blank in this case ) Scatter plots: when checking for an association between two numerical variables you will often want to start your analysis with a scatter plot. The function plot(x,y) creates a scatter plot of \\(x\\) and \\(y\\). Make sure these lists have the same length! As above, well make a basic scatter plot, then play around with the labels and options of the same graph. #basic plot of years experience vs total teacher salary plot(teacher$years, teacher$total) #same plot with updated options plot( teacher$years, teacher$total, pch = 16, #change mark to filled in circle main = &quot;Teacher experience vs total salary&quot;, xlab = &quot;Teaching experience (in years)&quot;, ylab = &quot;Total annual salary (USD)&quot;) Bar charts: bar charts help visualize one or two categorical variables; you can think of them as a visualization of one-way and two way tables. The basic function is, no surprise, barplot(...), but you must input a table instead of vectors. #define our tables tbl1 &lt;- prop.table(table(teacher$degree)) # one-way rel. freq. table tbl2 &lt;- xtabs(~degree + fte, data = teacher ) #two-way table #visualize one-way table barplot(tbl1, main = &quot;Relative frequency of teacher degrees&quot;, xlab = &quot;Degree&quot;, ylab = &quot;Proportion&quot;) #visualize two-way table barplot(tbl2, main = &quot;Teacher employment employment by degree&quot;, xlab = &quot;Employment (by FTE)&quot;, ylab = &quot;Frequency&quot;, col = c(&quot;red&quot;, &quot;blue&quot;), #set colors for degree types, alphabetically legend = rownames(tbl2) #create legend with rows from the table ) Box plots: Box plots help us visualize a the distribution of a numerical variable; stacking box plots side by side help us compare the distribution of a numerical variable over several values of a categorical variable. No suprise, both it is easy to make these plots in both situations using the function boxplot. The syntax for creating a box plot for a single numerical variable is simple, as the next example shows. boxplot(teacher$base, ylab = &quot;Base salary (USD)&quot;, main = &quot;Distribution of MI teacher salaries&quot;) When creating a box plot for a numerical variable over several values of a categorical variable, you need to use Rs formula syntax. This syntax will appear a few more times throughout the course since it is used often when looking at relationships between two variables. The basic notation is Response variable ~ explanatory variables or, if your response variable is y and your explanatory variable is x, simply y ~ x. Note the ~ symbol is to the left of the 1 key on your keyboard. The next examples shows a breakdown of teacher salary by highest degree. boxplot(base ~ degree, data = fullTime, xlab = &quot;Degree&quot;, ylab = &quot;Base salary (USD)&quot;, main = &quot;MI teacher base salary by degree&quot;) Before moving on to plotting with ggplot2, we remind the reader that this is by no means a comprehensive list of all plotting features in R. Moreover, each of the plotting functions used above have many more features and are very customizable; you can go down a deep rabbit hole making plots look just how you want them to. 3.2.2 ggplot2 This section will guide you through using the package ggplot2 for data visualization in R. Many find the output from ggplot2 to be more visually appealing than plots from base R, but the plotting syntax in ggplot2 is often much more intuitive than plotting in base R, especially if youre making more complicated visualizations. The basic idea of plotting with ggplot2 is to Open an empty plot based on a particular data set using the function ggplot(data, aes(...)) where data is the name of the data set youre using and aes(...) specifies the aesthetic of the plot to come. You specify which variables are assigned to which axes inside aes(...). This will become clearer through examples. Add layers to the empty plot to construct the visualization youre imagining, separating each layer with +. These layers are geoms, labels, legends, etc. Again, this will become much clearer through some examples. The ability to sequentially add layers to a plot makes complicated plotting more intuitive in ggplot2. For example, adding a line to a scatter plot is simple under this organization because you simply have to add another layer! We will reproduce the plots from the previous section in the examples that follow, tweaking some parameters here and there to give you an idea of how to change visualizations. Histograms: The geom for a histogram is geom_histogram(...). Notice that our plot below does open an empty plot first, then adds layers to it. # open empty plot ggplot(teacher, aes(x = base)) + # add histogram layer with 15 bins. fill changes bar color, but color changes outline color geom_histogram(bins = 15, fill = &quot;steelblue&quot;, color = &quot;black&quot;) + # adds label layer with only a title. can add subtitle and caption. labs( title = &quot;Distribution of MI teacher base salary&quot;) + # changes x-axis label xlab(&quot;Base salary (USD)&quot;) Scatter plots: The geom for a scatter plot is geom_point(...). You can tweak the appearance quite a bit using different arguments inside this (and all!) geoms. For instance the option alpha sets the transparency, color the color of each point, shape the shape, etc. #open empty plot and specify explanatory (x) and response (y) variables ggplot(teacher, aes(x = years, y = total)) + geom_point(color = &quot;steelblue&quot;, alpha = .5) + labs(title = &quot;Teacher salary vs years experience&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) Adding a line of best fit (see Chapter 8 for more information on modeling) to a scatter plot is straightforward with a geom_smooth(...) layer, setting method = 'lm' (this means add a linear model to your plot) ggplot(teacher, aes(x = years, y = total)) + geom_point() + geom_smooth( method = &#39;lm&#39;, formula = y~x, se = FALSE) + #adds line of best fit without error bars labs(title = &quot;Teacher salary vs years experience&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) We can make the same plot, but color the points by each teachers degree and change the shape of each point by full/part time status. ggplot(teacher, aes(x = years, y = total, color = degree)) + geom_point(aes(shape = fte)) + labs(title = &quot;Teacher salary vs years experience, by highest degree&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) This school districts pay scale becomes much more apparent! Teachers with masters degrees earn a little bit more, but everyones salary increases at roughly the same rate during their first 11 years, then flattens out. We also see that the outlier with minimal experience and low pay is also the only part-time employee. As a final example of tweaking scatter plots, we can even add a line of best fit for each degree group! ggplot(teacher, aes(x = years, y = total, color = degree)) + geom_point(aes(shape = fte)) + geom_smooth(method = &#39;lm&#39;, formula = y~x, se = FALSE) + labs(title = &quot;Teacher salary vs years experience, by highest degree&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) You can see that it is easy to start adding a lot of information to a plot! Be careful, however; there is a fine line between a detailed, informative visualization, and a confusing, overly busy one. Bar graphs: We can make bar graphs easily with ggplot2 using the geom_bar(...) layer. The first two plots below look identical at first pass, but the one on the left is a frequency bar chart, so displays the raw count on the \\(y\\)-axis, but the chart on the right is a relative bar chart. To make the relative frequency bar chart, we include the option y = ..prop.. in the aesthetic. We also have to specify the number of groups to ensure that our proportions are out of the total. # first load a package for side by side plots library(gridExtra) # Frequency bar plot p1 &lt;- ggplot(teacher, aes(x = degree)) + geom_bar(fill = &quot;aquamarine3&quot;, color = &quot;black&quot;) + labs( title = &quot;MI teacher education (count)&quot;) # relative frequency bar plot p2 &lt;- ggplot(teacher, aes(x = degree, y = ..prop.., group =1)) + geom_bar(fill = &quot;aquamarine3&quot;, color = &quot;black&quot;) + labs( title = &quot;MI teacher education (proportion)&quot;) #display both plots side by side. grid.arrange(p1,p2, ncol = 2) Next well make stacked and side by side box plots. #stacked p1 &lt;- ggplot(teacher, aes(x = degree, fill = fte)) + geom_bar(color = &quot;black&quot;) + labs(title = &quot;MI teacher education&quot;) #side by side p2 &lt;- ggplot(teacher, aes(x = degree, fill = fte)) + geom_bar( position = position_dodge(), color = &quot;black&quot;) + # big change is setting position! labs(title = &quot;MI teacher education&quot;) #display both plots side by side grid.arrange(p1, p2, ncol = 2) One could combine our first and second examples of box plots to get side-by-side and stacked relative frequency plots. This takes a little bit of extra fuss that can be avoided by first manipulating the data you input into ggplot2. In other words, to make these types of plots, make the summary calculations on your own earlier, then feed those into the appropriate geom. Box plots: We will finish up our brief tour of ggplot2 by making a few box plots. The geom for box plots is, no surprise, geom_boxplot(...). Lets first look at a box plot of the teachers total salary. ggplot(teacher, aes(y = total)) + geom_boxplot() + theme(axis.text.x=element_blank(), #this layer just removes the scale on the x-axis axis.ticks.x=element_blank()) + labs(title = &quot;Total MI teacher salary&quot;) By adding an x value to our aesthetic, we can produce boxplots of the range of values of a categorical variable. The example below also adds a point representing the mean of each group and colors the outlier red. ggplot(teacher, aes(x = degree, y = total)) + geom_boxplot(outlier.color = &quot;red&quot;, outlier.size = 2) + stat_summary_bin(fun = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;blue&quot;, size = 2) + labs(title = &quot;MI teacher salaries by degree&quot;) There are many, many more geoms, options, and possible visualizations you can make with ggplot2; this section should give you enough to get started and make some high quality graphics. For more information, check out the official ggplot2 guide. 3.2.3 Saving plots Now that you know how to make plots in R, what do you do with them? Data visualization is essential for two aspects of data analysis: exploration and communication. Initial data visualization can help guide your analysis by illuminating patterns and trends that may be difficult to observe in your raw data. For the same reason, you will want to include high quality visualizations when you write up your work; visualizations help readers get a good understanding of your work without digging deep into the details. So how do you get your visualizations into a write up? There are two possibilities: Do your entire analysis and write-up in an RMarkdown document! You can hide code chunks so that output only shows plots and calculation results. You can use RMarkdown to create impressive, professional documents. You can save your plot in your favorite file format. There are two ways to do this, from the console or from the File/Plot explorer window in RStudio. Both options will save the file to whatever directory or folder you are currently working in. File/Plot Explorer: When you make a plot in RStudio it displays in the bottom right Plot window. That window has an Export button. Clicking this button reveals a bunch of options for saving your plot. Pick whichever you think is best for your application. From the console: You can also export an image programmatically. The basic process is 1) open a graphics device, 2) code your plot, then close the graphics device. This option is best if you have to save multiple images or want to customize the file youre saving. As an example, the following code will create a PDF file called salary_viz.pdf of the side-by-side box plot we created at the end of the base R plotting section. pdf(&quot;salary_viz.pdf&quot;) #opens graphic device boxplot(base ~ degree, data = fullTime, xlab = &quot;Degree&quot;, ylab = &quot;Base salary (USD)&quot;, main = &quot;MI teacher base salary by degree&quot;) dev.off() You can save your plot in many different file formats; the command to open the appropriate graphics device is usually obvious. For instance, a png graphics device is opened with the function png(...). "],["wrangling-data.html", "Chapter 4 Wrangling data 4.1 The goal: tidy data. 4.2 The pipe 4.3 Common tidying operations 4.4 Mutate, group by, and summarize", " Chapter 4 Wrangling data Wrangling data is a term used to describe the processes of manipulating or transforming raw data into a format that is easier to analyze and use. Data professionals often spend large chunks of time on the data wrangling phase of a project since the analysis and use flows much more smoothly when the wrangling is done appropriately. Like most chapters in this book, we wont go too in depth into this subject, but we will cover enough to get you started. In particular, this section aims to help you understand what the end goal of data wrangling might look like. In other words, start understanding what a good, easy to use data set might look like. be able to use Rs tidyverse package to perform some basic data wrangling tasks. This section of notes will can be thought of as a condensed version of the Wrangle and Program sections of R for Data Science and references this book substantially. Remember, since were using the tidyverse package in this section, you have to load the package with library(tidyverse) 4.1 The goal: tidy data. In the early days of STAT216, we stipulated that data sets should contain variables in columns and observations in rows. This is the common convention in data science, but this convention is not always followed, especially when youre collecting data from out in the wild. This can come about for a myriad of reasons, but one common reason youll encounter unconventional data organization is that the data enter prioritizes human-readability over machine-readability and ease of analysis. For example, suppose you were giving a blind taste test of three types of diet cola: diet Coke, diet Pepsi, and diet RC. You give 3 groups of 10 random people a drink and ask them to rate their preference on a scale of 1-5 with 5 being great and 1 being awful. A natural way to organize the data from your experiment may look something like the table below (the numbers are randomly generated). set.seed(1123) df &lt;- tibble( coke = sample(1:5, 10, replace = T), pepsi = sample(1:5, 10, replace = T), rc = sample(1:5, 10, replace = T)) #generate table of random numbers. df %&gt;% knitr::kable() coke pepsi rc 3 3 1 4 2 1 4 3 2 1 4 3 3 4 5 4 1 2 1 5 3 2 3 3 3 5 2 3 4 5 Note, however, that the drink someone is tasting is actually a variable! This means our data set doesnt have variables in columns, it splits one variable up into three different columns. This type of thing is quite common. Fortunately, R gives us a way to easily transform such data sets as well see. The appropriate format for our convention of variables in columns, observations in rows would be set.seed(1123) # make sure random numbers generated above are the same. dfTidy &lt;- tibble( #first a column of the drink participants had drink = c( rep(&quot;coke&quot;, 10), rep(&quot;pepsi&quot;, 10), rep(&quot;rc&quot;, 10)), # then a column of their scores score = c(sample(1:5, 10, replace = T), sample(1:5, 10, replace = T), sample(1:5, 10, replace = T) )) head(dfTidy) ## # A tibble: 6 x 2 ## drink score ## &lt;chr&gt; &lt;int&gt; ## 1 coke 3 ## 2 coke 4 ## 3 coke 4 ## 4 coke 1 ## 5 coke 3 ## 6 coke 4 This data set is less easy to read as a human (since it has 30 rows and two columns), but is much easier to analyze in R (and pretty much all other available software). This example typifies our goal of tidy data: Each variable should have its own column. Each observation/observational unit should have its own row. Again, the purpose of tidy data is to streamline and make the analysis stage of your data journey easier. Most functionality in R is substantially easier to implement when you start with a tidy data set. As an example, if wed like to make side-by-side box plots of the results of our cola taste test using the original data set df in ggplot2, we might try something like ggplot(df, aes(x = ???, y = ???)) + geom_boxplot() but run into an immediate problem. What is the \\(x\\) variable? What is the \\(y\\)? If we use the tidy version of the same data, the answer is clear however: \\(x\\) is the drink, \\(y\\) is the score, since drink is the explanatory variable and taste preference is the response. Using dfTidy from above instead, wed simply use the following. ggplot(dfTidy, aes(x = drink, y = score)) + geom_boxplot() Essentially all tools in the tidyverse package are designed around having tidy data. In the next section, well learn about one of the main functions from the tidyverse package used in tidying and analyzing data, the pipe. 4.2 The pipe The pipe, typed as %&gt;%, is a function from the tidyverse package whose intention is to make a sequence of transformations or operations on a data set more clear and easier for humans to follow and write. In some sense, you can think of it as adding layers of transformations to a data set, just like you use the + function to add layers to a graph in ggplot2. Note: this analogy is by design from the nice folks who created the tidyverse! In essence, the pipe tells R to do the operation on the right side of the pipe to the data set on the left of it; if there are multiple pipes in a sequence, they are evaluated left to right/top to bottom. Lets try an example to see how it works. Well start with the dfTidy data from above. Suppose we forgot to add the results from a blind taste test of Diet Dr.¬†Pepper. We load that data into R (simulating it of course in this case) set.seed(1123) dp &lt;- tibble(drink = rep(&quot;dp&quot;, 10), score = sample(1:5, 10, replace = TRUE)) Now suppose want to Add this data to dfTidy. Add a new column that rescales the scores so that theyre between 0 and 1 instead of 1 and 5. We can do this quickly using the following newDf &lt;- dfTidy %&gt;% #start with dfTidy bind_rows(dp) %&gt;% #adds dp to the bottom of dfTidy mutate(newScale = .25*score - .25) #add new column to whole dataset. To see that this did what we hope, lets look at a random sample of 10 rows using the slice_sample function set.seed(1123) #more piping! newDf %&gt;% slice_sample(n = 10) ## # A tibble: 10 x 3 ## drink score newScale ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 dp 2 0.25 ## 2 coke 1 0 ## 3 pepsi 1 0 ## 4 dp 4 0.75 ## 5 dp 3 0.5 ## 6 dp 3 0.5 ## 7 rc 3 0.5 ## 8 rc 2 0.25 ## 9 coke 2 0.25 ## 10 dp 1 0 It does as wed hoped! Great. You may be wondering why piping exists. Earlier we said it was to make code easier to read and intuitive to write. But with respect to what? There are two older programming conventions that piping is trying to clarify: overwriting the original and function composition. Lets create dfTidy using these two strategies. Overwriting the original: At each stage, overwrite what you did at the previous stage. Its not too bad in this example, but can be tedious if you have say 10 different transformations to make. It is especially inconvenient if you need to make changes since you have to track those changes through all your steps. newDf1 &lt;- bind_rows(dfTidy, dp) newDf1 &lt;- mutate(newDf1, newScale = .25*score - .25) #check to see we got the same thing all.equal(newDf, newDf1) ## [1] TRUE Function composition: This is probably the least human readable way to write create our new data set, but it also makes explicit what the computer is doing and it looks shorter. Notice that you have to read from the inside out and right to left. Once again, this isnt too bad with only two transformations, but you could imagine how much of a nightmare this could be with 10 transformations, especially in terms of reading the code. newDf2 &lt;- mutate( bind_rows(dfTidy, dp), newScale = .25*score - .25) #are they equal? all.equal(newDf, newDf2) ## [1] TRUE Hopefully you think the piped sequence of transformations is more straightforward to read and write than the other two options. Now that were more comfortable with the pipe, lets use it to implement some common forms of transformations! 4.3 Common tidying operations If youre dealing with a data set that isnt tidy, it typically violates our tidy criteria in one of two ways: Variables can be split across several columns. In this case, the column names are not variable names, but values of a variable like the original data set for our diet cola blind taste test df above. Observations can be split across several rows. There are two functions in the tidyverse used to handle these cases: pivot_longer(...) and pivot_wider(...). Caution: it can take a little bit of time get these functions to do exactly what you want. Have a bit of patience and youll improve with time! The function pivot_longer(...) is used to tidy a data set when variables are split across several columns; it takes the values of the variable that are the untidy column names, makes a new column for them, then organizes the values of the columns appropriately. This is best observed in an example: instead of reentering the data in our cola example, we could instead use pivot longer. dfTidy1 &lt;- df %&gt;% #specify the columns are actually values pivot_longer(cols = c(&quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;), #then specify column/variable name names_to = &quot;drink&quot;, #finally specify the name of the values values_to = &quot;score&quot; ) glimpse(dfTidy1) ## Rows: 30 ## Columns: 2 ## $ drink &lt;chr&gt; &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;, &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;, &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;, &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;~ ## $ score &lt;int&gt; 3, 3, 1, 4, 2, 1, 4, 3, 2, 1, 4, 3, 3, 4, 5, 4, 1, 2, 1, 5, 3, 2, 3, 3, 3, 5, 2, 3, 4, 5 At first it doesnt seem like this data set is the same as dfTidy, but notice that R arranged the values as coke, pepsi, rc. If we rearrange the rows alphabetically by drink, well see that they are in fact that same data sets. all.equal( dfTidy %&gt;% arrange(drink), dfTidy1%&gt;% arrange(drink)) ## [1] TRUE The function pivot_wider(...) is used to tidy a data set when observations are split across several rows. The tidyverse has a data set simply called table2 that provides an example of observations being split across several rows. This table shows the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000 (from the documentation). table2 ## # A tibble: 12 x 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 In this example, the observations are a country in a year, so cases and population are not values. Instead, they are variables, since in a given year each country has a population and a certain number of TB cases. table2 %&gt;% #first specify where the new column names are coming from pivot_wider(names_from = &quot;type&quot;, #next specify where the new column values are coming from values_from = &quot;count&quot;) ## # A tibble: 6 x 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Much better! Notice that the same information is displayed in both tables. The latter option is simply easier to work with. 4.4 Mutate, group by, and summarize Once youre working with a tidy data set, you may want to add columns to it, create a summary of the data, or both. The key functions for performing these operations are mutate(...), group_by(...), summarize(...), and transmute(...), all of which are from the dplyr package, which is part of the tidyverse. Before looking at specific examples of these functions in action, well define what they do. mutate(...) adds columns to an existing data set and keeps the old columns. transmute(...) creates new columns from a data set, but drops the old existing columns. group_by(...) takes column names as input and converts your data set to a grouped table. This means all operations and summary/aggregate functions are performed by group. The groups are defined by the values in the columns you specified. summarize(...) creates a new data set giving a customized summary of your data set. All operations are done by the grouping variables. For examples of each function, we will use the gss2010 data set, which is part of the openintro package. As a reminder, to follow along you need to attach this package with library(openintro) data(gss2010) The gss2010 data set has 2044 measurements of 5 varibles: hrsrelax: hours relaxed after work on an average work day. mntlhlth: number of days in the last month where mental health was not good. hrs1: weekly hours worked. degree: highest educational attainment or degree. grass: Should marijuana be legalized? Working with this data set provides an obstacle that we havent encountered much so far in these notes: NA values. A quick glimpse at the data reveals that there are quite a few missing values. In learning how to use the functions listed above, well also see a few strategies for dealing with missing values. glimpse(gss2010) ## Rows: 2,044 ## Columns: 5 ## $ hrsrelax &lt;int&gt; 2, 4, NA, NA, NA, NA, 3, NA, 0, 5, 5, NA, 3, NA, NA, NA, NA, NA, NA, 2, 5, 10, NA, NA, ~ ## $ mntlhlth &lt;int&gt; 3, 6, NA, NA, NA, NA, 0, NA, 0, 10, 0, NA, 0, NA, NA, NA, NA, NA, NA, 5, 0, 0, NA, NA, ~ ## $ hrs1 &lt;int&gt; 55, 45, NA, NA, NA, NA, 45, NA, 40, 48, 26, NA, 40, NA, NA, NA, NA, NA, NA, 50, 40, 25,~ ## $ degree &lt;fct&gt; BACHELOR, BACHELOR, LT HIGH SCHOOL, LT HIGH SCHOOL, LT HIGH SCHOOL, LT HIGH SCHOOL, JUN~ ## $ grass &lt;fct&gt; NA, LEGAL, NA, NOT LEGAL, NOT LEGAL, LEGAL, NA, NA, NOT LEGAL, NA, NA, NA, NA, NA, LEGA~ First, lets add a new column to our data set that gives the total number of hours each person in this survey spend at work or relaxing and another column that reports the proportion of their time spent working or relaxing Monday through Friday. Note that since hrsrelax gives the number of hours they relax in a day, we should add 5 times that amount for total time spent relaxing during the work work. new_gss &lt;- gss2010 %&gt;% mutate(totalHours = 5*hrsrelax + hrs1, #hours spent working and relaxing propTimeWorking = totalHours/(5*24)) #proportion of time spent working/relaxing # random sample of 7 rows from the data set glimpse( slice_sample(new_gss, n = 7) ) ## Rows: 7 ## Columns: 7 ## $ hrsrelax &lt;int&gt; 4, 6, 6, NA, NA, 7, NA ## $ mntlhlth &lt;int&gt; 0, 0, 2, NA, NA, 0, NA ## $ hrs1 &lt;int&gt; 55, 35, 9, NA, NA, 20, NA ## $ degree &lt;fct&gt; LT HIGH SCHOOL, HIGH SCHOOL, LT HIGH SCHOOL, HIGH SCHOOL, HIGH SCHOOL, LT HIGH S~ ## $ grass &lt;fct&gt; NOT LEGAL, NA, LEGAL, NA, NA, NA, LEGAL ## $ totalHours &lt;dbl&gt; 75, 65, 39, NA, NA, 55, NA ## $ propTimeWorking &lt;dbl&gt; 0.6250000, 0.5416667, 0.3250000, NA, NA, 0.4583333, NA Note that whenever one of our variables was unavailable, both of our new columns gave an NA value as well. Suppose now that we wanted the same information, but we wanted to exclude all rows with NA values, and we only wanted to see our new columns and the respondents degree. This is a perfect situation for transmute(...). new_gss &lt;- gss2010 %&gt;% transmute(degree = degree, #keeps the degree column totalHours = 5*hrsrelax + hrs1, #hours spent working and relaxing propTimeWorking = totalHours/(5*24)) %&gt;% drop_na() #drops all rows with an NA slice_sample(new_gss, n = 7) ## # A tibble: 7 x 3 ## degree totalHours propTimeWorking ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GRADUATE 70 0.583 ## 2 LT HIGH SCHOOL 54 0.45 ## 3 HIGH SCHOOL 52 0.433 ## 4 BACHELOR 55 0.458 ## 5 BACHELOR 65 0.542 ## 6 BACHELOR 70 0.583 ## 7 GRADUATE 50 0.417 Next up, lets start making some summary tables. First, lets see if theres a correlation between educational attainment and hours worked, hours relaxing, or days with mental health struggles. To do this, we want to group our dataset by the degree variable so that all of our summary functions are applied to the groups of respondents with the same degrees. gss2010 %&gt;% group_by(degree) %&gt;% summarize( meanHrsWrk = mean(hrs1, na.rm = TRUE), # avg hours worked, removing NA sdHrsWrk = sd(hrs1, na.rm = TRUE), # sd of hours worked, removing NA meanHrsRlx = mean(hrsrelax, na.rm = TRUE), meanMntlHlth = mean(mntlhlth, na.rm = TRUE) ) %&gt;% arrange(desc(meanHrsWrk)) # arrange results in descending order ## # A tibble: 5 x 5 ## degree meanHrsWrk sdHrsWrk meanHrsRlx meanMntlHlth ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BACHELOR 42.5 13.6 3.48 2.67 ## 2 JUNIOR COLLEGE 41.4 18.1 3.53 3.56 ## 3 GRADUATE 40.8 15.5 3.67 2.40 ## 4 HIGH SCHOOL 39.6 15.0 3.79 4.58 ## 5 LT HIGH SCHOOL 38.7 15.8 3.73 4.87 So everyone works fairly similarly and relax a similar number of hours every night, but we observe that the number of days per month with mental health struggles seems to decrease with an increase in educational attainment. We could also see if theres any sort of relationship between degree and stance on marijuana legalization. First, lets throw out all the rows with an NA for the grass variable and see how many people have type of degree. gss2010 %&gt;% filter( !is.na(grass) ) %&gt;% #selects all rows where grass is *not* NA group_by(degree) %&gt;% summarize( degCount = n(), # special function to count the number in each group legalCount = sum(grass == &quot;LEGAL&quot; ), legalProp = legalCount / degCount ) %&gt;% arrange(desc(legalProp)) ## # A tibble: 5 x 4 ## degree degCount legalCount legalProp ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 GRADUATE 136 73 0.537 ## 2 BACHELOR 231 119 0.515 ## 3 HIGH SCHOOL 611 304 0.498 ## 4 JUNIOR COLLEGE 86 42 0.488 ## 5 LT HIGH SCHOOL 195 65 0.333 So, roughly speaking, the percentage of people who believe marijuana should be legalized increases with educational attainment. This is likely not too surprising, but interesting to see shake out in the summary table none the less! As usual, one can (and should!), do quite a bit more with these functions, but these examples should give you a reasonable idea about how they work and what they can do for you. "],["distribution-calculations.html", "Chapter 5 Distribution calculations 5.1 Finite discrete distribution calculations 5.2 Named distribution calculations", " Chapter 5 Distribution calculations The second module of STAT216 at FVCC focuses on the basics of probability theory. We start out learning the foundations: interpretations of probability (frequentist vs Bayesian) along with the notions of independence, mutually exclusive events, conditional probability, and Bayes Theorem. We then move on to thinking about random variables. Recall that a random variable is simply the assignment of a number to every element of the sample space for the random process in question. From this assignment we can assign probabilities to the values our random variable can take on. Random variables also give us a way to define the average outcome of a random process and measure the spread of outcomes around that average. Recall that random variables come in two primary flavors, discrete and continuous. Continuous random variables can take on at least some interval worth of real numbers. Discrete random variables, on the other hand, can only take on values with sufficiently large gaps in between. For example, the height of a STAT216 student is a continuous random variable, but the number of books a STAT216 student owns in discrete. Distinguishing between these two types of random variables is important because the flavor dictates quite a bit about how we use and make calculations associated with a given random variable. The probability distribution of a discrete random variable can be represented in a tabular form, but the probability distribution of a continuous random variable must be represented by a function. The table below helps compare and contrast discrete and continuous probability distributions. Discrete Continuous all probabilities \\(\\geq 0\\) graph of function above or on \\(x\\)-axis sum of probabilities is 1 area between graph of function and \\(x\\)-axis is one Short technical note: Using the right mathematical perspective you do not actually treat discrete and continuous random variables any differently, but this perspective takes a lot of technical machinery and isnt necessary. Suffice it to say that the analogies above are more than just analogies. There are many, many different types of each flavor of random variable - too many to name or describe - but statisticians and mathematicians have studied many families of random variables that have commonalities. In STAT216, we learn about a handful of these families, discussed below. You can skip the details for now and come back to these as we learn more about them in class. Discrete distributions: Bernoulli random variables: only two outcomes, typically 0 and 1 with 0 corresponding to failure and 1 to success for the random process youre studying. \\(x_i\\) 0 1 \\(P(X = x_i)\\) \\(1-p\\) \\(p\\) Geometric random variables: repeat independent identical (iid) trials of a Bernoulli random variable until the first success and count the number of trials. If \\(p\\) is the probability of success in a single trial, \\[ P(X = k) = (1-p)^{k-1}p \\] Binomial random variables: repeat a fixed number \\(n\\) of iid trials of a Bernoulli random variable and count the number of successes, \\(k\\). \\[ P(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] Continuous distributions: Normal distributions: a family of symmetric, unimodal continuous distributions determined by an average and standard deviation. Students \\(t\\)-distribution: a family of symmetric, unimondal continuous distributions determined by a single quantity: degrees of freedom, \\(df\\). These distributions have fatter tails than standard normal distribution. \\(\\chi_^2\\)-distributions: a family of right-skewed distributions determined by a single parameter, \\(df\\), or degrees of freedom. A \\(\\chi^2\\)-distribution is a sum of \\(df\\) squared standard normal distributions. \\(F\\)-distributions; a family of right-skewed distributions determined by two degrees of freedom \\(df_1\\) and \\(df_2\\). Roughly speaking, an \\(F\\)-distribution is the ratio of two \\(\\chi^2\\) distributions The remainder of this chapter focuses on how to make calculations for each of these distributions in R. This may come as a welcome relief from the last few chapters because well be using R more as a big calculator again for a bit. 5.1 Finite discrete distribution calculations This short section will describe how to calculate the expected value, variance, and standard deviation of a finite, discrete probability distribution. The key is to remember that R performs all list calculations component-wise. With this in mind, recall that the expected value and variance of a finite discrete random variable \\(X\\) are \\[ E(X) = \\sum_{i= 1}^n x_i P(X = x_i) \\quad \\text{and} \\quad V(X) = \\sum_{i = 1}^n P(X = x_i)\\left(x_i - E(X) \\right)^2\\] so \\(E(X)\\) is just the average outcome of \\(X\\), weighted by the probabilities associated to each value of \\(X\\). Similarly \\(V(X)\\) is just the average squared deviation of the values of \\(X\\) from the mean. Lets learn how to make these calculations in R from an example. Suppose you and your friends invent a silly game to pass the time based on flipping a coin and rolling a standard six-sided die. If you flip a heads and roll an even number, you win a dollar. If you flip a heads and roll an odd prime, you win two dollars. If you flip a tails and roll a 6, you win five dollars. Otherwise, you win nothing. Question How much should you and your friends charge to play the game? To answer this, we need to know the expected winnings since expected value gives us the average outcome. You and your friends need to charge at least the expected winnings. To make this calculation in R we need to store the values of our random variable and the associated probabilities in lists. The sample space for our game has size 12; you can think about is as the collection of ordered pairs \\(\\{ (x,y) | x \\in \\{H, T\\}, y \\in \\{1,2,\\ldots, 6\\} \\}\\). There are 3 ways to flip a heads and roll an even number, two ways to flip a heads and roll an odd prime, and only one way to flip a tails and roll a six. Thus the probability distribution of our winnings is \\(x_i\\) 1 2 5 0 \\(P(X = x_i)\\) 3/12 2/12 1/12 6/12 Storing this in R val &lt;- c(1,2,5,0) prob &lt;- c(3/12, 2/12, 1/12, 6/12) Now, to calculate the expected value, we just need to add up the product of val and prob: exp_val &lt;- sum(val*prob) exp_val ## [1] 1 So on average, a player of this game will win one dollar. This means you and your friends should charge at least one dollar to play the game. Lets also calculate the variance and standard deviation of the winnings for this game. The strategy for calculating variance is exactly the same as with the expected value. varX &lt;- sum(prob*(val - exp_val)^2) varX ## [1] 2 As usual, the standard deviation is just the square root of the variance so sdX &lt;- sqrt(varX) sdX ## [1] 1.414214 This means, on average, someone playing this game will win $1 with an average change between games of about $1.41. Next up, well learn about making probability calculations for some of the named distributions discussed in the beginning of this chapter. 5.2 Named distribution calculations R has a robust library of probability distribution calculators built in. The functions are organized so that they behave similarly for all of the families of distributions. The basic syntax is \\[ \\text{function type} + \\text{distribution} \\] The table below describes the type of function and describes what they do for both continuous and discrete probability distributions Function input Continuous Discrete d (density) value of random variable value of function / height of graph of probability density function at a given \\(x\\)-value Probability that random variable takes on a specific value p (percentile) value of random variable returns a percentile, ie a lower tail probability of associated to a value of the random variable, \\(P(X \\leq x)\\). Same as continuous q (quantile) a percentile returns a quantile, ie the value of a random variable corresponding to the percentile input Same as continuous r (random) whole number \\(n\\) \\(n\\) randomly sampled numbers from the distribution Same as continuous Note that the biggest difference lies in the the density functions. This is because probabilities correspond to areas under the graph of the probability density function for continuous random variables, so the probability of observing any particular value is zero in this case. For a discrete probability distribution, however, the density function actually returns probabilities. The percentile p and the quantile q functions both have an option called lower.tail that is a logical, so either TRUE or FALSE. If lower.tail=TRUE, the percentile functions return \\(P(X \\leq x)\\) and the quantile functions return the value \\(x\\) such that \\(P(X \\leq x)\\). If lower.tail=FALSE, the percentile functions return \\(P(X &gt; x)\\) and the quantile functions return the value \\(x\\) such that \\(P(X &gt; x)\\). We will look at examples in what follows that will clarify these functions and their arguments. 5.2.1 Discrete random variables Lets start off with some named families of discrete random variables. Well only look at binomial and geometric distributions, but once you have these down, you should be be able to figure out how to use any other discrete random variable distribution functions such as those associated to Poisson or hypergeometric random variables. Note: we do not cover Poisson on or hypergeometric distributions in this class! 5.2.1.1 Binomial distribution Well start with the definition and a motivating example. Definition: Suppose you repeat independent and identical trials of a Bernoulli experiment a fixed number \\(n\\) times. The random variable \\(X\\) counting the number of successes \\(k\\) is called a binomial random variable. If the probability of success in each trial is \\(p\\), then the probability of failure is \\(1-p\\) and A little work shows \\[ P(X = k) = { n \\choose k } p^k (1-p)^{n-k}. \\] In class we learn formulas for the expected value and variance of geometric random variables: \\[ E(X) = np \\quad \\text{and} \\quad V(X) = np(1-p). \\] The R shorthand for binomial distributions is binom, so the primary functions for binomial calculations and their outputs are: dbinom(k, n, p) \\(= P( X = k) = { n \\choose k } p^k (1-p)^{n-k}\\) pbinom(k, n, p) \\(= P( X \\leq k) = \\sum_{i=0}^k { n \\choose i } p^i (1-p)^{n-i}\\) pbinom(k, n, p, lower.tail = FALSE) \\(= P( X &gt; k) = \\sum_{i=k+1}^n { n \\choose i } p^i (1-p)^{n-i}\\) qbinom(prob, n, p) is the integer \\(k\\) such that \\(P(X \\leq k) =\\) prob. qbinom(prob, n, p, lower.tail = FALSE) is the integer \\(k\\) such that \\(P(X &gt; k) =\\) prob. rbinom(N, n, p) generates N numbers between \\(0\\) and \\(n\\) by repeating the binomial experiment N times. Now lets look at an example to see how we can use these functions in practice. Example: A Gallup poll showed that roughly 43% of all voters in the USA identify as politically independent as of September 16, 2022. You decide to loosely test this claim in the Flathead Valley by polling a random sample of 15 people about their political ideology. Assuming the distribution of political party affiliation in the Flathead is the same as the nation, counting the number of independents in your poll determines a binomial random variable. We can get a feel for the distribution of this variable with a bar chart. bf &lt;- tibble(k = 0:15, prob = dbinom(k, 15, .43)) ggplot(bf, aes(x = as.factor(k), y = prob)) + geom_col(color = &quot;black&quot;, fill = &quot;steelblue&quot;) + xlab(&quot;k&quot;) + ylab(&quot;P(X=k)&quot;) + labs(title = &quot;Binomial Prob dist w/ p = .43 and n = 15&quot;) Now lets use R to answer the following questions: What is the probability of exactly 4 independents in your poll? We want \\(P(X = 4)\\), with \\(n = 15\\), \\(k = 4\\) and \\(p = .43\\), so dbinom(4, 15, .43) ## [1] 0.0963008 What is the probability of at most 4 independents in your poll? We want \\(P(X \\leq 4)\\), with \\(n = 15\\), \\(k = 4\\) and \\(p = .43\\), so pbinom(4, 15, .43) ## [1] 0.1545517 What is the probability of at least 4 independents in your poll? We want \\(P(X \\geq 4) = P(X &gt; 3)\\), with \\(n = 15\\), \\(k = 4\\) and \\(p = .43\\), so pbinom(3, 15, .43, lower.tail= FALSE) ## [1] 0.9417491 #or 1 - pbinom(3, 15, .43) ## [1] 0.9417491 What is the third quartile of this distribution? We want to find the value \\(k\\) so that \\(P(X \\leq k) = .75\\), so qbinom(.75, 15, .43) ## [1] 8 Now note: pbinom(8, 15, .43) ## [1] 0.857269 which is greater than .75; R finds the closest integer valued quantile without going under the given percentile. 5.2.1.2 Geometric distribution As above, well start with a definition then move to a motivating example. Definition: Suppose you repeat independent and identical trials of a Bernoulli experiment until your first success. The random variable \\(X\\) counting the number of trials you perform is a geometric random variable. If the probability of success in each trial is \\(p\\), then the probability of failure is \\(1-p\\) and some work shows \\[ P(X = k ) = p(1-p)^{k-1}\\] In class we learn formulas for the expected value and variance of geometric random variables: \\[ E(X) = \\frac{1}{p} \\quad \\text{and} \\quad V(X) = \\frac{1-p}{p^2}. \\] The R shorthand for binomial distributions is geom. R requires that you enter the number of failures before your first success, so the primary functions for binomial calculations and their outputs are: dgeom(k-1, n, p) \\(= P(X = k ) = p(1-p)^{k-1}\\) pgeom(k-1, n, p) \\(= P( X \\leq k) = \\sum_{i=1}^{k} p (1-p)^{i-1}\\) pgeom(k-1, n, p, lower.tail = FALSE) \\(= P( X &gt; k) = \\sum_{i=k+1}^\\infty p (1-p)^{i-1}\\) qgeom(prob, n, p) is the integer \\(k\\) such that \\(P(X \\leq k) =\\) prob. qgeom(prob, n, p, lower.tail = FALSE) is the integer \\(k\\) such that \\(P(X &gt; k) =\\) prob. rbinom(N, n, p) generates N integers by performing the experiment \\(N\\) times and count the number of trials until the first success. Lets use the same example, but in a slightly different context to help differentiate between binomial and geometric random variables. Example: A Gallup poll showed that roughly 43% of all voters in the USA identify as politically independent as of September 16, 2022. You decide to loosely test this claim in the Flathead Valley by polling random people until you find someone who identify as politically independent. You count the number of people you survey. Assuming the distribution of political party affiliation in the Flathead is the same as the nation, counting the number of people you survey until your first independent determines a geometric random variable. We can get a feel for the distribution of this variable with a bar chart. gf &lt;- tibble(k = 1:15, prob = dgeom(k-1,.43)) ggplot(gf, aes(x = as.factor(k), y = prob)) + geom_col(color = &quot;black&quot;, fill = &quot;steelblue&quot;) + xlab(&quot;k&quot;) + ylab(&quot;P(X=k)&quot;) + labs(title = &quot;Geometric Prob. dist. w/ p = .43 for k = 1 to 15&quot;) What is the probability you the 6th person you poll is the first independent? Were trying to find \\(P(X = 6)\\), so dgeom(5, .43) ## [1] 0.02587276 # or (1-.43)^5*.43 ## [1] 0.02587276 What is the probability you will survey at most 6 people before meeting your first independent? Were trying to find \\(P(X \\leq 6)\\) so pgeom(5, .43) ## [1] 0.9657036 What is the probability you will survey at least 6 people before meeting your first independent? Were trying to find \\(P(X \\geq 6) = P(X &gt; 5)\\) so pgeom(4, .43, lower.tail = FALSE) # 1 - P(X &lt;= 5) = P(X &gt;5) = P(X &gt;=6) ## [1] 0.06016921 # or 1 - (dgeom(0, .43) + dgeom(1, .43) + dgeom(2, .43) + dgeom(3, .43) + dgeom(4, .43)) ## [1] 0.06016921 What is the third quartile of this distribution? qgeom(.75, .43) ## [1] 2 As with a binomial random variable, note that pgeom(2, .43) ## [1] 0.814807 is greater than .75; R finds the nearest integer yielding a larger percentile. 5.2.2 Continuous random variables As mentioned earlier in this Chapter, we will encounter numerous families of continuous distributions throughout this course. The normal distributions are likely the most important for our purposes. Moreover, once you get a feel for making normal distribution calculations in R, youll be in a great place for understanding and making calculations using any of the remaining distributions. Thus, we will mostly focus on normal distributions in this section. We remind the reader that if \\(X\\) is a continuous random variable, then \\(X\\) has an associated function called a probability density function \\(f\\). An area between the graph of \\(f\\) and the horizontal axis represents a probability. With this in mind \\(P(X = k) = 0\\) for any value \\(k\\) since the area of a line is 0. Thus: \\(P(X \\leq k ) = P(X &lt; k)\\) and \\(P(X \\geq k ) = P(X &gt; k)\\) For us, from a calculation perspective, this is one of the main differences between discrete and continuous random variables. 5.2.2.1 Normal distributions Long before you enrolled in this class you likely interacted with normal distributions: they are the famous bell-shaped curves. In more mathematical language, they are symmetric, unimodal distributions. Note that they are not the only distributions with these properties, but in some sense, they are the most common. Normal distributions arise often in practice because many naturally observed quantities follow approximately normal distributions (eg height, weight, etc of populations of critters) and the distributions of many sample statistics are approximately normal. For instance, imagine all possible random samples of 1000 Flathead county residents. For each of these samples, record the proportion of people who identify as politically Republican. This is a random variable that is approximately normally distributed. More on this in later chapters - this is a quick example of a sampling distribution which a key tool for statistical inference. From an intuitive perspective, a bell shaped curve should be determined by two quantities: its center (determining the line of symmetry) and its spread. In fact, these two quantities are the parameters determining a normal distribution! We call the center the mean \\(\\mu\\) and the spread standard deviation \\(\\sigma\\). If a random variable \\(X\\) we may sometimes write \\[ X \\sim N( \\mu, \\sigma ) \\] to indicate that \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). There is one special distinguished normal distribution that we call the standard normal distribution: it has a mean of \\(\\mu = 0\\) and a standard deviation of \\(\\sigma = 1\\). We call this the standard normal distribution and typically denote it with \\(Z\\), so \\[ Z \\sim N(\\mu = 0, \\sigma = 1). \\] The plot below shows the standard normal curve. ggplot( tibble( x = c(-3,3)), aes(x = x)) + stat_function(fun = dnorm) + xlab(&quot;Z&quot;) + ylab(&quot;&quot;) + labs(title = &quot;Prob. Density Fun. of Std. Normal Dist.&quot;) The standard normal is distinguished because any normal distribution \\(X\\) can be transformed into the standard normal by taking Z-scores. The \\(Z\\)-score of a measurement \\(x\\) from a normal distribution $ X N( , )$ is \\[ z = \\frac{x - \\mu}{\\sigma}.\\] Notice that if \\(x = \\mu\\), then \\(z = 0\\), \\(x = \\mu \\pm \\sigma\\), then \\(z = \\pm 1\\). From this we see that \\(X\\) in fact is transformed into the standard normal distribution. Now that weve reviewed the basics of normal distributions, lets see how we can make some probability calculations using R. It should be no surprise that the main functions are pnom(...) and qnorm(...). If \\(X \\sim N(m, s)\\) dnorm(k, m, s) is the height of the graph of the probability density function at \\(X = k\\). !Caution! this is not a probability! pnorm(k, m, s) \\(= P( X \\leq k)\\) pnorm(k, m, s, lower.tail = FALSE) \\(= P( X &gt; k) = P(X \\geq k)\\) qnorm(p, m, s) = k is the value \\(k\\) such that \\(P(X \\leq k) = p\\). qnorm(p, m, s, lower.tail = FALSE) = k is the value \\(k\\) such that \\(P(X &gt; k) = p\\). rnorm(n, m, s) returns a list of \\(n\\) randomly sampled numbers from \\(N(m, s)\\). Note: if you dont specify a mean or standard deviation, R will default to the standard normal distribution. Lets close this section with an example, starting with a sample and approximating the samples distribution with a normal distribution to make some probability estimates. The data well use is the bdims data set from the openintro package. This data set gives many measurements of 507 physically active individuals; we will investigate the height variable hgt, which records a participants height in centimeters. The sample contains data on both men and women; lets focus on men in this sample. maleBdim &lt;- bdims %&gt;% filter(sex == 1) Male height is approximately normally distributed as evidenced by the histogram below. ggplot(maleBdim, aes(x = hgt)) + geom_histogram(bins = 14, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + labs(x = &quot;height (cm)&quot;, title = &quot;Height of active men&quot;) the mean and standard deviation of this sample are m &lt;- mean(maleBdim$hgt) s &lt;- sd(maleBdim$hgt) c(m, s) ## [1] 177.745344 7.183629 and we can overlay a normal curve onto our histogram to heuristically assess the fit. ggplot(maleBdim, aes(x = hgt)) + # scales y-axis to a probability density geom_histogram(aes(y = ..density..), bins = 14, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + # adds our normal curve stat_function(fun = dnorm, args = list(mean = m, sd = s), color = &quot;red&quot;) + labs(x = &quot;height (cm)&quot;, title = &quot;Height of active men&quot;) Its not a perfect fit, of course, but its looking pretty good! If we think of our sample as an approximation of the population were sampling from, we can use the normal distribution above to make probability calculations for the population. Below well answer these questions and plot the regions corresponding to the probabilities in question. What is the probability a randomly sampled active male is shorter than 185 centimeters? First well plot the region. ggplot(tibble(height = c(155,200)), aes(x = height)) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;area&quot;, xlim = c(155, 185), #define fill region fill = &quot;steelblue&quot;) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;line&quot;, color = &quot;black&quot;) + labs(x = &quot;height (cm)&quot;, y = &quot;density&quot;, title = &quot;Height of active men, P(H &lt;= 185)&quot;) From the image above, we infer that the probability better be greater than .5. To make this calculation exactly we use pnorm(185, mean = m, sd = s) ## [1] 0.8437254 recalling that we stored the mean as m and the standard deviation as s above. What is the probability a randomly sampled active male is taller than 175 cm? Again well plot the region. ggplot(tibble(height = c(155,200)), aes(x = height)) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;area&quot;, xlim = c(175, 200), #define fill region fill = &quot;steelblue&quot;) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;line&quot;, color = &quot;black&quot;) + labs(x = &quot;height (cm)&quot;, y = &quot;density&quot;, title = &quot;Height of active men, P(H &gt;= 175)&quot;) The exact probablity can be calculated as pnorm(175, mean = m, sd = s, lower.tail = FALSE) ## [1] 0.6488312 What is the probability a randomly sampled active male is between 165 and 175 cm? Again well plot the region. ggplot(tibble(height = c(155,200)), aes(x = height)) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;area&quot;, xlim = c(165, 175), #define fill region fill = &quot;steelblue&quot;) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;line&quot;, color = &quot;black&quot;) + labs(x = &quot;height (cm)&quot;, y = &quot;density&quot;, title = &quot;Height of active men, P(165 &lt;= H &lt;= 175)&quot;) To calculate the probability, observe that \\(P(165 \\leq H \\leq 175) = P(H \\leq 175) - P(H \\leq 165)\\). In other words, we take the larger lower tail and subtract off the smaller lower tail. Making this calculation in R: pnorm(175, mean = m, sd = s) - pnorm(165, mean = m, sd = s) ## [1] 0.3131555 You meet someone online and they say that theyre an active guy in the 80th percentile for height. How tall is this person? We are given a probability/percentile and want the value associated to that, so we need to use the qnorm(...) function. In particular, qnorm(.8, mean = m, sd = s) ## [1] 183.7912 We can check this calculation with pnorm(...). pnorm(183.7912, mean = m, sd = s) ## [1] 0.7999985 Pretty good! This means if youre about 183.8 cm tall (about 6ft.) youd be in the 80th percentile for active adult US men. These calculations and visuals should help you get started in calculating normal distribution probabilities. It is crucial to visualize or even actually sketch out the probability youre trying to calculate, especially when youre getting started. Visualizing these probabilities helps you figure out exactly what combination of R functions you need to use. The next distributions will be used later in this course when we discuss more involved types of sample statistics. 5.2.2.2 Chi-squared (or \\(\\chi^2\\) ) distributions This section will be completed later in the semester when these distributions are needed. 5.2.2.3 Students \\(t\\)-distribution The Students \\(t\\)-distributions are a family of symmetric, unimodal distributions that have fatter tails than the standard normal distribution. We end up needing to use \\(t\\)-distributions when were analyzing single sample and two sample numerical data because the sample standard deviation is a biased estimator of the population standard deviation. We fatten up the tails of a normal distribution to account for the added uncertainty of this biased estimate. The uncertainty is larger when our sample size is small and decreases as the sample size increases. The parameter that changes the area for the \\(t\\)-distributions is called degrees of freedom or \\(df\\). In fact, this is the only parameter needed to determine a \\(t\\)-distribution. As \\(df\\) goes to infinity, the corresponding \\(t\\)-distributions limit to the standard normal distribution. The plot below helps to see how this limiting occurs. ggplot(data.frame(x = c(-3,3)), aes(x = x)) + stat_function(fun = dnorm, geom = &quot;line&quot;, aes(color = &quot;a: normal&quot;), show.legend = T) + stat_function(fun = dt,args = c(df = 1), geom = &quot;line&quot;, aes(color = &quot;b: df = 1&quot; ), alpha = 1, show.legend = T) + stat_function(fun = dt,args = c(df = 2), geom = &quot;line&quot;, aes(color = &quot;c: df = 2&quot; ), alpha = 1/2, show.legend = T) + stat_function(fun = dt,args = c(df = 4), geom = &quot;line&quot;, aes(color = &quot;d: df = 4&quot;), alpha = 1/4, show.legend = T) + stat_function(fun = dt,args = c(df = 8), geom = &quot;line&quot;, aes(color = &quot;e: df = 8&quot;), alpha = 1/8, show.legend = T) + stat_function(fun = dt,args = c(df = 16), geom = &quot;line&quot;, aes(color = &quot;f: df = 16&quot;), alpha = 1/16, show.legend = T) + stat_function(fun = dt,args = c(df = 32), geom = &quot;line&quot;, aes(color = &quot;g: df = 32&quot;), alpha = 1/32, show.legend = T) + stat_function(fun = dt,args = c(df = 64), geom = &quot;line&quot;, aes(color = &quot;h: df = 64&quot;), alpha = 1/64, show.legend = T) + scale_color_manual(values = c(&quot;steelblue&quot;, rep(&quot;black&quot;, 7))) + labs(x = &quot;t&quot;, y = &quot;density&quot;, title = &quot;t-distributions with varying degrees of freedom&quot;) The main workhorse functions for making \\(t\\)-distribution calculations are, no surprise, pt(q, df) and qt(p, df). Well make a few quick calculations with these functions, but they work exactly like pnorm and qnorm, so if youre comfortable with those functions, \\(t\\)-distributions will be a breeze. What proportion of values fall below -2 in a \\(t\\) distribution with \\(df = 10\\)? First, well visualize this proportion as an area. ggplot(tibble(t= c(-3,3)) , aes(x = t)) + stat_function( fun = dt , args = c(df = 10)) + stat_function(fun = dt , args = c(df = 10) , geom = &#39;area&#39; , fill = &#39;steelblue&#39; , xlim = c(-3, -2)) Following the lead from normal distributions this proportion is given by pt(-2, df = 10) ## [1] 0.03669402 To emphasize that the tails of \\(t\\)-distribtuions are fatter, notice that the same proportion for the standard normal distribution is pnorm(-2) ## [1] 0.02275013 While it might not seem like a huge difference, this will make an impact once we get to statistical inference! What proportion of values fall between -1 and 1 in a \\(t\\)-distribution with \\(df = 16\\)? First, well visualize this proportion as an area in steelblue with the same central area of a normal distribution overlayed in red. ggplot(tibble(t= c(-3,3)) , aes(x = t)) + stat_function( fun = dt , args = c(df = 16) , color = &#39;steelblue&#39;) + stat_function(fun = dt , args = c(df = 16) , geom = &#39;area&#39; , fill = &#39;steelblue&#39; , xlim = c(-1, 1)) + stat_function(fun = dnorm , args = c(mean = 0, sd = 1) , color = &#39;red&#39;) + stat_function(fun = dnorm , args = c(mean = 0, sd = 1) , geom = &#39;area&#39; , fill = &#39;red&#39; , xlim = c(-1,1) , alpha = .25) We see that since the \\(t\\)-disribution has fatter tails, the central area is less than that of the strandard normal distribution. There are many ways to make the calculation at hand, but the following code chunk gives one easy way. pt(1, df = 16 ) - pt(-1, df = 16) ## [1] 0.667805 Lets contrast that with the same area for a standard normal curve. pnorm(1) - pnorm(-1) ## [1] 0.6826895 As expected, the proportion for the standard normal is greater than that of the \\(t\\)-distribution. What value corresponds to the 90th percentile in a \\(t\\)-distribution with \\(df = 6\\)? Since were given a proprotion and want a value, this calculation requires qt. Its a simple one since the 90th percentile corresponds to a lower tail area. Well also print out the corresponding value for the standard normal distribution for comparison. qt(.9, df = 6) ## [1] 1.439756 qnorm(.9) ## [1] 1.281552 As expected, the 90th percentile in a \\(t\\)-distribution is larger (ie further away from the mean) than it is for the standard normal distribution. If \\(T\\) is a \\(t\\)-distribution with \\(df = 11\\), what value \\(t^\\star\\) corresponds to 95% of the central area? In other words, find the value \\(t^star\\) so that \\(P(|T| \\leq t^\\star) = .95\\). These types of calculations will become important for us in the inferential statistics portion of the class. The image below shows the central area were trying to find, but we hide the code because it gives away the answer. From the image above, we expect the value to be a little larger than 2. We can find this value using some quick reasoning that will become second hand to you. Since the central area is .95 the remaining tail area is .05. The tails are symmetric, so each tail has an area of .025. This means we want to find the value \\(t^\\star\\) corresponding to an upper tail area of \\(.025\\). This value is calculated below. qt(.025, df = 11, lower.tail = FALSE) ## [1] 2.200985 And as a quick double check, lets make sure the corresponding central area is .95. tStar &lt;- qt(.025, df = 11, lower.tail = F) pt(tStar, df = 11) - pt(-tStar, df = 11) ## [1] 0.95 Success! Next up, well look at \\(F\\)-distributions which are not symmetric and unimodal. 5.2.2.4 \\(F\\)-distributions This section will be completed later in the semester when these distributions are needed. Now that weve finished up the basics of of probability, were ready to move on to inferential statistics! "],["inferential-statistics-take-1.html", "Chapter 6 Inferential statistics: Take 1 6.1 Basics of statistical inference 6.2 Analyzing categorical variables 6.3 Analyzing numerical variables", " Chapter 6 Inferential statistics: Take 1 This chapter and the next are going to introduce inferential statistics in R, but from two different perspectives. This chapter will focus on doing all of the calculations involved in statistical inference by hand (where here we really mean using R to implement the formulas/ideas). The next chapter will show you how to perform many of the same calculations using functions native to R. It is valuable knowing how to do both so that you can check one or the other or possibly write your own type of statistical test one day! Both chapters will be organized according to the type of variables we will be analyzing. So, for example, well start with statistical inference for a single categorical variable. One can (and probably should) read the by hand section of this chapter in tandem with the corresponding native function section of the next chapter. This chapter will include a brief summary of the ideas involved in statistical inference, but the next will just go straight into the details. The code chunk below attaches all packages well use in this chapter and sets a seed which basically determines a starting point for some of the randomization well employ in the chapter. Setting a seed insures that the same random samples will be drawn every time the code is run. library(tidyverse) library(knitr) library(openintro) library(infer) library(patchwork) set.seed(1187) 6.1 Basics of statistical inference We will learn about three primary types of statistical inference in this class, each aimed at answering different types of questions. To illustrate the types of questions we endeavor to answer, think about analyzing the political landscape in America. You could ask: What proportion of Americans identify as politically independent? Are there more politically independent people in Montana than in Idaho? How what proportion of voters will identify as politically independent in the future? The first question is asking for an estimate of a population parameter (the true proportion of independents in America). The second question, on the other hand, is asking about a difference between two population parameters. The third requires a prediction based on past data. These types of questions are certainly related (for instance, you could answer the second question by estimating each of the two population parameters), but the most common tools for answering them are slightly different. Our main tool for answering the first question above is a confidence interval, which uses an estimate of the amount of variable we expect between samples to provided a range of plausible values for the population parameter. The tool we use to answer the second question is called a hypothesis test; these test assess how likely or unlikely your sample is if there were no differences. Hypothesis tests involve understanding the amount of variability we expect to see between samples. We access this this quantity by understanding the distribution of possible samples. The probability distribution associated to all possible samples is called a sampling distribution, which we discuss more in the next section. 6.1.1 Sampling distributions Taking a simple random sample is, of course, a random process. Assigning a number like the sample proportion or sample average to this sample, then, naturally turns sample statistics into random variables. Since we can think of each sample statistic as a random variable, each sample statistic has an associated probability distribution called a sampling distribution! The overall population under consideration determines the sampling distribution and we almost never have access to population-level data, so you may wonder how or if we have any hope of understanding the sampling distribution of our statistic of interest. In particular, we hope to understand the shape, the center, and the spread of the sampling distribution. There are two primary was of doing this: through simulation and through theory. This class touches briefly on the simulation approach, but focuses mostly on the theoretical approach to accessing sampling distributions. Before moving on to describe these approaches, we need a quick definition that helps us differentiate from talking about a population distribution and talking about a sampling distribution. The standard error, abbreviated SE, of a sampling distribution is simply its standard deviation. One of the miracles of statistics (Note: it is not a real miracle since it is a mathematical theorem, but it amazes the author to this day.) is the Central Limit Theorem. We will state the Central Limit Theorem in a few different ways throughout this chapter, but each version essentially says the same thing. Namely, if you have a sufficiently large (caution: this means different things in different contexts) and high quality sample (IE a sample with independent observations, usually obtained through random sampling), the sampling distribution will be symmetric and unimodal with its center at the true population parameter. Moreover, as the sample size increases, the standard error decreases. In other words, all sample statistics will cluster around the true population parameter and observing a sample stat above the population parameter is just as likely as observing one below. The fact that the standard error decreases as the sample size increases codifies our intuition that large samples are usually better, since it implies that a sample statistic calculated from a larger sample is more likely to be close to the true population parameter than one calculated from a smaller sample. Before moving on to discuss how we use sampling distributions, lets try to make this a bit more concrete by simulating a sampling distribution using a technique called bootstrapping which takes a large, random sample, and resamples it many times with replacement to simulate the process of taking many different samples. Well use the gss2010 data set from the openintro package which records the highest educational attainment (among other things) of 2044 randomly sampled Americans in 2010. Suppose were interested in knowing more about the proportion of all Americans who do not have a high school diploma. The gss2010 data set serves as our sample and we can use the sample proportion \\(\\hat{p}\\), read as p hat, as a point estimate for the true population proportion \\(p\\). pHat &lt;- sum(gss2010$degree == &quot;LT HIGH SCHOOL&quot;) / length(gss2010$degree) pHat ## [1] 0.1492172 So our sample indicates that in 2010, about 15% of the American population had less than a high school diploma. But this is only a sample of 2044 Americans. If we took another sample of 2044 people and calculated the same proportion, how different do we expect that proportion to be? We can simulate this by drawing from our original sample, but doing so with replacement. # Take sample sample2 &lt;- sample( gss2010$degree, size = 2044, replace = TRUE) #Find proportion of high school dropouts pHat2 &lt;- sum(sample2 == &quot;LT HIGH SCHOOL&quot;) / length(sample2) pHat2 ## [1] 0.1364971 This sample proportion is different, but not by too much. Of course, 2 different samples dont provide enough evidence to make any conclusions about the amount of variability we should expect among all possible samples. To estimate the standard error, we need to simulate many more samples. Well use the following code, using commands from the infer package, to do this. The code below takes 5000 samples of size 2044 from our original sample again with replacement, changes the degree column in each sample to a binary outcome (no HS diploma, or other), counts the number of people in each sample without a high school diploma, then calculates the corresponding sample proportion. simulated_sampling_distr &lt;- gss2010 %&gt;% rep_sample_n(size = 2044, reps = 5000, replace = TRUE) %&gt;% mutate( degree = if_else(degree == &quot;LT HIGH SCHOOL&quot;, &quot;LT HIGH SCHOOL&quot;, &quot;other&quot;)) %&gt;% count(degree) %&gt;% mutate(p_hat = n / sum(n)) %&gt;% filter( degree == &quot;LT HIGH SCHOOL&quot;) head(simulated_sampling_distr) ## # A tibble: 6 x 4 ## # Groups: replicate [6] ## replicate degree n p_hat ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 LT HIGH SCHOOL 322 0.158 ## 2 2 LT HIGH SCHOOL 315 0.154 ## 3 3 LT HIGH SCHOOL 317 0.155 ## 4 4 LT HIGH SCHOOL 310 0.152 ## 5 5 LT HIGH SCHOOL 314 0.154 ## 6 6 LT HIGH SCHOOL 311 0.152 Weve simulated a sampling distribution! Lets look at a histogram of this simulated distribution to assess its shape and spread. Remember, the Central Limit Theorem says this distribution should likely by symmetric and unimodal. ggplot(simulated_sampling_distr, aes(x = p_hat)) + geom_histogram(bins = 15, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + labs(title = &quot;Distribution of sample proportions, n = 2044&quot;, subtitle = &quot;Estimating SE for 2010 proportion of Americans without a HS diploma&quot;, x = &quot;Sample proportion, p-hat&quot;) Voila! Our sampling distribution does seem to be symmetric and unimodal. Where precisely is the center? #average sample proportion mean(simulated_sampling_distr$p_hat) ## [1] 0.1492041 What, approximately, is the standard error? Simply the standard deviation of the column p_hat! sd(simulated_sampling_distr$p_hat) ## [1] 0.007862322 This means if we were to repeatedly sample 2044 Americans and record the proportion of people with less than a college degree, we should expect to see a difference of about 0.8% between the proportions on average. As will see in the next two sections, understanding the standard error is the key to performing inferential statistics. However, before we move on to using sampling distributions, lets simulate the Central Limit Theorems other main claim: the standard error decreases as the sample size increases. To show this, well go the opposite direction and take smaller samples from the gss2010 data set. We should see the standard error of this simulated distribution increase. The code below simulates a sampling distribution with sample size \\(n = 100\\). sampling_dist2 &lt;- gss2010 %&gt;% rep_sample_n(size = 100, reps = 5000, replace = TRUE) %&gt;% mutate( degree = if_else(degree == &quot;LT HIGH SCHOOL&quot;, &quot;LT HIGH SCHOOL&quot;, &quot;other&quot;)) %&gt;% count(degree) %&gt;% mutate(p_hat = n / sum(n)) %&gt;% filter( degree == &quot;LT HIGH SCHOOL&quot;) head(sampling_dist2) ## # A tibble: 6 x 4 ## # Groups: replicate [6] ## replicate degree n p_hat ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 LT HIGH SCHOOL 8 0.08 ## 2 2 LT HIGH SCHOOL 20 0.2 ## 3 3 LT HIGH SCHOOL 21 0.21 ## 4 4 LT HIGH SCHOOL 15 0.15 ## 5 5 LT HIGH SCHOOL 15 0.15 ## 6 6 LT HIGH SCHOOL 20 0.2 Note that just looking at the first 6 sample proportions we can already see more variability than our first distribution. Lets visualize this one. ggplot(sampling_dist2, aes(x = p_hat)) + geom_histogram(bins = 15, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + labs(title = &quot;Distribution of sample proportions, n = 100&quot;, subtitle = &quot;Estimating SE for 2010 proportion of Americans without a HS diploma&quot;, x = &quot;Sample proportion, p-hat&quot;) Notice that the center is about the same as our first sampling distribution, but our second is more spread out as we claimed above! To check these claims, lets look at the average sample proportion and the estimated standard error. mean(sampling_dist2$p_hat) ## [1] 0.14884 sd(sampling_dist2$p_hat) ## [1] 0.0353421 The average sample proportions are super close, but the standard error of our distribution with \\(n = 100\\) is about 5 times as large as the standard error of the distribution with \\(n = 2044\\). This fits our intuition: small samples are less reliable because they have more variability around the population parameter. Now that we have a feel for sampling distributions, lets use these simulated sampling distributions to make some inferences. 6.1.2 Confidence intervals Recall that a confidence interval provides a range of plausible values for a population parameter. For example, we will use our point estimates from the previous section to estimate the true proportion of Americans without a high school diploma. The width of a confidence level depends on two quantities: the confidence level: a measure of the likelihood that our interval contains to true population parameter. Typical confidence levels are 90%, 95%, and 99%. the standard error: the standard deviation of the sampling distribution. Your confidence level determines a critical value \\(CV\\). If your confidence level is \\(CL\\), then \\(CL\\)% of all sample proportions will fall within \\(CV\\) standard deviations of the true population parameter within the sampling distribution. Thus, as your confidence level increase, the critical value increases, and so your confidence interval ends up getting wider. This makes sense: to be more confident in your estimate, your range of plausible values better be larger. Now we can describe the general formula for confidence intervals: \\[ \\text{point estimate} \\pm CV \\cdot SE\\] In the sections to come, the details of your point estimates along with the procedure to calculate the standard error and critical value will change, but the general formula for a confidence interval is always the same. With this in mind, it becomes much more important to be able to correctly interpret confidence intervals. Before closing out this section, lets use our simulated sampling distribution from the previous section to estimate the proportion of Americans without a high school diploma in 2010. In particular, well calculate a 95% confidence interval. Looking back at the histogram of simulated_sampling_distr, we see that it is approximately normally distributed. Recall that roughly 95% of all observations in a normal distribution fall within two standard deviations from the center. Thus, as a rough approximation, our critical value is \\(CV = 2\\). Weve also already found our point estimate, the sample proportion of Americans without a high school diploma pHat, and our standard error, the standard deviation of the sampling distribution. pHat ## [1] 0.1492172 SE &lt;- sd(simulated_sampling_distr$p_hat) SE ## [1] 0.007862322 Using the formula, \\(\\text{point estimate} \\pm CV \\cdot SE\\), our 95% confidence interval is CI &lt;- pHat + c(-1,1)*2*SE CI ## [1] 0.1334926 0.1649419 In other words, with 95% confidence, we estimate that between 13.3% and 16.5% of all Americans in 2010 did not have a high school diploma. 6.1.3 Hypothesis tests We use confidence intervals to estimate population parameters, but sometimes we only want to figure out if a population parameter differs from some value. This situation arises when you ask questions like Will a majority of American Republicans vote for Donald Trump in the 2024 presidential election primary? or Do more than 10% of Americans fail to graduate high school? To answer these types of questions statistically, we perform hypothesis tests. Every hypothesis test has two competing claims: \\(H_0\\), the null hypothesis. This typically represents the status quo, that there is no difference, that there is no effect, or that the variables independent. \\(H_a\\): the research or alternative hypothesis. This represents, well, your research question. There is a difference, or an effect, or the variables in question are dependent. We always express the null and alternative hypothesis as statements involving population parameters. For instance, if \\(p\\) represents the proportion of Americans without a high school diploma in 2010, we would turn the question Do more than 10% of Americans fail to graduate high school? into a hypothesis test as \\(H_0 : p = .1\\) (or \\(p \\leq .1\\)) \\(H_a : p &gt; .1\\) We use sample data to evaluate hypothesis tests and do so by determining how likely or unlikely our sample data would be if the null hypothesis were true. This is called the \\(p\\)-value of a sample. More precisely, the \\(p\\)-value of a sample is the probability of observing a sample at least as a favorable for the alternative hypothesis as your own, assuming the null hypothesis is true. Lets think about what a \\(p\\)-value tells is. If you have a relatively small \\(p\\)-value, say .01, then there would be a 1% chance of observing the data you have (or data thats even more extreme) if the null were true. We have two competing possibilities here: either the null hypothesis is true and we have witnessed a rare event simply by chance or the alternative hypothesis is true. Unfortunately (and this is especially if you only have a single sample) theres no way to tell which world youre living in! As an attempt to remedy this dilemma, we must set a significance level before performing our hypothesis test. The most common significance levels are \\(\\alpha = .01, .05, \\text{ and } .1\\). You use the significance level \\(\\alpha\\) to evaluate the hypothesis test. In particular, if the \\(p\\)-value is less than \\(\\alpha\\), reject the null and accept the alternative. Or the \\(p\\)-value is greater than or equal to \\(\\alpha\\), you fail to reject the null. Note that a hypothesis test is like a jury trial: a jury either finds the defendant guilty or not guilty, they never deem a defendant innocent. Similarly, we can never prove the null hypothesis; instead, we can only collect enough evidence to believe that it is false. With this in mind, we think of the significance level \\(\\alpha\\) as the probability of a false positive in a hypothesis test. In other words, if the null were true and you could repeatedly perform the same hypothesis test, you would end up rejecting the null \\(\\alpha\\)% of the time. In general, there are three broad types of hypothesis tests: one-tailed upper, one-tailed lower, and two-tailed tests. The type of test determines how you calculate the \\(p\\)-value of your sample because it determines what counts as more favorable for the alternative hypothesis. To illustrate this idea, lets pretend were performing a hypothesis test for a proportion with \\[ H_0: p = .5 \\] and suppose we took a sample with size \\(n = 200\\) and \\(\\hat{p} = .55\\). Below well look at the three types of alternative hypotheses and visualize the corresponding \\(p\\)-values for our imaginary sample. Two-sided hypothesis test. In this case, were simply checking to see if the population parameter deviates from the null value. For our example: \\[ H_a: p \\neq .5 \\] Samples that are at least as favorable for \\(H_a\\) as our own come in two flavors: samples with a proportion of at least .55 and those with a proportion of at most .45. Notice that the samples with proportions of at most .45 really are just as favorable as those with proportions of at least .55 when were checking for a difference because they are equally as far away from the null proportion of .5. Thus, the \\(p\\)-value of a two-sided hypothesis tests will be a two-tailed probability. The plot below helps visualize this \\(p\\)-value as an area in the sampling distribution. We have hidden the plotting code here to prioritize the concepts. One-sided upper hypothesis test: In this case, were simply checking to see if the population parameter is greater than the null value. For our example: \\[ H_a: p &gt; .5 \\] Samples that are at least as favorable for \\(H_a\\) as our own only one flavor this time: samples with a proportion of at least .55. Thus the \\(p\\)-value for this hypothesis test is an upper-tail area. The plot below helps us visualize this. One-sided lower hypothesis test: In this case, were simply checking to see if the population parameter is less than the null value. For our example: \\[ H_a: p &gt; .5 \\] Samples that are at least as favorable for \\(H_a\\) as our own only one flavor: samples with a proportion of at most .55. Thus the \\(p\\)-value for this hypothesis test is a lower-tail area in the sampling distribution. The plot below helps us visualize this. Our \\(p\\)-value is large because our sample proportion is above the null value when we thought it was going to be below. We should revise our alternative hypothesis in this case. Lets finish this section using the high school diploma question we started this chapter with. In particular, in 2010 did more than 10% of Americans fail to graduate high school? As a hypothesis test: \\(H_0: p = .1\\) \\(H_a: p &gt; .1\\) Lets set our significance level to \\(\\alpha .05\\). To evaluate the hypothesis test need to calculate a \\(p\\)-value. To accomplish this, we need to be able to assess the likelihood of our sample assuming the null hypothesis is true. Here were still using the gss2010 data for our sample; recall the recall the sample proportion is \\(\\hat{p} \\approx 0.149\\) and the sample size is \\(n = 2044\\). In the next section well learn some theory that describes the sampling distribution, but we will simulate a sampling distribution under the null hypothesis. # generates results from 5000 binomial experiments w/ probability of # success = .1 and sample size of n = 2044, returns sample proportion. null_dist &lt;- tibble(p_hat = rbinom(5000, 2044, .1)/2044) # center of distribution mean(null_dist$p_hat) ## [1] 0.1000021 # standard error SE &lt;- sd(null_dist$p_hat) SE ## [1] 0.006583714 Lets visualize the null distribution and our sample proportion \\(\\hat{p}\\)s location in the nul distribution. ggplot(null_dist, aes(x = p_hat)) + geom_histogram(bins = 15, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + geom_vline(xintercept = pHat, color = &quot;red&quot;) + labs(title = &quot;Null distribution with p = .1 and n = 2044, pHat = .149&quot;, x = &quot;Sample proportion, p-hat&quot;) As wed expect the null sampling distribution is approximately normal. The plot above indicates that our sample proportion \\(\\hat{p}\\) would be quite unlikely if the null hypothesis were true. To estimate exactly how unlikely, lets estimate the \\(p\\)-value of our sample. # count the number of simulated samples with proportions at least as large as # our actual sample proportion, divide by number of samples in simulated dist. pValue &lt;- sum(null_dist$p_hat &gt;= pHat ) / length(null_dist$p_hat) pValue ## [1] 0 This means that if 10% of Americans didnt have high school diplomas in 2010, the probability of finding a sample of 2044 Americans of which 14.9% or more do not have a high school diploma is essentially 0. Because our \\(p\\)-value is less than the significance level \\(\\alpha = .05\\), we reject the null hypothesis and accept the alternative. In other words, our data provide compelling evidence to believe that more than 10% of Americans in 2010 did not have a high school diploma. We finish this section by noting that we spent a lot of time on this example and a lot of time/space on explanation here. Once you get the hang of this, you will perform and interpret hypothesis tests quickly and efficiently. Moreover, most of what well do in this class will not require simulating sampling distributions. For more on this, read on! 6.2 Analyzing categorical variables Now we will move in to using theoretical sampling distributions to calculate confidence intervals and perform hypothesis tests. The key almost all of the theoretical sampling distributions is the central limit theorem, which states that, if you have a large enough sample of independent observations, the sampling distribution will be approximately normal. Moreover, the standard error decreases as your sample size increase. In the following sections we will see a few different incarnations of the central limit theorem; the big picture will stay the same while the fine details change as we analyze different types of data. This section focus on analyzing categorical variables. When making inferences about categorical variables it is best to analyze proportions because these generalize to populations much better than counts. In particular we want to look at the proportion of responses in a sample that take on a specific value of the categorical variable. Weve seen this already, of course (see the last section for an example). The following subsections describe how to make inferences using a single sample proportion, two sample proportions, or (in some sense) many sample proportions. 6.2.1 Single sample proportion When analyzing a single categorical variable, the most basic parameter to make an inference about is the population proportion \\(p\\), the proportion of the population taking on a particular value of the categorical variable. For example, the proportion of American adults in 2010 that do not have a high school diploma. We use sample proportions \\(\\hat{p}\\) to make inferences about \\(p\\) and under the right circumstances can apply the Central Limit Theorem to understand the distribution of sample proportions. Sample statistic: sample proportion \\(\\hat{p}\\) Population parameter: population proportion \\(p\\) The Central Limit Theorem for a Single-sample Proportion states that if you have a sample of \\(n\\) independent observations (hard to guarantee, but safe to assume if n &lt; 10% of the populatino and you employ random sampling) and and at least 10 (expected) success and failures (ie \\(\\hat{p}n \\geq 10\\) and \\((1- \\hat{p})n \\geq 10\\) ) Then the sampling distribution of \\(\\hat{p}\\) is approximately normal with a mean of \\(p\\) and standard error \\(SE = \\sqrt{ \\frac{p(1-p)}{n}}\\). In symbols: \\[ \\hat{p} \\sim N \\left( \\text{mean} = p, SE =\\sqrt{ \\frac{p(1-p)}{n}} \\right) \\] Moreover, if \\(p\\) is unknown (as it usually is!), \\[ SE \\approx \\sqrt{ \\frac{\\hat{p}(1-\\hat{p})}{n}}\\] In other words, you can approximate the standard error using the sample proportion. In this situation, the hypotheses you need to meet in order to apply the Central Limit Theorem are typically easy to satisfy, which means we can use the CLT to make inferences in all of our favorite ways! 6.2.1.1 Confidence intervals Suppose you have a sample of \\(n\\) observations that meets the conditions for applying the Central Limit Theorem. Then the distribution of sample proportions is approximately normal, centered at the true population proportion with a standard error of \\[ SE \\approx \\sqrt{ \\frac{\\hat{p}(1-\\hat{p})}{n}}.\\] We can use these facts to calculate a confidence interval to estimate the population proportion. Recall that every confidence interval has the form \\[ \\text{point estimate} \\pm CV \\cdot SE\\] where \\(CV\\) is the critical value determined by the confidence level. The CLT provides the standard error and our sample gives the point estimate, so we only need to worry about the critical value. Recall that if your confidence level is \\(CL\\), then \\(CL\\)% of all sample proportions will fall within \\(CV\\) standard deviations of the true population parameter within the sampling distribution. The sampling distribution is normal by the Central Limit theorem, so we can use the qnorm(...) function to find precise critical values. As an example, lets find the critical value for a 95% confidence interval. We want to find the value \\(z^\\ast\\) so that 95% of all sample proportions fall within \\(z^\\ast\\) standard deviations of the population proportion. In other words, we need to find \\(z^\\ast\\) so that \\(P ( |Z| \\leq z^\\ast ) = .95\\). The graph below helps visualize the setup. We can see that the bounds are almost \\(\\pm2\\), but not quite. If the central area is .95, then the tail area is .05. The upper and lower tails are symmetric, so the upper tail area is .025. Thus, to find our critical value, we simply evaluate qnorm(.025, lower.tail = FALSE) ## [1] 1.959964 Lets use the same idea to find the critical values for some common confidence levels. cvTable &lt;- tibble( conf.level = c(.9, .95, .98, .99), crit.val = qnorm( (1 - conf.level)/2, lower.tail =F)) cvTable ## # A tibble: 4 x 2 ## conf.level crit.val ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9 1.64 ## 2 0.95 1.96 ## 3 0.98 2.33 ## 4 0.99 2.58 With that taken care of, we can calculate confidence intervals for any proportion we want at any confidence level! In particular, an \\(\\alpha\\)% confidence level for a single proportion is \\[\\hat{p} \\pm z^\\ast_\\alpha \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}. \\] Note there that this really is the same formula as above just with the details of the CLT filled in. Lets close this off with an example, recreating what we did in the last section to estimate the proportion of American adults without high school degrees. As a reminder, this is from the gss2010 data set from the openintro package. The code chunk below goes through the calculation of a 95% confidence interval for the proportion in question. # components of CI pHat &lt;- sum( gss2010$degree == &quot;LT HIGH SCHOOL&quot;) / length(gss2010$degree) SE &lt;- sqrt(pHat*(1-pHat)/length(gss2010$degree)) zCrit &lt;- qnorm( (1 - .95)/2, lower.tail = FALSE) # CI pHat + c(-1,1)*zCrit*SE ## [1] 0.1337709 0.1646636 So with 95% confidence, between 13.4% and 16.5% of all Americans in 2010 did not have a high school diploma. Compare this to the simulated example from the previous section: the confidence intervals are almost identical! You can think of this as the central limit theorem verifying simulations or simulations verifying the central limit theorem. In their case, its cool! Next up, well learn how to perform hypothesis tests using theoretical sampling distributions. 6.2.1.2 Hypothesis testing Suppose you have a sample of \\(n\\) observations and youd like to perform a hypothesis test using this sample. Recall that you assume the null hypothesis is true when performing a hypothesis test. Thus, in order to apply the Central Limit Theorem, we need to make sure our sample is sufficiently large under the null hypothesis. More explicitly, suppose the null proportion is \\(p_0\\). If your sample has \\(n\\) independent observations and your sample has at least 10 expected success and failures, ie \\(n p_0 \\geq 10\\) and \\(n (1- p_0) \\geq 10\\) then the distribution of sample proportions is approximately normal: \\[ \\hat{p} \\sim N\\left( \\text{mean} = p_0 , SE = \\sqrt{\\frac{p_0(1-p_0)}{n}} \\right). \\] We can use use this sampling distribution to calculate \\(p\\)-values thus execute hypothesis tests. We will show how to do this first with the toy examples from the last section which demonstrate the three types of hypothesis tests then one last time using the familiar gss2010 data. For the next three examples, suppose we have a sample of \\(n = 200\\) independent observations and a sample proportion of \\(\\hat{p} = .55\\). We will perform three hypothesis tests to see if our sample provides sufficient evidence to conclude that the true population proportion is greater than .5, less than .5, or different from .5 using a siginificance level of \\(\\alpha = .05\\). In all cases, note that \\(.5*200 = 100 \\geq 10\\), so the Central Limit Theorem applies. Furthermore \\[ \\hat{p} \\sim N\\left(\\text{mean} = .5, SE = \\sqrt{\\frac{.5\\cdot .5}{200}} \\right) \\] Two-sided hypothesis test: Our hypothesis test is \\[\\begin{array}{cc} H_0: p = .5 \\\\ H_a: p \\neq .5 \\end{array} \\] and the image below helps us visualize the \\(p\\)-value of our sample with \\(\\hat{p} = .55\\) The upper and lower tails of the \\(p\\)-value are symmetric about the mean, so we can calculate one of them and double it. pNull &lt;- .5 SE &lt;- sqrt(pNull*(1 - pNull)/200) pHat &lt;- .55 pVal &lt;- 2*pnorm(.55, mean = pNull, sd = SE, lower.tail = F) pVal ## [1] 0.1572992 With a \\(p\\)-value of approximately .16 we fail to reject the null hypothesis. The data do not provide compelling evidence of a difference. One-Sided Upper Hypothesis test: Our hypothesis test is \\[\\begin{array}{cc} H_0: p = .5 \\\\ H_a: p &gt; .5 \\end{array} \\] and the image below helps us visualize the \\(p\\)-value of our sample with \\(\\hat{p} = .55\\) The \\(p\\)-value is the shaded upper tail area and can be calculated as follows. pNull &lt;- .5 SE &lt;- sqrt(pNull*(1 - pNull)/200) pHat &lt;- .55 pVal &lt;- pnorm(.55, mean = pNull, sd = SE, lower.tail = F) pVal ## [1] 0.0786496 With a \\(p\\)-value of approximately .08 we fail to reject the null hypothesis at a significance level of \\(\\alpha =.05\\) . The data do not provide compelling evidence that the true proportion is greater than .5. One-Sided Lower Hypothesis test: Our hypothesis test is \\[\\begin{array}{cc} H_0: p = .5 \\\\ H_a: p &lt; .5 \\end{array} \\] and the image below helps us visualize the \\(p\\)-value of our sample with \\(\\hat{p} = .55\\) The \\(p\\)-value is the shaded llower tail area and can be calculated as follows. pNull &lt;- .5 SE &lt;- sqrt(pNull*(1 - pNull)/200) pHat &lt;- .55 pVal &lt;- pnorm(.55, mean = pNull, sd = SE) pVal ## [1] 0.9213504 No suprise here. With a \\(p\\)-value of approximately .92 we fail to reject the null hypothesis at a significance level of \\(\\alpha =.05\\) . The data do not provide compelling evidence that the true proportion is less than .5. We close this section using the gss2010 data to perform a hypothesis test to see if more than 10% of Americans in 2010 did not have a high school diploma. Again well use a significance level of \\(\\alpha = .05\\). Recall the following sample size and statistic from earlier. n &lt;- length(gss2010$degree) n ## [1] 2044 pHat &lt;- sum(gss2010$degree == &quot;LT HIGH SCHOOL&quot;)/n pHat ## [1] 0.1492172 We reformulate our question as a hypothesis test. \\[\\begin{array}{cc} H_0: p = .1 \\\\ H_a: p &gt; .1 \\end{array} \\] We assume the observations in the gss2010 data set are independent. Before preceding with the hypothesis test, we need to make sure our sample size is sufficiently large so that we can use the Central Limit Theorem. pNull &lt;- .1 # will return a logical, T/F n*pNull &gt;= 10 ## [1] TRUE n*(1 - pNull) &gt;= 10 ## [1] TRUE Since the calculation above returned two TRUEs were good to go! The CLT implies. \\[ \\hat{p} \\sim N \\left(\\text{mean} = .1, SE = \\sqrt{\\frac{.1\\cdot.9}{2044}} \\right)\\] We are performing a one-sided upper hypothesis test, so our \\(p\\)-value is SE &lt;- sqrt(pNull*(1- pNull)/n) pValue &lt;- pnorm(pHat, mean = pNull, sd = SE, lower.tail = F) pValue ## [1] 5.983973e-14 Since \\(0 &lt; .05 = \\alpha\\), we reject the null and accept the research. Our data provide sufficient evidence to believe that more than 10% of Americans in 2010 did not have a high school diploma. This \\(p\\)-value is the same as the one we calculated in the previous section! This is no surprise: you can either think of this example as a check on simulation techniques or as the simulation techniques as a check on the central limit theorem. 6.2.2 Two sample proportion We often want to compare the same measurement or parameter for two different populations. As usual, we almost never know any single population parameter, so the best we can do is take a sample from each population and compare the sample statistics. When analyzing a categorical variable, we end up comparing sample proportions to make inferences about population proportions. To make this more precise, suppose youre investigating some proportion for two populations. Our population parameter of interest is \\(p_1 - p2\\) Our sample statistic is \\(\\hat{p}_1- \\hat{p}_2\\) To make an inference about the population parameter, we need to learn about the sampling distribution of \\(\\hat{p}_1- \\hat{p}_2\\). Fortunately, if we can apply the central limit theorem for each sample individually, a version of the central limit theorem applies again! The formula for the standard error looks more complicated, but note that the variance of the sampling distrubtion for \\(\\hat{p}_1- \\hat{p}_2\\) is just the sum of the variances for each each of the individual sampling distributions. CLT for two sample proportions: Suppose you are analyzing the difference between two population proportions \\(p_1\\) and \\(p_2\\), take two samples of size \\(n_1\\) and \\(n_2\\) from each population respectively. If the observations within each sample are independent from one another, the observations between the samples are independent, and each sample has at least ten expected or observed success and failures (ie \\(n_ip_i \\geq 10\\) and \\(n_i(1 - p_i) \\geq 10\\) for \\(i= 1,2\\)) Then the sampling distribution for \\(\\hat{p}_1- \\hat{p}_2\\) is approximately normally distributed. In particular, \\[ \\hat{p}_1- \\hat{p}_2 \\sim N\\left( \\text{mean} = p_1 - p_2 \\text{ and } SE = \\sqrt{\\frac{p_1(1-p_1)}{n_1}+ \\frac{p_2(1-p_2)}{n_2}} \\right) \\] The details on how we employ the CLT for two sample proportions changes depending on whether wed like to calculate a confidence interval or perform a hypothesis test. As above, this difference arise because always assume the null hypothesis is true when performing a hypothesis test. The following subsections highlight exactly what these differences entail, but, briefly, it only changes the standard error calculation. 6.2.2.1 Confidence intervals As a reminder, the general formula for a confidence interval is \\[\\text{point est.} \\pm (\\text{crit. val.})\\cdot SE \\] so to calculate and interpret a confidence interval for two sample proportions, we simply need to figure out each of these three pieces. Well do so by example using the cancer_in_dogs data set from the openintro R package. This data set is the result of a 1994 study from 1994 designed to determine if exposure to the herbicide herbicide 2,4-Dichlorophenoxyacetic acid (2,4-D) increases dogs risk of developing cancer. Lets calculate a 95% confidence interval estimating the difference in cancer rates between dogs exposed to the herbicide and those who are not. There are 491 dogs in the data set who were exposed and 945 dogs that were not. The data set has two columns: order and response; the first indicates whether or not the dogs were exposed to the herbicide and the second indicates whether or not the dog developed cancer. Below we display 4 randomly selected rows from the dataset. slice_sample(cancer_in_dogs, n = 4) ## # A tibble: 4 x 2 ## order response ## &lt;fct&gt; &lt;fct&gt; ## 1 no 2,4-D no cancer ## 2 2,4-D cancer ## 3 2,4-D cancer ## 4 2,4-D cancer Let \\(p_1\\) denote the true proportion of dogs who are exposed to the herbicide and develop cancer; let \\(p_2\\) denote the proportion of dogs who are not exposed but still develop cancer. We need to determine our sample proportions \\(\\hat{p}_1\\) and \\(\\hat{p}_2\\). There are many ways to do this in R. A quick way is to use the table(...) function. Since the table only has two columns we can run the following to create a simple two-way table and store it in memory for future use. sumTab &lt;- table(cancer_in_dogs$response, cancer_in_dogs$order) sumTab %&gt;% kable() 2,4-D no 2,4-D cancer 191 300 no cancer 304 641 The columns in this two way table correspond to the samples of interest (ie the explanatory variable) and the rows the response variable. Thus, the denominator of our sample proportions will be the column sums and the numerators will be the entries in the cancer row. n1 &lt;- sum(sumTab[,1]) n2 &lt;- sum(sumTab[,2]) pHat1 &lt;- sumTab[1,1] / n1 pHat2 &lt;- sumTab[1,2] / n2 c(pHat1, pHat2) ## [1] 0.3858586 0.3188098 When we calculate a confidence interval for two sample proportions, were estimating the difference between the two proportions, so our point estimate is pDiff &lt;- pHat1 - pHat2 pDiff ## [1] 0.06704881 This means that roughly 6.7% more dogs exposed to the herbicide developed cancer in the sample data. Before we calculate a confidence interval, we need to make sure our samples meet the conditions needed to apply to Central Limit theorem. Independence within samples and between samples. The study employed random sampling for both groups and each sample size is less than 10% of its corresponding population. Moreover, the samples were taken independently of one another. Were thus good to go here. Sample size requirements. From the two-way table sumTab above we see that we have more than 10 successes (cancer) and failures (no cancer) in each group of dogs. Good to go here too! The CLT applies, so the sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\) is approximately normally distributed. Moreover, the standard error is \\[ SE \\approx \\sqrt{\\frac{\\hat{p}_1(1-\\hat{p}_1)}{n_1}+ \\frac{\\hat{p}_2(1-\\hat{p}_2)}{n_2}} \\] which we can now easily calculate! SE &lt;- sqrt(pHat1*(1- pHat1)/n1 + pHat2*(1- pHat2)/n2) SE ## [1] 0.02663677 This means that if we repeated the study over and over we should expect about changes of about 2.7% between the various differences in sample proportions. Were finally ready to calculate our confidence interval! Since the sampling distribution is approximately normal, our the critical value for our 95% confidence interval is zCrit &lt;- qnorm( (1-.95)/2, lower.tail = F) and our confidence interval is ciDiff &lt;- pDiff + c(-1, 1)*zCrit*SE ciDiff ## [1] 0.01484171 0.11925591 How should we interpret this confidence interval? Remember, were estimating the difference in cancer rates between dogs who were exposed to the herbicide and those who were not. Thus: With 95% confidence, the cancer rate of dogs who are exposed to 2,4-Dichlorophenoxyacetic acid is between 1.5% and 11.9% higher than dogs who were not exposed to the herbicide. The next section will go through hypothesis testing using the same data. 6.2.2.2 Hypothesis testing Out in the wild you may want to simply check for a difference between two population proportions. The statistical inference for this is, of course, a hypothesis test. When youre investigating a question like this your null and research hypotheses are \\[ \\begin{array}{cl} H_0: &amp; p_1 - p_2 = 0 \\\\ H_a: &amp; p_1 - p_2 \\neq 0. \\end{array} \\] The alternative hypothesis can by one-sided if youd like; if it were \\(p_1 - p_2 &gt; 0\\) youd be asking if the first proportion is greater than the second and the opposite if it were \\(p_1 - p_2 &lt; 0\\). Youd could use a one sided hypothesis test to answer a question like are there proportionally more Republicans in Montana than in Wyoming? As usual the CLT is the main tool for performing a 2-sample proportion hypothesis test. In this case we need to make sure that our observations are independent within and between groups each sample has at least 10 expected success and failures. Theres a small difficulty we need to deal with here. Recall that we always assume the null hypothesis is true when performing a hypothesis test. In this case that only means \\(p_1 = p_2\\), but it doesnt make any claim about what these proportions are actually equal to. The remedy is to estimate this value using a pooled proportion. Suppose your first and second samples have \\(n_1\\) and \\(n_2\\) observations respectively and \\(suc_1\\) and \\(suc_2\\) successes. The pooled sample proportion is \\[ \\begin{array}{ccc} \\hat{p}_{pool} &amp; = &amp; \\frac{suc_1 + suc_2}{n_1 + n_2} \\\\ &amp; = &amp; \\frac{ \\hat{p}_1 n_1 + \\hat{p}_2 n_2}{n_1 + n_2} \\end{array} \\] or, in words, the pooled proportion is the total number of successes in both samples divided by the total number of observations. We use \\(\\hat{p}_{pool}\\) to assess the sample size requirements and in the formula for the standard error if our samples meet the conditions to apply the CLT. Lets see how this plays out using the herbicide and cancer in dogs example from the last section. A two-way table summarizing the results of the experiment is below. sumTab %&gt;% kable() 2,4-D no 2,4-D cancer 191 300 no cancer 304 641 The pooled proportion is the sum of the first row of this table divided by the sum of all entries in this table. pPool &lt;- sum(sumTab[1,])/sum(sumTab) pPool ## [1] 0.341922 In other words, if we ignored herbicide exposure about 34% of the dogs in our sample developed cancer. We use this proportion to ensure that our samples are both large enough to apply the central limit theorem. c( n1*pPool, n1*(1-pPool)) ## [1] 169.2514 325.7486 c( n2*pPool, n2*(1-pPool)) ## [1] 321.7486 619.2514 Thus both of our samples have at least 10 expected success and expected failures, so we can apply the CLT! Remember were assuming that the null hypothesis is true and in turn that \\[p_1 = p_2 \\approx \\hat{p}_{pool}\\]. Thus \\[ \\begin{array}{ccc} SE &amp; = &amp; \\sqrt{\\frac{p_1(1-p_1)}{n_1}+ \\frac{p_2(1-p_2)}{n_2}} \\\\ &amp; \\approx &amp; \\sqrt{\\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_1} + \\frac{\\hat{p}_{pool}(1-\\hat{p}_{pool})}{n_2}} \\end{array} \\] The calculation for our current working example is below. SE &lt;- sqrt( pPool*(1-pPool)/n1 + pPool*(1-pPool)/n2) SE ## [1] 0.02633795 Were finally ready to calculate the \\(p\\)-value for this experiment. Lets perform this hypothesis test with a significance level of \\(\\alpha = .05\\). Our point estimate of \\(p_1 - p2\\) is pDiff below. pHat1 &lt;- sumTab[1,1] / n1 pHat2 &lt;- sumTab[1,2] / n2 pDiff &lt;- pHat1 - pHat2 pDiff ## [1] 0.06704881 The test statistic, which in this case is a \\(Z\\)-score, of our data is \\[ \\begin{array}{ccc} Z &amp; = &amp; \\frac{\\text{point est.} - \\text{null value}}{SE} \\\\ &amp; = &amp; \\frac{ \\hat{p}_1 - \\hat{p}_2}{SE} \\end{array} \\] or as follows in R. Z &lt;- pDiff / SE Z ## [1] 2.545711 Since \\(Z &gt;0\\) the \\(p\\)-value of our hypothesis test is pVal &lt;- 2*pnorm(Z,lower.tail = FALSE) pVal ## [1] 0.01090555 In other words, if there were no difference between cancer rates in dogs exposed to the herbicide and those who were not there would be a 1.1% chance of observing data like our own or more extreme. Since \\(p &lt; \\alpha = .05\\), we reject the null and accept the research: The data provide compelling evidence to believe that there the cancer rate of dogs exposed to the herbicide is different from the rate of dogs who were not exposed. Note that this may have been a good candidate for a one-sided hypothesis test since were not simply interested in detecting a difference. We may be more interested in seeing if dogs exposed to the herbicide developed cancer at greater rates than those who were not exposed. In this situation, it is common to perform a two-sided hypothesis test, but also report a confidence interval to indicate which side of the null value the true difference falls on if your tests results in a rejection of the null. 6.2.3 Chi-squared tests We can get a lot of mileage making inferences about categorical variables using single and two sample proportion tests and confidence intervals, but they dont do everything we need. A single proportion may not always capture the subtleties were interested in. For example, if you want to the test whether or not the outcomes of a die toss really are equally likely, it isnt enough to check if a 1 occurs 1/6 of the time using a single sample proportion test because say 4s could be less likely and 6s more likely. Similarly, when were testing for an association between two categorical variables a 2-sample proportion test may miss some differences or subtleties were interested in. For instance, if youre testing for an association between smoking and socio-economic status, it is possible that lower and upper class individuals smoke at the same rate, but that middle class people smoke at a different rate. We thus need to develop a tool that can capture these differences that our previous techniques miss. This is where chi-squared (or \\(\\chi^2\\)) tests come in. These tests are different from our the previous ones because you analyze counts instead of proportions. Counts are summarized in either one-way or two-way tables; one-way tables display the counts of the values a single categorical variable in a sample while two-way tables display the counts of the values of two categorical variables. The data set sp500_seq from the openintro package is a summary of the all S&amp;P500 trading day outcomes from 1950 to 2018. To get to the data set in question, first label each day as a down day or an up day, then count the number of days between two up days. The outcomes in this data set are 1, 2, 3, , 7+ and we treat these values as an ordered categorical variables because of the 7+. Below we see a one-way table for the counts of lag times between up days. We will analyze this data set in the next section, but we will use it to test to see if the direction of the S&amp;P500 on one day is independent from its direction on the previous day. table(sp500_seq$race) ## ## 1 2 3 4 5 6 7+ ## 1532 760 338 194 74 33 17 For an example of a two-way table, well use the smoking data set from the openintro package. This data set presents some survey data on smoking habits in the UK. You might wonder if theres an association between marital status and smoking in the UK. This data set could provide some insight on this question and you could attempt to answer it by analyzing the following two-way table. table(smoking$smoke, smoking$marital_status) ## ## Divorced Married Separated Single Widowed ## No 103 669 46 269 183 ## Yes 58 143 22 158 40 The basic idea behind all chi-squared tests is that under the null hypothesis each cell of your tabular data has an expected count; the goal is then is to measure the deviation between the observed counts and the expected counts. Moreover, we want a single test statistic that summarizes this. You might start by simply looking at the difference between the observed \\(O\\) and expected \\(E\\) counts, then adding these differences up. Here we run into a dilemma that weve encountered before: positive and negative differences will possibly cancel each other out and make it look like there are no differences when in fact the differences are extreme. Instead, we square the difference between the observed and expected counts: \\((O - E)^2\\). We still have a small dilemma. Suppose you have two cells with observed counts 11 and 101. The expected counts for these cells are 10 and 100 respectively; \\((O- E)^2 = 1\\) for both of these cells, but the first observed count is 10% different from the expected while the second is only 1% different from the expected. We thus need to measure the squared deviation from the expected counts relative to the expected counts in each cell. For each cell in our summary table, we calculate \\[ \\frac{(O-E)^2}{E}\\] This value ends up being a squared \\(Z\\)-score! The single test statistic we calculate from our sample is the sum of these squared \\(Z\\)-scores and is called a \\(\\chi^2\\)-test statistic (or chi-squared test statistic). \\[ \\chi^2 = \\sum_{all \\, cells \\\\ in \\, table } \\frac{(O-E)^2}{E}\\] Since the test statistics are sums of squared \\(Z\\)-scores, the corresponding sampling distribution is a sum of squared standard normal distributions. The number of squared normal distributions is a parameter called degrees of freedom, \\(df\\). Thus you completely determine a chi-squared distribution with \\(df\\). Once youve calculated your \\(\\chi^2\\)-test statistic and have determined \\(df\\), you can calculate a \\(p\\)-value and execute your hypothesis test. For any chi-squared test, more favorable for the alternative hypothesis will always mean roughly more deviation from the expected counts, so \\(p\\)-values are always upper tail areas. For example, suppose youre performing some hypothesis test and find \\[ \\begin{array}{ccl} df &amp; = &amp; 5 \\text{ and } \\\\ \\chi^2 &amp; = &amp; 6.2. \\end{array} \\] The image below shows a plot of the chi-squared distribtuion with \\(df = 5\\) and shades the \\(p\\)-value of our imaginary hypothesis test. ggplot(tibble(x = c(0, 18)), aes(x = x)) + geom_function(fun = dchisq, args = list(df = 5)) + stat_function(fun = dchisq, args = list(df = 5), geom = &quot;area&quot;, xlim = c(6.2, 18), fill = &quot;steelblue&quot;) + labs( x = &quot;chi-squared&quot;, y = &quot;&quot;, title = &quot;Chi-squared distribution with df = 5&quot;, subtitle = &quot;p-value for chi-squared = 6.2 shaded&quot;) From this we see that we can calculate \\(p\\)-values for chi-squared tests using the function pchisq(test_stat, df = df, lower.tail = FALSE). For example, the \\(p\\)-value for our imaginary hypothesis test would be pchisq(6.2, df = 5, lower.tail=FALSE) ## [1] 0.2872417 For more information about Rs functions for chi-squared distributions, see Chapter 4s section on these distributions. This section summarized the basics of all chi-squared tests from a big picture. From this summary note that we essentially only need to figure out two things. How does one determine the expected counts for each cell of the table? What are the appropriate numbers of degrees of freedom for the hypothesis test? The answers to these questions depends on the type of chi-squared test youre performing. The next sections provide these answers for the two types of chi-squared tests. 6.2.3.1 Chi-squared goodness of fit These hypothesis tests are used to see if your sample data deviates from its expected distribution. For example, we expect the outcomes of a single die to be equally likely and could use a chi-squared goodness of fit test to check this. The null and alternative hypothesis for a goodness of fit test are \\[ \\begin{array}{cl} H_0 &amp; \\text{the data follow the expected distribution} H_a &amp; \\text{the data do not follow the expected distribution} \\end{array} \\] and rejecting the hypothesis implies that the population distribution differs from the expected distribution.No surprise, the expected distribution determines the expected counts for each cell in your table. For example, if you toss a single die 120 times and record the outcome, youd expect to get \\(120/6 = 20\\) of each number 1-6. To perform these hypothesis tests we need to know the number of degrees of freedom. You can think of this as the minimum number of cell counts in your table you need to completely determine the remaining cells. For example, if you toss a die 120 times and know the counts of the outcomes 1 through 5, you can figure out how many 6s you got (120 minus the sum of 1-5 counts). But if you removed any of the other counts, you wouldnt have enough information to complete the table. Thus you have 5 degrees of freedom in the die toss example. For one-way tables and goodness of fit tests \\[ df = k -1\\] where \\(k\\) is the number of cells in your table. Determining the expected counts can be slightly more involved than the die toss example. Lets return to the S&amp;P500 example mentioned in the previous section. We want to test to see if the direction of the stock market on one day is independent from the direction of the stock market on the previous day. If we assume that the directions are independent, then the probability of the market going up on any day is the same. This might be tough to test on its own, but notice that we can count the number of days between two consecutive up days. For instance, up up would give a count of one, up down up a count of two, etc. The table below tallies all such runs from 1990 to 2011. spTable &lt;- table(sp500_seq$race) spTable ## ## 1 2 3 4 5 6 7+ ## 1532 760 338 194 74 33 17 If the daily directions are independent, what would the table above look like? The number of days between consecutive up days should follow a geometric distribution! To determine a geometric distribution, we need to know the probability of a success, which is the probability that the S&amp;P500 goes up on a given day. We can estimate this probability using the following code chunk looks at the raw S&amp;P500 data from 1990 to 2011. sp500_1950_2018 %&gt;% filter( as.character(Date) &gt;= &#39;1990&#39; &amp; as.character(Date) &lt;= &#39;2012&#39;) %&gt;% mutate(dir = if_else( Close &gt; Open, TRUE, FALSE)) %&gt;% summarize( upProb = sum(dir)/length(dir)) ## # A tibble: 1 x 1 ## upProb ## &lt;dbl&gt; ## 1 0.531 So the probability that the S&amp;P500 goes up on a given day is rough 53.1%. Using this, we can generate a table of expected counts. # total number of up-to-up runs n &lt;-sum(spTable) #list of factors days &lt;- c(1:6, &quot;7+&quot;) #probability/proportion of each factor level probs &lt;- c( dgeom(0:5,.531), pgeom(5, .531, lower.tail = F)) # expected counts expected &lt;- n*probs expected ## [1] 1565.38800 734.16697 344.32431 161.48810 75.73792 35.52108 31.37361 # display all of the above in a horizontally organized table t( tibble(days, prob = round(probs, 3), expected = round(expected, 3))) %&gt;% kable() days 1 2 3 4 5 6 7+ prob 0.531 0.249 0.117 0.055 0.026 0.012 0.011 expected 1565.388 734.167 344.324 161.488 75.738 35.521 31.374 Were finally ready to calculate our test statistic and execute a hypothesis test. Lets use a significance level of \\(\\alpha = .05\\). Were specifically testing \\[ \\begin{array}{cl} H_0: &amp; \\text{the number of days between consecutive &quot;up&quot; days} \\\\ &amp; \\text{on the S&amp;P500 follows a geometric distribution} \\\\ H_a: &amp; \\text{the count does not follow a geometric distribution.} \\end{array} \\] Note that our table has 7 cells in it, so \\[ df = 7 - 1 = 6. \\] We will calculate our test statistic in a few steps. Recall that it is \\[ \\chi^2 = \\sum_{all \\\\cells} \\frac{(O-E)^2}{E} \\] # each individual squared z-score (spTable - expected)^2/expected ## ## 1 2 3 4 5 6 7+ ## 0.71212923 0.90898305 0.11616053 6.54551974 0.03987916 0.17893220 6.58517561 # test stat chi.sq &lt;- sum((spTable - expected)^2/expected) chi.sq ## [1] 15.08678 Now that we have our test statistic in hand we can calculate a \\(p\\)-value. pchisq(chi.sq, df = 6, lower.tail = FALSE) ## [1] 0.0195924 Since our \\(p\\)-value is less than .05, we reject the null and accept the alternative. In other words, the counts of runs of different lengths do not appear to follow a geometric distribution. This implies that the direction of the S&amp;P500 on one day depends on the direction of the previous day. This seems like it could be useful information! But how could one use it? This is a harder question and for the most part beyond the scope of this course. For fun, lets look at the z-scores of each cell instead of the squared z-scores. (spTable - expected)/sqrt(expected) ## ## 1 2 3 4 5 6 7+ ## -0.8438775 0.9534060 -0.3408233 2.5584213 -0.1996977 -0.4230038 -2.5661597 The only two run lengths that seem particularly unusual are runs of length 4 and 7 or more. In particular there are more runs of length 4 than wed expect and fewer runs of length seven or more. Does that mean you should bet on the market going up if there are three down days in a row? Not without more analysis! At a bare minimum youd want to calculate a conditional probability: whats the probability the market goes up after three down days? Note: This is, of course, far from anything related to actual financial advice! This example is merely to demonstrate one of the myriad of things you can investigate with goodness of fit tests. 6.2.3.2 Chi-squared test of independence These tests are used to check for an association between two categorical variables. Because of this you will likely see chi-squared tests of independence all over the place. For example, the results of clinical trials in medicine are often analyzed using chi-squared tests of independence since you have two groups of individuals (those in the treatment group and those in the placebo group) and there are often two or more outcomes (symptoms improve, symptoms dont improve, etc.). Recall from above that one applies these tests to two-way tables. Well work through the example testing for an association between marital status and smoking in the United Kingdom (the data comes from the smoking data set from the openintro package). The observed counts from a survey are displayed below. observed &lt;- table(smoking$smoke, smoking$marital_status) observed ## ## Divorced Married Separated Single Widowed ## No 103 669 46 269 183 ## Yes 58 143 22 158 40 The null and research hypotheses for a test of independence are \\[ \\begin{array}{cl} H_0: &amp; \\text{the variables are independent} \\\\ H_a: &amp; \\text{the variables are dependent} \\end{array} \\] For our example, the null hypothesis is that smoking and marital status are not associated; the research hypothesis is that there is an association between smoking and marital status. As with goodness of fit tests, we need to determine the expected counts for our two-way table assuming the null hypothesis is true. Recall from probability that if two events \\(A\\) and \\(B\\) are independent, then \\(P(A \\&amp; B) = P(A)P(B)\\) (caution! This is only true when the events are independent). Suppose event \\(A\\) corresponds to row \\(i\\) in your two way table and event \\(B\\) to column \\(j\\). Let \\(n\\) denote the sum of all entries in your table; \\(n\\) is sometimes called the grand total. Then \\[ P(A) = \\frac{\\text{row i total }}{n} \\quad \\text{and} \\quad P(B) = \\frac{\\text{column j total }}{n} \\] so if \\(A\\) and \\(B\\) are independent \\[ P( A \\&amp; B ) = \\frac{\\text{row i total }}{n} \\frac{\\text{column j total }}{n} = \\frac{\\text{row i total}\\cdot \\text{column j total }}{n^2} \\] Thus the expected count in the \\((i,j)\\)-cell of our two-way table is \\[ n \\cdot P( A \\&amp; B ) = \\frac{\\text{row i total}\\cdot \\text{column j total }}{n} \\] To make this more concrete, lets find the expected count for the \\((1,1)\\)-cell in our observed two-way table above. This cell corresponds to divorced people who do not smoke. The code chunk below calculates the corresponding row and column totals along with the grand total. n &lt;- sum(observed) row1 &lt;- sum(observed[1,]) column1 &lt;- sum(observed[,1]) c(n, row1, column1) ## [1] 1691 1270 161 So if the smoking and marital status are independent, wed expect to see exp11 &lt;- row1*column1/n exp11 ## [1] 120.9166 divorced non-smokers in our survey results. The squared \\(z\\)-score for this cell would then be (observed[1,1]- exp11)^2/exp11 ## [1] 2.654765 Now lets make a table of expected counts. In practice, youd use one of Rs in-built functions to perform this test, but here were demonstrating whats going on beneath the hood. We use two nested for loops to make the table of expected counts. # define an matrix with the right dimensions of all zeroes expected &lt;- matrix( rep(0, prod(dim(observed))), nrow = dim(observed)[1], ncol = dim(observed)[2]) # iterate over all rows for(i in 1:dim(observed)[1]){ #iterage over all columns for(j in 1:dim(observed)[2]){ #update i,j-th entry with expected count expected[i,j] &lt;- sum(observed[i,])*sum(observed[,j])/sum(observed) } } expected ## [,1] [,2] [,3] [,4] [,5] ## [1,] 120.91662 609.8403 51.07037 320.6919 167.48078 ## [2,] 40.08338 202.1597 16.92963 106.3081 55.51922 Is it a problem that none of the entries in the table above are integers? No! We should think of the expected counts as average outcomes. We can now easily calculate our table of squared \\(z\\)-scores and our chi-squared test statistic. After that, to perform out hypothesis test, we simply need to determine the appropriate number of degrees of freedom \\(df\\). (observed - expected)^2/expected ## ## Divorced Married Separated Single Widowed ## No 2.6547648 5.7389881 0.5033971 8.3321480 1.4380526 ## Yes 8.0084354 17.3123870 1.5185614 25.1349832 4.3380684 The chi-squared test statistic is the sum of the entries in the table above. chi &lt;- sum((observed - expected)^2/expected) chi ## [1] 74.97979 To calculate a \\(p\\)-value and perform this hypothesis test, we simply need to determine \\(df\\). As with goodness of fit tests, \\(df\\) is the minimum number of cell counts in your table you need to completely determine the remaining cells. If \\(R\\) and \\(C\\) are the number of rows and columns in your two-way table, then \\[ df = (R-1)\\times (C-1) \\] so for our example \\[ df = (2-1)\\times (5 -1 ) = 4. \\] Thus the \\(p\\)-value for our hypothesis test is pchisq(chi, df = 4, lower.tail = F) ## [1] 2.012302e-15 This \\(p\\)-value is smaller than all typical significance levels, so we reject the null and accept the research. In particular, the data provide compelling evidence to believe that there is an association between marital status and smoking. What might this association look like? Lets look at the \\(z\\)-scores of our table counts instead of the squared \\(z\\)-scores. (observed - expected)/sqrt(expected) ## ## Divorced Married Separated Single Widowed ## No -1.6293449 2.3956185 -0.7095048 -2.8865460 1.1991883 ## Yes 2.8299179 -4.1608157 1.2322992 5.0134801 -2.0828030 It appears that married people are way less likely to smoke and single people are way more likely to smoke than expected. As in the previous example for goodness of fit, we could do a more in-depth analysis to describe the association we detected. 6.3 Analyzing numerical variables Weve finished our discussion of analyzing categorical variables and now turn our attention to analyzing numerical variables. There are many different numerical variables one can analyze but we will focus on making inferences about population averages. We use the Greek letter mu \\(\\mu\\) to denote a population average and \\(\\overline{x}\\) to denote a sample average. Similarly, we use the Greek letter \\(\\sigma\\) to denote the population standard deviation and \\(s\\) to denote sample standard deviations. We will use sample averages to make inferences about population averages, but in the process we will have to estimate \\(\\sigma\\) using \\(s\\). This creates a small dilemma that well define and address below. One distinction that becomes more important and sometimes difficult to make when analyzing numerical variables is the difference between population distributions and sampling distributions. This was an easy distinction to make when we were dealing with categorical variables, because the population distributions were discrete things (proportion of success and proportion of failure), but sampling distributions were continuous. When dealing with numerical variables, however, the population and sampling distributions are often both continuous. As an example, suppose you are trying to investigate the average income of residents in Flathead County. You take a sample of 100 adult county residents. You sample from a population and then use the sampling distribution of \\(\\overline{x}\\) to make an inference about the county average population. The plot below helps us visualize the difference between these two distributions. The distributions below are made up and serve merely to illustrate the differences between the two distributions. The plots above demonstrate that population distributions can be skewed (or in practice have a totally unknown shape), but under fairly mild hypotheses, the sampling distribution of \\(\\overline{x}\\) will be symmetric and unimodal. This is the central limit theorem in action again. As with categorical variables, this will be the main driver of inference about numerical variables for us. Central Limit Theorem (for a single sample average): Suppose we have \\(n\\) independent observations of a numerical variable. If either the population were sampling from is approximately normal or \\(n\\) is sufficiently large or then the sampling distribution of \\(\\overline{x}\\) is approximately normal with \\[ E(\\overline{x}) = \\mu \\quad \\text{and} \\quad SE( \\overline{x}) = \\frac{\\sigma}{\\sqrt{n}}\\] No surprise, the standard error of the sampling distribution of \\(\\overline{x}\\) depends on the standard deviation of the population. Unfortunately, we almost never know \\(\\sigma\\) and almost never know if the population were sampling from is approximately normally distributed. For the latter dilemma we have a few rules of thumb to guide us. Normality conditions: \\(n &lt;30\\) and there are no clear outliers in your data, it is common to assume your sample comes from an approximately normal population. Note that this is simply a rule of thumb! To check for outliers, use \\(z\\)-scores and look for the maximum in absolute value. \\(n \\geq 30\\) and there are no extreme outliers in your data. If either of these conditions holds and your observations are independent, then the we assume our sample meets the criteria in order to apply the CLT as described above. There is, however, one more small obstacle: we almost never know \\(\\sigma\\), the population standard deviation, which determines the standard error of \\(\\overline{x}\\). We do what appears to be the most logical thing, and estimate \\(\\sigma\\) with \\(s\\), our sample standard deviation so that \\[ SE = \\frac{\\sigma}{\\sqrt{m}} \\approx \\frac{s}{\\sqrt{n}}\\] but (because of technical reasons that well skip) it turns our that \\(s\\) is a biased estimator of \\(\\sigma\\). Moreover, it consistently underestimates \\(sigma\\). Thus, our estimate of \\(SE\\) above is also an underestimate, which means that we should expect more variability among our sample statistics. This bias is especially pronounced when your sample size is small, and tends to zero as the sample size increases. The key to remedying this problem, at least for our purposes, is to model the sampling distribution of \\(\\overline{x}\\) using \\(t\\)-distribution. Recall from Chapter 4 that a \\(t\\)-distribution is a symmetric and unimodal distribution determined by a single parameter, \\(df\\), or degrees of freedom. Each \\(t\\)-distribution has fatter tails than the standard normal distribution, meaning that a higher proportion of observations will fall more than 2 standard deviations from the mean. As \\(df\\) gets larger, \\(t\\)-distributions approach the standard normal distribution, so the fat tails get skinnier. Notice that this helps solve the dilemma of using \\(s\\) to estimate \\(\\sigma\\)! In the next sections well see how to use \\(t\\)-distributions to make inferences about population averages. Everything in the following two sections should feel very similar from a big picture perspective because the overall strategy for statistical inference remains the same. 6.3.1 Single sample mean We begin our exploration of numerical variables by learning how to make inferences about a single population average. We will see how to calculate and interpret confidence intervals, then proceed to hypothesis testing. To illustrate the techniques, we will use two examples in what follows. One example will be based on the faithful data set that is loaded by default in R. It gives the duration of 272 eruptions at Old Faithful along with the wait time between eruptions. We will also look the women data set which gives the height and weight of 15 American women from 1975. For both examples we will need to verify that we can apply the Central Limit Theorem, so we need to to check the normality conditions listed above. faithful: Well investigate the wait time between eruptions, the waiting variable in the faithful data set. Since the number of observations \\(n = 272\\), we simply need to check that the data set has no extreme outliers. We can do this in two ways, visually and numerically. First, well look at a histogram of the data. ggplot(faithful, aes(x = waiting)) + geom_histogram(color = &quot;black&quot; , fill = &quot;steelblue&quot; , bins = 20) + labs( title = &quot;Wait time between Yellowstone eruptions&quot; , subtitle = &quot;n = 272&quot;) The wait time between eruptions appears to be bimodal, but this is no problem for us. From the histogram above, no observations seem particularly extreme. Lets double check this with some calculations. We can look at a summary of the wait times: summary(faithful$waiting) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 43.0 58.0 76.0 70.9 82.0 96.0 or translate this summary into \\(z\\)-scores (summary(faithful$waiting) - mean(faithful$waiting))/sd(faithful$waiting) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.0520 -0.9487 0.3754 0.0000 0.8167 1.8465 These calculations confirm our observation: since no measurement is more than roughly 2 standard deviations away from the mean, our sample contains no outliers. We also need to check independence of observations here. Does the wait time from one eruption to another influence the wait time until the next eruption? Quite possibly, but without more information it is hard to say. To keep things simple, we point out that independence is difficult to verify in this case, but we assume the observations are independent. women: Well look at the height variable. In this case the number of observations is small as \\(n = 15\\). But not all is lost! If we can safely assume the population is normally distributed, we can apply the central limit theorem. As above, we first look at a histogram ggplot(women, aes(x = height)) + geom_histogram(color = &quot;black&quot; , fill = &quot;steelblue&quot; , bins = 7) + labs( title = &quot;Height of American women in 1975&quot; , subtitle = &quot;n = 15&quot;) No surprise, the distribution of heights in this sample is fairly uniform and there are no clear outliers in the data. Well check this using a summary and \\(z\\)-scores again. summary(women$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 58.0 61.5 65.0 65.0 68.5 72.0 and \\(z\\)-scores: (summary(women$height) - mean(women$height))/sd(women$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.5652 -0.7826 0.0000 0.0000 0.7826 1.5652 Since all observations are less than 2 standard deviations from the mean, there are no outliers whatsoever in this data set. In this case it is safe to assume that the population is normally distributed, which means that we can apply the central limit theorem to this example as well! We will use the CLT to estimate the population averages and also make inferences about them. Before we can proceed, we need to establish one more fact. Recall from above that we will model the sampling distribution of \\(\\overline{x}\\) with a \\(t\\)-distribution. Unlike normal distributions that are determined by a mean \\(\\mu\\) and a standard deviation \\(\\sigma\\), a \\(t\\)-distribution is determined by a single parameter, degrees of freedom \\(df\\). If your sample has \\(n\\) measurements, then the sampling distribution is best modeled by a \\(t\\)-distribution with \\[ df = n - 1\\] 6.3.1.1 Confidence intervals Confidence intervals for single sample means are calculated using the typical pattern: \\[ \\text{point est.} \\pm \\text{critical value}\\cdot SE \\] where the point estimate is the sample average \\(\\overline{x}\\) and the standard error \\(SE\\) is given by the CLT \\(SE = \\sigma / \\sqrt{n} \\approx s / \\sqrt{n}\\). If your sample meets the criteria for applying the central limit theorem, the only new datum we need to calculate is the critical value. Since the sampling distribution of \\(\\overline{x}\\) is best modeled with a \\(t\\)-distribution with \\(df = n-1\\), we should use this distribution to find the critical value. You can calculate these \\(t\\)-critical values using the R function qt in the exact same way you found \\(z\\)-critical values in the previous section! In particular, if calculating an \\(\\alpha\\)% confidence interval, the critical value is \\(t^\\star_{df} =\\) qt( (1-alpha)/2, df = n-1, lower.tail = FALSE ) where alpha is the confidence level as a decimal rather than a percentage. Lets now calculate and interpret 95% confidence intervals for the examples discussed in the section above, women and faithful. faithful: We want to calculate a 95% confidence interval for the average wait time between eruptions at the Old Faithful geyser in Yellowstone (in minutes). To do so, we need four numbers: \\(\\overline{x}\\) the sample average, \\(n\\) the sample size, \\(s\\) the sample standard deviation, and \\(t^\\star_df\\) the \\(t\\)-critical value. The code chunk below calculates all of these numbers along with the standard error, then prints the \\(t\\)-critical value. n &lt;- length(faithful$waiting) xBar &lt;- mean(faithful$waiting) s &lt;- sd(faithful$waiting) SE &lt;- s / sqrt(n) tStar &lt;- qt( (1 - .95)/2, df = n-1, lower.tail = FALSE) tStar ## [1] 1.968756 Notice that \\(t^\\star_df\\) is very close to the \\(z\\)-critical value for a 95% confidence interval. This is because 272 is large, so the associated \\(t\\)-distribution is very close to the standard normal curve. Now we can combine the above into a confidence interval. cI &lt;- xBar + c(-1,1)*tStar*SE cI ## [1] 69.27418 72.51994 Thus, with 95% confidence, the average wait time between two eruptions at Old Faithful is between roughly 69.3 and 72.5 minutes. women: We want to calculate a 95% confidence interval for the average height of American women in 1975. We need the same numbers as above to make this calculation and the code chunk below does exactly that. n &lt;- length(women$height) xBar &lt;- mean(women$height) s &lt;- sd(women$height) SE &lt;- s / sqrt(n) tStar &lt;- qt( (1 - .95)/2, df = n-1, lower.tail = FALSE) tStar ## [1] 2.144787 This time around, notice that \\(t^\\star_df \\approx 2.144\\) which is larger than the \\(z\\)-critical value for a 95% confidence interval. This is because the \\(t\\)-distribution with \\(14\\) degrees of freedom has fatter tails than the standard normal distribution. This is exactly what we want because we are more uncertain about our use of \\(s\\), the sample standard deviation, as an estimate of \\(\\sigma\\), the population standard deviation. Now, calculating the confidence interval: cI &lt;- xBar + c(-1,1)*tStar*SE cI ## [1] 62.52341 67.47659 Our calculations imply that the average height of American women in 1975 was between about 62.52 and 67.48 inches, with 95% confidence. 6.3.1.2 Hypothesis testing Now that we have confidence intervals under our belt and understand the details of using \\(t\\)-distributions for making inferences about means, we can move through hypothesis testing fairly quickly. When making inferences about an average, the null and research hypotheses are statements about the value of the true population mean. \\[ \\begin{array}{cc} H_0: &amp; \\mu = \\mu_0 \\\\ H_a: &amp; \\mu \\neq \\mu_0 \\text{ or } \\mu &gt; \\mu_0 \\text{ or } \\mu &lt; \\mu_0 \\end{array} \\] Remember that one assumes the null hypothesis is true when performing a hypothesis test. Note: unlike analyzing proportions, the standard error depends only on your sample and not the null mean! This is because \\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{s}{\\sqrt{n}} \\] whenever we can apply the Central Limit Theorem. To execute this hypothesis test, we perform the usual steps. Set a significance level \\(\\alpha\\) Calculate all appropriate sample statistics. In this case the sample average \\(\\overline{x}\\) and standard deviation \\(s\\). Calculate the standard error \\(SE\\). Calculate your test statistic. In this case, it is called a \\(T\\)-statistic since the sampling distribution is best modeled with a \\(t\\)-distribution with \\(df = n-1\\). These statistics are the same as \\(z\\)-scores. \\[ T = \\frac{\\overline{x} - \\mu_0}{SE} \\] Note that this is where were using the null hypothesis! The \\(T\\)-statistic measures how far our sample average is from the null average in terms of standard error. Calculate the \\(p\\)-value of your test statistic. This depends on the type of test youre performing. In this case, \\(p\\)-value calculations are exactly like they are for single sample proportions, except you use the function pt in R instead of pnorm. Compare your \\(p\\)-value to the significance level \\(\\alpha\\) and make a conclusion. Well use the two data sets above to go through two different hypothesis test examples. women: As usual, we start with a question: Were American women taller in 1975 than they were in 2022? This question is actually a bit more complicated than it seems at first pass since the category adult spans a wide range of ages and average height varies with time. Our data set women looks at the average height of women aged between 30 and 39 in 1975, so really we should ask Were American women aged 30-39 taller in 1975 than they were in 2022?. Well use a significance level of \\(\\alpha = .1\\). Pulling data from Our World in Data, the average height of an woman aged between 30 and 39 in 2022 was roughly 64.4 inches. If \\(\\mu\\) denotes the average height of women in 1975, then we set up our hypothesis test as \\[\\begin{array}{cc} H_0: &amp; \\mu = 64.4 \\\\ H_1 &amp; \\mu &gt; 64.4 \\end{array}\\] The our sample mean, standard deviation, sample size, and standard error are calculated below. n &lt;- length(women$height) xBar &lt;- mean(women$height) s &lt;- sd(women$height) SE &lt;- s / sqrt(n) #printing the calculated values c(n = n, xBar = xBar, s = s, SE = SE) ## n xBar s SE ## 15.000000 65.000000 4.472136 1.154701 We now use these values to calculate the \\(T\\) statistic. Note: we label this test statistic t because T is R shorthand for TRUE. t &lt;- (xBar - 64.4)/SE t ## [1] 0.5196152 so our sample lies about half a standard deviation above the null average. We calculate the \\(p\\)-value for this test using the function pt with \\(df = n -1 = 15 -1 = 14\\). Recall that we are performing a one-sided upper hypothesis test. pVal &lt;- pt(t, df = 15 - 1, lower.tail = F) pVal ## [1] 0.3057246 This means that if women aged between 30-39 were the same height in 1975 as they were in 2022 there would be about a 30% chance of observing a sample with an average height of 65 inches or more. Of course, since \\(p \\approx .306 \\geq .1\\) we fail to reject the null hypothesis. faithful: Now imagine youre a program director at Yellowstone National Park and in charge of the Old Faithful eruption presentations. You currently have presentations every 75 minutes, but for some reason, your scheduling seems off. You decide to to check to see if the average wait time between eruptions is different from 75 minutes using the data we have available in faithful and we will perform out hypothesis test using a significance level of \\(\\alpha = .01\\). Re-framing your question as a hypothesis test yields \\[\\begin{array}{cc} H_0: &amp; \\mu = 75 \\\\ H_1 &amp; \\mu \\neq 75 \\end{array}\\] We derive the necessary numbers for this hypothesis test in the code chunk below. n &lt;- length(faithful$waiting) xBar &lt;- mean(faithful$waiting) s &lt;- sd(faithful$waiting) SE &lt;- s / sqrt(n) #printing the calculated values c(n = n, xBar = xBar, s = s, SE = SE) ## n xBar s SE ## 272.0000000 70.8970588 13.5949738 0.8243164 We use these values to calculate the \\(T\\)-test statistic. t &lt;- (xBar - 75)/SE t ## [1] -4.977387 This shows our sample average is almost 5 standard deviations below the mean in the null sampling distribution. With this in mind, we expect to reject the null and accept the alternative hypothesis, but well calculate the \\(p\\)-value for our sample just to be sure. Since were performing a two-sided hypothesis test and \\(T &lt; 0\\), the \\(p\\)-value is pVal &lt;- 2*pt(t, df = n-1) In other words, if the average wait time between eruptions were actually 75 minutes, theres about a 0.00011% a sample of 272 eruptions with an average wait time as different from 75 minutes as our own. Since \\(p &lt; \\alpha\\), we reject the null and accept the research: the data lead us to conclude that the average wait time between eruptions at Old Faithful is different from 75 minutes. Now, its up to you as the imaginary program manager to figure out how to use this information. 6.3.2 Paired data 6.3.2.1 Confidence intervals 6.3.2.2 Hypothesis testing 6.3.3 Two sample mean 6.3.3.1 Confidence intervals 6.3.3.2 Hypothesis testing 6.3.4 Analysis of variance (ANOVA) "],["inferential-statistics-take-2.html", "Chapter 7 Inferential statistics, take 2 7.1 Analyzing categorical variables 7.2 Analyzing numerical variables", " Chapter 7 Inferential statistics, take 2 This chapter will show you how to calculate confidence intervals and perform hypothesis tests using Rs in-built hypothesis testing functions. We will quickly go over the same examples from the previous sections using Rs functions to recover the values we calculated earlier. 7.1 Analyzing categorical variables 7.1.1 Single sample proportions The main function we will use in this section is prop.test(...); it both calculates confidence intervals and performs hypothesis tests all in one fell swoop! The important thing for us, then, is being able to read and interpret the outputs. 7.1.2 Two sample proportions The main function we will use in this section is again prop.test(...); its a workhorse! 7.1.3 Chi-squared goodness of fit test 7.1.4 Chi-squared test of independence 7.2 Analyzing numerical variables 7.2.1 Single sample mean 7.2.2 Two sample mean 7.2.3 Paired data 7.2.4 Analysis of variance (ANOVA) "],["modeling-relationships.html", "Chapter 8 Modeling relationships 8.1 Simple linear regression 8.2 Multiple linear regression 8.3 Bonus topic: logistic regression", " Chapter 8 Modeling relationships To do 8.1 Simple linear regression 8.2 Multiple linear regression 8.3 Bonus topic: logistic regression "],["references.html", "References", " References "]]
