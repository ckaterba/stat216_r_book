[["index.html", "R and RStudio for STAT216 Chapter 1 About 1.1 Organization 1.2 Getting started 1.3 Code in this book 1.4 Help 1.5 Debugging 1.6 Other resources", " R and RStudio for STAT216 Charles Katerba Last update: 2022-11-04 Chapter 1 About This ‘book’ is intended to be a quick and dirty introduction to R (a statistical programming language) and RStudio (an integrated development environment, or IDE, for R) for students in the introductory statistics class at Flathead Valley Community College. This book aims to introduce you to all of the features you’ll need to use in R to be successful in the course. Moreover, we hope that it will help you develop the tools you need to use R in your future endeavors. This book is by no means a complete guide to everything one can do in R, but the aim is to cut down on content and theory in the name of practicality. This class will provide many students with their first interaction with computer programming. The coding required is not extensive and the coding-based activities are heavily scaffolded. The point of including R in this course is not to introduce another layer of difficulty; the opposite is true in fact! The goal is to give you a glimpse at how programming can make your life easier, at least when it comes to statistics and data science. In the process, you will hopefully add another useful and practical tool to your tool belt. Computers are not exiting our lives any time soon and being somewhat familiar with a little programming will only be an advantage. Some might say such familiarity will become essential. 1.1 Organization The chapters of this book will be organized to follow the flow of STAT216 so that reading the book sequentially teaches you the tools as you need them in class. This book will almost certainly be a work in progress, so if there’s something that you’d like to see or if you think something is missing, don’t hesitate to reach out to the author (or your instructor if they are different people). 1.2 Getting started The programming language R is somewhat old, appearing first in 1993. That said, it is still quite useful and will remain so for a long time coming since it is an open-source programming language and software environment. This means that the development and maintenance of the language and software are in the hands of all users, not just a small, select group of developers. Fear not! You will not have to do any developing. For our purposes, the open-source nature of R implies that the community of users will continue to grow the features and power of the language to meet the needs of the times. There are two mains ways to use R: through a cloud computing service or through an installation directly to your computer. The easiest way to start is through a cloud computing service. In both cases, we recommend you use the IDE RStudio as it makes using R more intuitive and straightforward. Below we describe how to use RStudio on the cloud and on your own computer. RStudio (which will rebrand as Posit in October 2022) hosts its own cloud computing service called RStudio Cloud. To get started: Click the previous link to access their website. Create a new account, for free. Your free account gives 25 project hours per month. This should be enough for your work in STAT216, but there’s a chance you’ll need more. There is a small fee in this case. Click “Start a new project” in the top right corner of your browser. This will open an RStudio session for you. Change the name of your project to something evocative to tell your future self what you were working on. Some ideas: STAT216 HW, STAT216 Activities, etc. You are now ready to rip! You can also download a desktop version of RStudio. Accessing R in this way is slightly more involved, but offers more flexibility and no computational limitations. It is also free. To install R to your own computer: First, you must install R. This link takes you to CRAN, the Comprehensive R Archive Network, where you can download the latest version of R. Be sure to select the download appropriate to your operating system and follow the instructions provided at the link above. Next, download and install RStudio Desktop following the directions at the link. Again, be sure to select the download appropriate to your operating sytem. You can now open RStudio by finding the application on your computer. You, too, are now ready to rip! No matter what version of RStudio you are using, you should now see a window that looks something like this on your computer. An RStudio terminal. Before proceeding, notice that your RStudio session has 3 windows open. The console: this is the most interactive window in your session. You can run quick calculations here. For example, type 2 + 2 then hit enter. Files/plots window: The files tab shows all of the files present in your current working directory. We don’t need to talk much about file management here and will cover it as needed in the future. When you make quick plots, they will appear under the plot tab. For example, evaluating plot(rnorm(10), rnorm(10)) in the console will produce a plot similar to the one above in the Plots panel. This little bit of code generates 2 collections of 10 random numbers and makes a scatter plot of the results. Environment window: This window displays the names of the objects you have stored in your computer’s memory. As an example, if you evaluate x &lt;- rnorm(10), you will see the value x appear in the Environment tab. It is good to check the values you have stored when you’re running into problems with your code. At this point, you can skim the rest of this chapter or move on to RStudio basics. 1.3 Code in this book You should be able to copy and paste all code chunks in this book into your own RStudio terminal and reproduce the results you see in the book. All data used in this book will be sourced from base R, other commonly used packages, or appropriate links will be given for downloaded data sets. The author will endeavor to be clear about what packages are loaded and where data sets are coming from throughout the book. 1.4 Help There are many avenues for finding help using R. Your instructor, the STEM tutoring center, and the internet are great places to start. For the latter, you have to carefully phrase what you’re asking, but if you do so, you can usually find great results with some clear examples. R also has a lot of built in help documentation. You can access this documentation using a ? before the name of the function/object that you’re intereted in. For example, R has a function called prop.test(...) that we’ll learn how to use in this class. To see R’s documententation for prop.test(...), simply run ?prop.test in the console. The examples are usually helpful, but sometimes can create more confusion. In any case, this is a great place to start when you feel confused about a particular function or object. If you bring up the documentation for an in-built data set, it will describe what the variables of that data set are. For instance, try running ?mtcars in the console 1.5 Debugging In this class, you will very likely come to a point where some code you’ve written will not run. This experience can be very frustrating. Remember, it happens to everyone and it frustrates everyone. Hadley Wickham provides a a detailed guide on how to debug code in this book Advanced R, but here is my take with a few additions. Double and triple check your syntax. New coders are often stymied by small syntax mistakes. For instance, if you want to use a function called f in R, but you type F instead, the machine won’t do the right thing. Similarly, be careful about commas; for example, elements of lists need to be separated by commas and they can be easy to forget. There are a too many syntax errors one can make to list here. Search the internet for the error codes you’re seeing. You can simply copy and paste them into your web browser. Most likely someone before you has made the same mistake and asked the same question. Figure out exactly where the error is. This is harder than it seems like it should be sometimes. In short pieces of code it is usually easy to identify, but when you become more advanced and write longer code the problem can get tricky. Reach out to your instructor and/or the STEM tutoring center at FVCC. These people are invaluable resources. Be sure to not only share the error code, but the entire chunk of code that you think is problematic. Even better, share all of your code. The website CrossValidated is a stack exchange where users can ask and answers questions about statistics, especially R coding. You may be able to get questions answered here, but be warned: the community is very picky about how you ask questions and about the type of questions you ask. 1.6 Other resources As mentioned above, these notes are only going to give you a brief glimpse of R and RStudio. The resources linked below will either provide a much more in-depth look at R and its applications or provide a very quick summary of certain packages and/or features. RStudio cheatsheets. These are invaluable as they provide a lot of information in a small amount of space. A list of helpful R functions. Thomas Scofield made a super consice list of commonly used base R functions and provided examples on how to use them. R for Data Science. As the title suggests, this book focuses more on data science than on statistics, so it spends more time on data visualization, programming, data wrangling, and modeling. Advanced R. This book goes into the more nitty gritty details of programming in R. In writing this book the author has referenced many of the works above along with a collection of other resources. See the bibliography for a somewhat complete list. "],["rstudio-basics.html", "Chapter 2 RStudio basics 2.1 A big calculator 2.2 Naming things 2.3 Lists and vectors 2.4 Packages 2.5 Data frames and tibbles 2.6 R Markdown documents", " Chapter 2 RStudio basics At this point you should have an operational RStudio terminal at your disposal. If not, go back to Getting started. This chapter will focus some of the most basic tools and skills you’ll need to successfully use RStudio. We’ll start with using R as a calculator, learn about naming and storing values/objects, then progress to loading and using data frames (the bread and butter of R in some sense), and finish with R Markdown documents, which allow you to create fully reproducible documents that contain a mixture of text and code. Fun fact: these notes were written in a collection of RMarkdown documents and compiled with a package called bookdown. As you proceed, we encourage you to try to run any code you see in this book and to play around and do things on your own. The best way to learn how to code is to experiment. The immediate feedback of running some successful code can be quite gratifying! 2.1 A big calculator At its most basic, R is a big, fancy, and initially cumbersome calculator. Think of any calculation you’d like to do and R can likely do it. R can handle more than just numbers! For our purposes, we’ll need numbers, strings, and logicals. 2.1.1 Numbers R has three “atomic” types of numbers: real, integer, and complex, but we will only need to think of real numbers. You can do arithmetic and evaluate the standard elementary functions with numbers as you would expect. Addition, multiplication, subtraction, and division are +, *, -, and / respectively. For example: 2 + 2 ## [1] 4 3 * 1.5 ## [1] 4.5 9 - 4 ## [1] 5 5/3 ## [1] 1.666667 Exponentiation is either ^ or **, the latter is old school. You can raise a number to a power of 10 using scientific notation, for example 1.2e4 is 12000. 2^3 ## [1] 8 2**3 ## [1] 8 1.2e4 ## [1] 12000 All of your favorite “elementary” functions are built into R. Note that the log function is base \\(e\\), not base 10. The example below shows how to change base. exp(2) # e^2 ## [1] 7.389056 sin(pi/2) # note the trig functions are in radians ## [1] 1 log(10) # natural log of 10 ## [1] 2.302585 log(100, 10) # the log of 100, base 10 ## [1] 2 Reminder! Be careful and mindful about the order of operations; computers and calculators read your input very literally. For instance, \\(6 \\div 2(1 + 2)\\), is an annoying statement with respect to the order of operations. R will evaluate this expression correctly if you type 6/2*(1+2). Make sure you can evaluate it correctly as well so that you input what you mean. 2.1.2 Strings We want to be able to analyze both numerical and categorical variables in R. For this reason, along with many others, your can use and manipulate strings of character in R. We won’t be doing much more than using strings for values of categorical variables, so we won’t go into too much detail. To define a string, simply put the expression in question in quotation marks. &quot;R might be cool? I don&#39;t know.&quot; Base R has some handy functions for manipulating strings. We may not need all of them, but they’re good to be aware of. substr(string, start=n1, stop=n2) will return a subset of your string starting at the n1th character, ending at the n2 character. substr(&quot;R might be cool? I don&#39;t know.&quot;, start = 18, stop = 30) ## [1] &quot;I don&#39;t know.&quot; nchar(string) counts the number of characters in a string. nchar(&quot;R might be cool? I don&#39;t know.&quot;) ## [1] 30 toupper/tolower(string) converts all letters to either upper or lower case. toupper(&quot;R might be cool? I don&#39;t know.&quot;) ## [1] &quot;R MIGHT BE COOL? I DON&#39;T KNOW.&quot; paste(..., sep = \" \") will concatenate a collection of strings, separated by a space. You can change what separates the strings. This function is most useful when you have to repeatedly concatenate some strings. paste(&quot;R might be cool? I don&#39;t know.&quot;, &quot;Yes, it is!&quot;) ## [1] &quot;R might be cool? I don&#39;t know. Yes, it is!&quot; You can do a lot more with strings, but things start to get more complicated quickly. The package stringr has some really handy functions, but are unnecessary for this course. 2.1.3 Logicals and Logical operators Throughout this class you will need to compare various objects in R using standard “logical operators” like “equals” (==), “less than” &lt;, “greater than or equal to &gt;=” etc. When you compare objects using these operators, R returns a new type of object called a “logical”. Logicals are just TRUE and FALSE. You can check equality of numbers and strings using ==. To check if two objects are unequal, use !=. 4 == 5 ## [1] FALSE 2^3 == 8 ## [1] TRUE &quot;cat&quot; == &quot;dog&quot; ## [1] FALSE &quot;dog&quot; != &quot;cat&quot; ## [1] TRUE You can compare numbers with your favorite inequalities (&lt;, &gt;, &lt;=, &gt;=) as well. More interestingly, you can also compare strings with inequalities. R defaults to lexicographic (ie dictionary) ordering of strings. 10 &lt; 9 ## [1] FALSE exp(3) &gt;= exp(2) ## [1] TRUE &quot;a&quot; &lt; &quot;b&quot; ## [1] TRUE &quot;aa&quot; &gt; &quot;ab&quot; ## [1] FALSE For convenience, TRUE sometimes behaves like 1 and FALSE like 0. In particular, you can perform arithmetic with logicals. This makes counting the number of matches you have quite easy in some situations, as we’ll see. TRUE + TRUE ## [1] 2 TRUE*FALSE ## [1] 0 Note: R may occasionally exhibit behavior that you find goofy when comparing objects. This is usually because of coercion. When comparing objects, R will try to make the objects into the same type, if possible. We don’t want to get bogged down in these details, but it is something to be aware of. The following example illustrates coercion when trying to compare a string to a number. &quot;1&quot; == 1 #R converts the string to 1, as a number, then compares. ## [1] TRUE &quot;01&quot; == 1 # R leaves the string as is, so the objects are unequal. ## [1] FALSE Finally, you can combine logical into compound expressions using the operators AND (in R AND is &amp;) and OR (in R |). The expression P &amp; Q is TRUE if and only if P and Q are both true. The expression P | Q yields TRUEif and only if at least one of P or Q is true. Let’s look at some examples in code. (5 &gt; 2) &amp; (4 == 2^2) # both statements are true, so compound is as well ## [1] TRUE (5 &lt; 2) &amp; (4 == 2^2) # first statement false, so compound is false ## [1] FALSE (5 &lt; 2) | (4 == 2^2) # second statement is true, so compound is true ## [1] TRUE (5 &lt; 2) | (4 != 2^2) # both false, so compound is false ## [1] FALSE This might seem odd right now, but this simple feature is very helpful when identifying subsets of a dataset with certain properties as well see later on in these notes. 2.2 Naming things Now that we know what types of objects we can use in R, we can talk about naming objects and storing them in your computers memory so that you can reference them easily later. You can name objects in two ways, either with the classic &lt;- or with =. These two methods of naming objects technically do different things, but the difference is subtle enough that we won’t worry about it. Note that when you name an object, it will appear in your “Environment” window in RStudio. The example below will illustrates the basics of naming objects. x &lt;- 1.123e6 y &lt;- 5 x ## [1] 1123000 y ## [1] 5 x/y ## [1] 224600 Once you’ve named an object and stored it in memory, you can use its name anywhere else to call the value associated to the name. Read the following only if you’ve had some programming experience. If not, skip to the next section. R makes copies of objects in a different way from, say, Python (and many other languages).The difference lies in deep vs shallow copies. Basically, a deep copy creates an entirely new object in memory, but a shallow copy merely points to the original object’s position in memory. R will actually create a shallow copy until the original object is modified, then it will create a deep copy in order to save on memory. To illustrate this point, if you were to run the following code in Python: x = [1,2,3] y = x #shallow copy created here x.append(5) y it would print [1,2,3,5]. However, the equivalent code in R is x = 1:3 y = x # shallow copy of x created here x &lt;- append(x, 5) #but deep copy of original x created here for y y ## [1] 1 2 3 It’s good to know how R is behaving; deep copies can end up using more memory, but that won’t be an issue in this class with modern computing power. 2.3 Lists and vectors One of the many advantages to using a computer for mathematics, statistics, and data analysis is their ability to store and organize large chunks of information. A list/vector is the most basic way to start organizing data in R. This section will guide you through creating and manipulating lists. 2.3.1 Creating lists The most basic tool for creating a list in R is the function c(..). According to the documentation c(...) “is a generic function which combines its arguments.” All that to say that it is a function that creates a list. Here’s a few examples. You can make lists with any types of data in them. x &lt;- c(1,2,3) y &lt;- c(&quot;cat&quot;, &quot;dog&quot;, &quot;rabbit&quot;) You can combine lists easily with the c(...) function as well, but notice in the example below that something funny happens. z &lt;- c(x, y) z ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;cat&quot; &quot;dog&quot; &quot;rabbit&quot; R coerced the numbers from the list x into characters/strings because y contains strings. This is a subtlety that can sometimes cause headaches. If your lists all contain the same types of objects, you have nothing to worry about. Throughout this course you’ll often want to know how many objects are in a list and most times we don’t want to manually count them. The length function does this for us. The examples above are easy to count by hand, but as a quick check: length(x) ## [1] 3 length(y) ## [1] 3 length(z) ## [1] 6 2.3.2 Subsetting lists After defining a list, sometimes we want to access only certain elements of that list; this is called subsetting. We’ll spend more time on fancier subsetting methods later in this course, but this will get us started with the basics. Technical note: R indexing starts at 1 and not 0, a difference between R and some other programming languages like python. To get started, we’ll create a long list of randomly generated numbers. rando &lt;- runif(1000, min = 0, max = 100) length(rando) ## [1] 1000 This list has 1000 elements in it. We can find the 497th element using rando[497] ## [1] 29.89864 and in general, use listName[number]. We can also easily select ranges of indicies. For instance, suppose we want to know what the first 10 values in our list are. rando[1:10] ## [1] 33.48089 19.48977 64.84354 34.90027 88.32681 48.65610 81.34832 10.88966 81.42149 92.13669 Finally, we can pick out specific elements from our big list using a smaller list of our desired indices. For example, if we wanted the entries number 2,3,5,7,11, and 13 from our list, we could use the following. indicies &lt;- c(2,3,5,7,11,13) rando[indicies] ## [1] 19.48977 64.84354 88.32681 81.34832 46.93847 51.08745 2.3.3 Special lists The last section taught us how to make the most basic type of list. While useful, making lists in this way can be tedious because it requires one to type in all values. Often out in the wild, you will want to create lists that follow specific patterns. For instance: the letters “c” through “p”; a list of the integers from 1 to 100; a list of numbers from -10 to 10, spaced by .01; the letters “d”, “g”, and “z” repeated 1000 times each. It is easy to construct these types of lists in R with minimal typing. Lists of integers: to make a list of sequential integers in R, use the format start : end, for instance 2:5 will return the list 2,3,4,5. x &lt;- -8:12 x ## [1] -8 -7 -6 -5 -4 -3 -2 -1 0 1 2 3 4 5 6 7 8 9 10 11 12 length(x) ## [1] 21 Lists of sequential numbers that are not integers: The function seq produces sequences of numbers that follow a particular pattern. The general syntax is seq( from = STARTING VALUE, to = ENDING VALUE, by = SPACE BETWEEN ENTRIES) So if we wanted a list of all numbers from -10 to 10, spaced by .01, we’d use foo &lt;- seq(from = -10, to = 10, by = .01) We can access the first 5 elements of this list and count the number of elements in our list foo with the following. foo[1:5] ## [1] -10.00 -9.99 -9.98 -9.97 -9.96 length(foo) ## [1] 2001 Lists of sequential letters: R has two in-built lists of letters called letters and LETTERS. No surprise, one of them is a list of lower case letters, the other upper case. letters ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; &quot;l&quot; &quot;m&quot; &quot;n&quot; &quot;o&quot; &quot;p&quot; &quot;q&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;u&quot; &quot;v&quot; ## [23] &quot;w&quot; &quot;x&quot; &quot;y&quot; &quot;z&quot; To select the 4th through 11th lowercase letters of the alphabet, use the following syntax (which will be explained in a subsequent section). letters[4:11] ## [1] &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot; &quot;h&quot; &quot;i&quot; &quot;j&quot; &quot;k&quot; Lists with a repeating patterns: If you’re performing an experiment or reformatting a data set, you may want to create a list with a repeated pattern. For example, suppose you have an experiment with 1000 people in the treatment group and 1000 people in the control group. The group a research subject is in is a variable, so should be a column in a data set. If the outcome of the experiment will be recorded in the order “treatment, control, treatment, control, etc.” for all 2000 participants, we would like to easily make a list of length 2000 following this pattern. The function rep( x, times = n) will do exactly this for us! In this case x is a list and times specifies the number of times you’d like that list to be repeated. As an example, to make the list described above, we would use the commands vals &lt;- c( &quot;treatment&quot;, &quot;control&quot;) group &lt;- rep( vals, times = 1000 ) To ensure this does what we want, let’s look at the length of group and at the first 10 elements. length(group) ## [1] 2000 group[1:10] ## [1] &quot;treatment&quot; &quot;control&quot; &quot;treatment&quot; &quot;control&quot; &quot;treatment&quot; &quot;control&quot; &quot;treatment&quot; ## [8] &quot;control&quot; &quot;treatment&quot; &quot;control&quot; Success! 2.3.4 List arithmetic Lists are great for storing information, but the real power comes from being able to manipulate them. You can weak lists in a few ways: using summary functions, which we’ll cover in the next chapter, and using usual arithmetic. Let’s start by defining 3 lists. x &lt;- 1:3 y &lt;- c(9, 18, 27) z &lt;- 10:15 #note length(z) = 6 We’ll start by adding, subtracting, multiplying, dividing lists, and exponentiate lists. We can also scale all values in a list by a constant and raise all elements to a certain power (note that R performs all of these operations component wise, so it may not be what you’d expect if you’ve had some experience with matrices). x + y ## [1] 10 20 30 x - y ## [1] -8 -16 -24 x*y ## [1] 9 36 81 y/x ## [1] 9 9 9 y^x ## [1] 9 324 19683 .5*x ## [1] 0.5 1.0 1.5 x^2 ## [1] 1 4 9 Many functions in R are vectorized, meaning that if you input a list, the function will be applied to every element of the list. For example: sin(x) ## [1] 0.8414710 0.9092974 0.1411200 log(y, 3) #log base 3 ## [1] 2.00000 2.63093 3.00000 We need to be careful when doing arithmetic with lists. If we’re not careful, seemingly mysterious behavior can crop up if the lists we’re working with have different length. For instance, adding the lists x and z above doesn’t seem to make sense since they have different lengths. But running the following returns a list with length 6. x + z ## [1] 11 13 15 14 16 18 So what’s going on? R will repeat the values from the shorter list until the length matches the longer one. In the example above, we’re really getting the same result as rep(x, 2) + z ## [1] 11 13 15 14 16 18 This feature can be annoying and confusing if you’re not aware of it, but helpful and powerful if you are. The next silly example will hopefully illustrate this point. Suppose we want to generate a list of random numbers such that the odd numbers are between -1 and 0, but the even numbers are between 1 and 2. Here’s our strategy: first, generate 10 random numbers between 0 and 1; next, add one to all the even indexed entries so that their possible range of values goes from 0 to 1 to 1 to 2; finally, multiply the odd entries by -1. In code, this looks like rando &lt;- runif(10, min = 0, max = 1) rando &lt;- c(0,1) + rando rando &lt;- c(-1,1)*rando rando ## [1] -0.94035980 1.32945010 -0.13041086 1.61328209 -0.39324088 1.51488573 -0.06710591 ## [8] 1.28432709 -0.14584588 1.69613835 A more practical application of this vectorization feature is to use logical operators and use them to help subset lists. As a basic example, R will compare lists component wise or to a particular value. x &lt; y ## [1] TRUE TRUE TRUE x &lt; 2 ## [1] TRUE FALSE FALSE Remember that we can combine logicals using &amp; and | (AND and OR). This gives us a way to check more complicated conditions on values. The list rando from above should have numbers between -1 and 0 in the odd entries and between 1 and 2 in the even. Let’s check that. #subsetting only odd indices (rando &lt;= 0 &amp; rando &gt;= -1)[seq(from = 1, to = 10, by = 2)] ## [1] TRUE TRUE TRUE TRUE TRUE #subsetting only even indices (rando &gt;=1 &amp; rando &lt;= 2)[seq(from = 2, to = 10, by = 2)] ## [1] TRUE TRUE TRUE TRUE TRUE So the even and odd index entries of rando have the properties we wanted them to! Yahoo. Hopefully you are starting to see how these simple functions and features can be combined and layered into much more sophisticated gadgets. 2.4 Packages This short section changes gears. Recall that R and RStudio are both open-source projects, so much of the development lies in the hands of the community of users. There are many features and functions built into R, but you’ll occasionally run into things that are not innate to it. When this happens, you have two options: you can either program the feature that you want on your own (which is hard), or try to figure out if someone else has already programmed the feature you’re interested in. Very often you’ll find that they have and typically these features will be available in an R package. Packages are simply extensions of the base R programming language; they typically contain code, data, and documentation. In this class, we’ll use only a handful of packages. Some of them are: tidyverse: actually a collection of packages that utilize the same underlying syntax, grammar, etc. In principle, the tidyverse makes data science more intuitive, cleaner, and faster than base R. openintro: a package that contains many of the data sets in our textbook, OpenIntro Statistics. infer: a package that implements the tidyverse syntax and grammar for statistical inference. BDSA: a package that has a handful of data sets and a few functions that will be useful. This package is associated with the book “Basic Statistics and Data Analysis” by Larry J. Kitchens. There are thousands of R packages available to help you accomplish almost any task you can imagine. Some fun examples: spotifyr: a package to streamline accessing Spotify’s API (application programming interface), so you can pull data on song, artist, and playlist information. There are similar packages for many of your favorite websites and applications. caret: a consolidated machine learning library that makes creating, analyzing, and utilizing machine learning algorithms a breeze. leaflet: a package for creating interactive maps in R. You have very likely interacted with a map created in leaflet! This link will show you some code to create a heatmap from Strava data using leaflet. bookdown: a package for authoring books in RStudio. The book you are reading was written using bookdown! You probably get the point: once you start using packages, the capabilities of R and RStudio are limitless. But how do you actually use them? First, you have to install the package, then load it into your current working environment. Installing packages: to install a package, use the function install.packages(...) in the console of RStudio. Be sure to put the name of the package in quotation marks. For example, to install the tidyverse package, execute the following code in the console: install.packages(&quot;tidyverse&quot;) Once you’ve install a package, you have to load and attach the package to your current working environment. Use the library(...) function to do this. After installing the package, you no longer need to call it with quotation marks. Thus, to attach the tidyverse package, execute the following in your Rstudio console: library(tidyverse) Caution 1: You have to attach the packages you’d like to use every time you start a new RStudio session! Alternatively, you have to load them at the beginning of your R script or R Markdown document. More on this in a coming section. Caution 2: In RStudio Cloud, packages need to be installed in every project you create. This means if you start a new project for every activity, you must re-install (and of course load) all of the packages required for the project. To avoid this difficulty, I recommend starting one project for activities and creating a new folder in this project for each individual directory. 2.5 Data frames and tibbles Data frames are the bread and butter of R and statistics in general. At their most basic, they are simply arrays of objects be they numbers, strings, or logicals with named columns (and sometimes named rows). If you’re recording data, remember: Each column of a data frame represents a variable. Each row of a data frame represents an observation, so you record a value of each variable for each observation. There are two primary types of data frames in R, data.frames and tibbles. The former is an object from base R and the latter is a data frame class from the tidyverse package. There are differences between these two classes of data frames, but we won’t worry about them too much. In this class, because they display in a nicer way and some operations are performed more quickly with them, we will primarily use tibbles. For more information on tibble vs data.frame, evaluate ?tibble in the console after loading the tidyverse package. 2.5.1 Creating data frames 2.5.1.1 From packages Many R packages contain test data sets that can be stored in your computer’s memory and used for exploration and demonstration of particular ideas. To use these data sets, you can either make a copy of them using the &lt;- operator or load them using the data function. As an example, tidyverse has a dataset labeled us_rent_income. You can load use this data set by calling df &lt;- us_rent_income #or data(&quot;us_rent_income&quot;) #note the quotation marks! These built-in data sets are great for examples, but they don’t help you analyze your own data. Next, we’ll learn how to input our own data into R. 2.5.1.2 Reading files The most common way to load your own data into R is by reading the data in from a file (eg csv, excel, pdf, etc files) stored on your computer. These other file formats are much easier and more flexible for entering data. Loading and analyzing the data in R also helps prevent the user from accidentally changing values in their data during their analysis; changing values in data sets in R is very difficult to do accidentally by design). We can’t and won’t try to cover all of the possible ways you can load a data set into R from a file on your computer, but we will cover some of the most common techniques. In general, you can figure out how to load a file of a particular type into R with a simple internet search. Below we list some functions and their use. In each case, the first input is file, which is possibly a the path of the file you’re trying to load. It’s easiest to put the file in the same directory/folder that you’re working in. read.csv(file, ...): this function reads a CSV (comma separated value) file and loads the data into R. Note that sometimes you have to tweak the arguments of this function to load the data set in exactly the way you want. read_excel(file, ...): from the readxl package does what you’d expect, reading and loading Microsoft Excel files. As above, you might have to play around with this to load the data appropriately. pdf_text(file, ...): from the pdftools reads all of the text from each page of a PDF document. This is handy, but requires quite a bit of tidying and is not something we’ll cover in this course. The basic take away is that you can read data from PDF files into R. download.file(url, destfile, ...): this function downloads a file from from the specified url and stores it as destfile. The following example illustrates how you might use a combination of these functions in practice. The code downloads a data set from Montana State University’s STAT216 GitHub page, loads it into R, then displays the results. The data set lists a handful of countries, the country’s max speed limit, and the average number of highway fatalities annually. #first we store the url and file name as strings url &lt;- &quot;https://raw.githubusercontent.com/MTstateIntroStats/IntroStatActivities/master/data/Highway.csv&quot; fileName &lt;- &quot;Highway.csv&quot; #the next command downloads the file if you haven&#39;t downloaded it already if(!file.exists(fileName)){ download.file(url, fileName) } #now load the file into R df &lt;- read.csv(fileName) #print data frame df ## limit death country ## 1 55 3.0 Norway ## 2 55 3.3 United States ## 3 55 3.4 Finland ## 4 70 3.5 Britain ## 5 55 4.1 Denmark ## 6 60 4.3 Canada ## 7 55 4.7 Japan ## 8 60 4.9 Australia ## 9 60 5.1 Netherlands ## 10 75 6.1 Italy 2.5.1.3 Manual entry When you are simulating data, working on homework, or simply have a small data set, it can be convenient to manually type your data into R. To make a data frame, you simply list the columns as named vectors. We will go through three examples of of how you can do this. First off, suppose you wanted to enter the following table as a data frame into R. animal weight length cat 5 24 dog 45 35 rabbit 2 10 turtle 15 15 To enter this table, we would use the following: df &lt;- tibble( animal = c(&quot;cat&quot;, &quot;dog&quot;, &quot;rabbit&quot;, &quot;turtle&quot;), weight = c(5, 45, 2, 15), length = c(24, 35, 10, 15)) df ## # A tibble: 4 × 3 ## animal weight length ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cat 5 24 ## 2 dog 45 35 ## 3 rabbit 2 10 ## 4 turtle 15 15 Now suppose we want to make a data set for the points on the line \\(y = 2x + 3 + \\text{some random noise}\\) with integer \\(x\\) values between 1 and 10. We can do this easily! df &lt;- tibble(x = 1:10, y = 2*x + 3 + rnorm(10)) df ## # A tibble: 10 × 2 ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 1 6.15 ## 2 2 6.73 ## 3 3 9.21 ## 4 4 10.0 ## 5 5 13.5 ## 6 6 14.6 ## 7 7 16.4 ## 8 8 20.1 ## 9 9 22.6 ## 10 10 21.6 As a final example, let’s start with a table that doesn’t follow the convention of variables in columns, observations in rows. For this example, suppose you recorded the weight in pounds of 5 turtles, cats, and dogs as follows: Turtle Cat Dog 15 2 26 10 13 60 15 12 60 12 8 22 12 2 59 In this case, notice that we only actually have two variables around, not three: species of pet and pet weight. When dealing with categorical variables, it is common to see data organized as above, but be aware that this isn’t the most effective way to organize your information for the purpose of analysis. Since the data set has only two variables, our data frame in R should only have two columns. We could enter this data as follows: df &lt;- tibble( species = c( rep(&quot;turtle&quot;, 5), rep(&quot;cat&quot;, 5), rep(&quot;dog&quot;, 5)), weight = c(15, 10, 15, 12, 12,2, 13, 12, 8 , 2,26, 60, 60, 22, 59)) df ## # A tibble: 15 × 2 ## species weight ## &lt;chr&gt; &lt;dbl&gt; ## 1 turtle 15 ## 2 turtle 10 ## 3 turtle 15 ## 4 turtle 12 ## 5 turtle 12 ## 6 cat 2 ## 7 cat 13 ## 8 cat 12 ## 9 cat 8 ## 10 cat 2 ## 11 dog 26 ## 12 dog 60 ## 13 dog 60 ## 14 dog 22 ## 15 dog 59 This format might not be as easy to see, but it will be substantially easier to analyze in R. 2.5.2 Using data frames You can think of the rest of this book as a quick summary on ways to use data frames, but this section will touch on a few basics we need before we can do anything else. In this section, we’ll use the starwars data set from the tidyverse package. First off, once you’ve stored a data set, it is good to look at it to see what you’re working with. There are a few different options that all have their places. view(df): from the tidyverse package and opens a spreadsheet-like view of your dataset. You cannot edit the data, only see it. head(df): returns the first 6 rows of a data set and may exclude some columns from view. tail(df): returns the last 6 rows of a data set and may exclude some of the columns from view. glimpse(df): from the tidyverse package and prints a few entries from every column of your data set. Let’s see how all of these (except view) behave on the starwars data set. Try view(starwars) out on your own. head(starwars) ## # A tibble: 6 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Luke… 172 77 blond fair blue 19 male mascu… Tatooine Human ## 2 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooine Droid ## 3 R2-D2 96 32 &lt;NA&gt; white, bl… red 33 none mascu… Naboo Droid ## 4 Dart… 202 136 none white yellow 41.9 male mascu… Tatooine Human ## 5 Leia… 150 49 brown light brown 19 fema… femin… Alderaan Human ## 6 Owen… 178 120 brown, gr… light blue 52 male mascu… Tatooine Human ## # … with 3 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; tail(starwars) ## # A tibble: 6 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Finn NA NA black dark dark NA male mascu… &lt;NA&gt; Human ## 2 Rey NA NA brown light hazel NA fema… femin… &lt;NA&gt; Human ## 3 Poe … NA NA brown light brown NA male mascu… &lt;NA&gt; Human ## 4 BB8 NA NA none none black NA none mascu… &lt;NA&gt; Droid ## 5 Capt… NA NA unknown unknown unknown NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 Padm… 165 45 brown light brown 46 fema… femin… Naboo Human ## # … with 3 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; glimpse(starwars) ## Rows: 87 ## Columns: 14 ## $ name &lt;chr&gt; &quot;Luke Skywalker&quot;, &quot;C-3PO&quot;, &quot;R2-D2&quot;, &quot;Darth Vader&quot;, &quot;Leia Organa&quot;, &quot;Owen L… ## $ height &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 228, 180, 173, … ## $ mass &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.0, 84.0, NA, 1… ## $ hair_color &lt;chr&gt; &quot;blond&quot;, NA, NA, &quot;none&quot;, &quot;brown&quot;, &quot;brown, grey&quot;, &quot;brown&quot;, NA, &quot;black&quot;, &quot;a… ## $ skin_color &lt;chr&gt; &quot;fair&quot;, &quot;gold&quot;, &quot;white, blue&quot;, &quot;white&quot;, &quot;light&quot;, &quot;light&quot;, &quot;light&quot;, &quot;white… ## $ eye_color &lt;chr&gt; &quot;blue&quot;, &quot;yellow&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;brown&quot;, &quot;blue&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;brown… ## $ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, 41.9, 64.0, 20… ## $ sex &lt;chr&gt; &quot;male&quot;, &quot;none&quot;, &quot;none&quot;, &quot;male&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;none&quot;, &quot;male… ## $ gender &lt;chr&gt; &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, &quot;masculine&quot;, &quot;feminine&quot;, &quot;masculin… ## $ homeworld &lt;chr&gt; &quot;Tatooine&quot;, &quot;Tatooine&quot;, &quot;Naboo&quot;, &quot;Tatooine&quot;, &quot;Alderaan&quot;, &quot;Tatooine&quot;, &quot;Tat… ## $ species &lt;chr&gt; &quot;Human&quot;, &quot;Droid&quot;, &quot;Droid&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Human&quot;, &quot;Droid&quot;, &quot;… ## $ films &lt;list&gt; &lt;&quot;The Empire Strikes Back&quot;, &quot;Revenge of the Sith&quot;, &quot;Return of the Jedi&quot;,… ## $ vehicles &lt;list&gt; &lt;&quot;Snowspeeder&quot;, &quot;Imperial Speeder Bike&quot;&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &quot;Imperial Speeder … ## $ starships &lt;list&gt; &lt;&quot;X-wing&quot;, &quot;Imperial shuttle&quot;&gt;, &lt;&gt;, &lt;&gt;, &quot;TIE Advanced x1&quot;, &lt;&gt;, &lt;&gt;, &lt;&gt;, &lt;… IMPORTANT! Now that we’ve looked at our data, we might want to access certain columns in that data frame; this is what the $ operator does in R. Explicitly, df$columnName yields the column with name columnName as a list, as in the example below. We’re using the head function because the list of characters is long. length(starwars$name) ## [1] 87 head( starwars$name ) ## [1] &quot;Luke Skywalker&quot; &quot;C-3PO&quot; &quot;R2-D2&quot; &quot;Darth Vader&quot; &quot;Leia Organa&quot; ## [6] &quot;Owen Lars&quot; Just as with lists, you can pick out particular elements or ranges of elements in a data frame using bracket notation. In general, the syntax is df[rowRange , columnRange] where rowRange and columnRange can be lists or single numbers. Using starwars as an example: starwars[2, 5] #second row, 5th column ## # A tibble: 1 × 1 ## skin_color ## &lt;chr&gt; ## 1 gold starwars[2,] #all columns and only second row ## # A tibble: 1 × 14 ## name height mass hair_color skin_color eye_color birth_year sex gender homeworld species ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 C-3PO 167 75 &lt;NA&gt; gold yellow 112 none mascu… Tatooine Droid ## # … with 3 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt; head(starwars[,5]) #head of 5th column ## # A tibble: 6 × 1 ## skin_color ## &lt;chr&gt; ## 1 fair ## 2 gold ## 3 white, blue ## 4 white ## 5 light ## 6 light We close this section with a discussion on the subset function, which allows one to select certain rows from a data set that have some desired features. The function returns a data set with all of the same columns as the original. The syntax is subset(data, condition on variables). As an example, suppose we wanted a data set of all Star Wars characters whose mass is less than 45 kilograms. smallCharacters &lt;- subset(starwars, mass &lt; 45) Since we reference the data set starwars inside the call to subset we don’t need to call the mass column with a $. 2.6 R Markdown documents In this course you will complete a handful of activities using RStudio. You’ll write up the results of these activities as lab reports in R Markdown documents. Markdown is an easy-to-learn “markup” language that converts a plain-text file into a nicely formatted document. R Markdown documents provide an interface between R and Markdown. In a markdown document, you can write R code along with markdown text. When you compile your R Markdown document, RStudio evaluates all code and prints the output. In this way, R Markdown provides a way to create fully reproducible data analysis reports and more (this book was written in R Markdown!). You can use Rmd documents to save and execute code in a notebook style environment and create beautiful reports. A lot of people have already written quite a bit about R Markdown, so we’ll keep this brief. For more detailed information, check out this tutorial provided by RStudio. To play around, start with the following: Create a new Rmd file by clicking “file -&gt; new file -&gt; R Markdown”. Enter a title, your name, and the date, then click OK. This creates a new Rmd file. A document should open that looks like this. An R Markdown document Notice that your document has three different types of environments. The YAML is at the beginning and delineated by --- at the beginning and end. This section determines the overall formatting of your document. You can go wild in this section with some guidance and internet sleuthing. Code chunks which start with ```{r} and end with ```. Code chunks allow you to use R in a notebook-style environment by clicking the green triangle in the upper right-hand corner of the chunk to run the code. You can run any code you can imagine in a code chunk. Plain text and text formatted using markdown. Write whatever you want. Headings are indicated by pound signs, with the size of the heading determined by the number of pound signs, so # Title would be a big title, ## Section would be a section heading, a ### Subsection, and so on. You can make text italic by surrounding the text with * on both sides. You can bold face text with **. For example *foo* renders as foo and **bar** renders as bar. There are a lot more formatting hacks, but that should get you started. As you’re writing a document, you should periodically knit the document Knitting starts at the beginning of your document and evaluates it from top to bottom, then renders the output in whatever format you’ve specified. RStudio executes code chunks in sequential order. RStudio ignores all values and data stored in memory and all loaded packages when knitting. In this way, knitting a document starts a new “environment” from scratch. This feature can lead to headaches if you don’t think about it ahead of time. Some common pitfalls when using R Markdown: forgetting to load packages at the beginning of a document and loading them from the console. In this case, all code chunks will run, but your document may not knit. Notice that I included a few packages in the screenshot above Don’t forget to load packages at the beginning! naming an object from the console (eg: x &lt;- 1:10) and using it in a code chunk, but forgetting to name the same object in a code chunk. In this case, code chunks will all evaluate, but your document will not knit. Some suggestions when using R Markdown: run your code chunks often. This helps ensure everything is working the way you think it should. knit your document early and often. It is a lot easier to find small mistakes as they arise than it is to debug an entire document. be patient. R Markdown is a powerful tool and you can do a lot with it, but it can take a little bit of getting used to. You can use it to do a lot more than write lap reports. For instance, you may be able to write a scientific paper in R Markdown and have all of your writing and data analysis in one package! Try new things; it is really easy to write code that doesn’t run, but it is really hard to break R, RStudio, or a Markdown document. You can only improve by trying. If you want to take a deep dive into markdown, check out the book R Markdown: The Definitive Guide. "],["cross.html", "Chapter 3 Summary statistics and data visualization 3.1 Summary statistics in R 3.2 Data visualization", " Chapter 3 Summary statistics and data visualization At this point you should start feeling more comfortable using R. You know how to define lists, make calculations involving lists, define data frames, install and load packages, and work in and on R Markdown documents. Up to this point you may feel like R has made your life harder; the point of this chapter is to convince you that, at least for doing statistics, R can make your life much, much easier. In all of the examples below we will be using the teacher data set from the openintro package in R which contains salary data for 71 teachers in the St. Louis Public Schools and other variables that may influence salary. For more information, evaluate ?teacher in the console. head(teacher) ## # A tibble: 6 × 8 ## id degree fte years base fica retirement total ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 BA 1 5 45388 3472. 7689. 56549. ## 2 02 MA 1 15 60649 4640. 10274. 75563. ## 3 03 MA 1 16 60649 4640. 10274. 75563. ## 4 04 BA 1 10 54466 4167. 9227. 67859. ## 5 05 BA 1 26 65360 5000. 11072. 81432. ## 6 06 BA 1 28.5 65360 5000. 11072. 81432. 3.1 Summary statistics in R R makes calculating summary statistics a breeze so that we can focus instead on appropriately interpreting and using these statistics. Of course, the software can handle both categorical and numerical variables equally as well. Remember, when analyzing numerical variables, we typically look at the mean/standard deviation or median/IQR as summary statistics; when analyzing categorical variables, we typically look at sample counts and/or proportions. 3.1.1 Numerical variables The commands we use to calculate all of your favorite summary statistics are fairly intuitive and straightforward in R. For example to calculate the mean of a data variable x, simply evaluate mean(x). The list below gives some common summary statistics and an example using the teacher data set. As usual, this is not a complete list. mean(x, ...) and median(x, ...): calculates the average and median of the data set. If x contains NA (unknown or not available) values, considering setting using the option na.rm = T which removes the NA values from your list. The average base salary for a teacher from our data set is mean(teacher$base) ## [1] 56415.96 median(teacher$base) ## [1] 59914 var(x, ...) and sd(x, ...): calculates the sample variance and standard deviation of the data set. For our base teacher salary: var(teacher$base) ## [1] 99684592 sd(teacher$base) ## [1] 9984.217 quantile(x, probs = # or list of percentiles ): returns the value in your data set corresponding to the given percentiles. For instance quantile(x, probs = .25) returns the first quartile of x. quantile(teacher$base, probs = .25) #first quartile ## 25% ## 51174 quantile(teacher$base, probs = .75) #third quartile ## 75% ## 65360 quantile(teacher$base, probs = c(.25, .5, .75)) #Q1, median, Q3 ## 25% 50% 75% ## 51174 59914 65360 IRQ(x,...): returns the interquartile range of the numerical variable x. IQR(teacher$base) ## [1] 14186 quantile(teacher$base, .75) - quantile(teacher$base, .25) ## 75% ## 14186 min(x,...) and max(x,...): returns the minimum and maximum values of the numerical varible x. min(teacher$base) ## [1] 19900 max(teacher$base) ## [1] 68230 summary(x,...): returns 6 summary statistics if x is a numerical variable: min, first quartile, median, mean, third quartile, and max. summary(teacher$base) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 19900 51174 59914 56416 65360 68230 fivenum(x, ...): returns a classic five number summary of the numerical variable x: min, Q1, median, Q3, and max. fivenum(teacher$base) ## [1] 19900 51174 59914 65360 68230 3.1.2 Categorical variables This section will focus on ways to create summary tables (ie frequency tables and contingency tables) for categorical variables. We will focus on using “base R” techniques for these purposes, but Chapter 3 will go into more detail about using the dplyr package to make the construction of more complicated summaries a bit easier. As above, the list below will help you summarize and analyze categorical variables. summary(x, ...): if x is a list of observations of a categorical variable, summary(x) will return a list counts for each value of the variable. Such a table is called a one-way frequency table. For example unique(teacher$degree) #unique values this variable takes on ## [1] BA MA ## Levels: BA MA summary(teacher$degree) #count the number of occurences of each value ## BA MA ## 39 32 table(...): this function is more flexible than summary in that will return either a one-way table (if only one categorical variable is specified) or a two-way/contingincy table (if two categorical variables are specified). #generates a one-way table table(teacher$degree) ## ## BA MA ## 39 32 #generates a two-way table table(teacher$degree, teacher$fte) ## ## 0.5 1 ## BA 0 39 ## MA 1 31 xtabs(~.): the output of this function is very similar to table(...), but the output labels the variables. The input syntax is slightly different because it uses R’s formula syntax. To illustrate the difference in input, we’ll create the same tables as the last example. Variable names follow the ~ and are separated by +. #one-way table xtabs(~degree, data = teacher) ## degree ## BA MA ## 39 32 #two-way table xtabs(~degree + fte, data = teacher) ## fte ## degree 0.5 1 ## BA 0 39 ## MA 1 31 prop.table(...): the input of prop.table is a table itself. This function converts a table of frequencies into a table of proportions. #define one-way table, convert to proportion table tbl1 &lt;- table(teacher$degree) prop.table(tbl1) ## ## BA MA ## 0.5492958 0.4507042 #define two-way table, convert to proportion table tbl2 &lt;- xtabs(~degree + fte, data = teacher) prop.table(tbl2) ## fte ## degree 0.5 1 ## BA 0.00000000 0.54929577 ## MA 0.01408451 0.43661972 3.1.3 Combining it all You can combine these summary statistic functions with various subsetting techniques from the previous chapter to tease out relationships between variables. As an example, using the teacher data set still, we’d expect teachers with more education to earn a higher salary on average. In the examples above we saw that there is only one part time teacher in the sample (the teacher with fte = 0.5) and this person has a lower salary simply from working less. Thus, to compare salaries by degree, we may want to exclude this observation. Doing this manually: #subset for only full time teachers fullTime &lt;- subset(teacher, fte == &quot;1&quot;) #subset by degree bach &lt;- subset(fullTime, degree == &quot;BA&quot;) mast &lt;- subset(fullTime, degree == &quot;MA&quot;) #finally calculate averages mean(bach$base) ## [1] 56257.1 mean(mast$base) ## [1] 57793.74 It would be nice if we could streamline this process. We can do so using the by function. The input is by(dataToSummarize, variableToGroupBy, summaryFunctionToApply). Thus, to recreate our work above we can use by(fullTime$base, fullTime$degree, mean) ## fullTime$degree: BA ## [1] 56257.1 ## ---------------------------------------------------------------------- ## fullTime$degree: MA ## [1] 57793.74 As a final example, we will give a preview of creating the same table in using tidyversesyntax and grammar. Check out the next chapter for much more information, starting with the original teacher data set. fullTime %&gt;% filter( fte == &quot;1&quot;) %&gt;% #select only full time group_by(degree) %&gt;% #specify variables to group by summarize( n = n(), #create summary table, first column is a count avgSal = mean(base)) #second column is average salary ## # A tibble: 2 × 3 ## degree n avgSal ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BA 39 56257. ## 2 MA 31 57794. 3.2 Data visualization There are three main ways to create plots in R: base R, lattice, and ggplot2. We will only learn about base R and ggplot2 in this course. In practice, I use base R to make graphs quickly to get an idea of what’s going on and ggplot2 to make more visually appealing and complicated graphics. In general, you should be able to make most graphs using either package, but sometimes it is easier to use one over the other. Caution: When creating data visualizations you may have to massage your data ahead of time to get it formatted correctly! 3.2.1 Base R The basic function for plotting in base R is simply plot. Often, R will correctly interpret what type of graph you’re trying to make if you format the data correctly. To avoid possible confusion, it’s best to call the type of plot you’re trying to make directly. Each of the functions below have a lot of options and arguments for customizing the visualizations. The best way to learn about these is to simply play around. Histograms: When trying to visualize the distribution of a single numerical variable you often want to start with a histogram. The basic command for this is hist(...). The first histogram below just gives you a rough idea of what the histograms look like, the second demonstrates the types of options you can specify. #no options specified hist(teacher$base) #vs many options specified hist(teacher$base, #variable to make a histogram of breaks = 11, #number of cells, can be left blank freq = FALSE, #displays relative frequency on y-axis, if true, displays frequency main = &quot;Histogram of Teacher Base Salary&quot;, #sets main title xlab = &quot;Base salary (USD)&quot;, #x-axis label ylab = &quot;Frequency&quot; #y-axis label, can be left blank in this case ) Scatter plots: when checking for an association between two numerical variables you will often want to start your analysis with a scatter plot. The function plot(x,y) creates a scatter plot of \\(x\\) and \\(y\\). Make sure these lists have the same length! As above, we’ll make a basic scatter plot, then play around with the labels and options of the same graph. #basic plot of years experience vs total teacher salary plot(teacher$years, teacher$total) #same plot with updated options plot( teacher$years, teacher$total, pch = 16, #change mark to filled in circle main = &quot;Teacher experience vs total salary&quot;, xlab = &quot;Teaching experience (in years)&quot;, ylab = &quot;Total annual salary (USD)&quot;) Bar charts: bar charts help visualize one or two categorical variables; you can think of them as a visualization of one-way and two way tables. The basic function is, no surprise, barplot(...), but you must input a table instead of vectors. #define our tables tbl1 &lt;- prop.table(table(teacher$degree)) # one-way rel. freq. table tbl2 &lt;- xtabs(~degree + fte, data = teacher ) #two-way table #visualize one-way table barplot(tbl1, main = &quot;Relative frequency of teacher degrees&quot;, xlab = &quot;Degree&quot;, ylab = &quot;Proportion&quot;) #visualize two-way table barplot(tbl2, main = &quot;Teacher employment employment by degree&quot;, xlab = &quot;Employment (by FTE)&quot;, ylab = &quot;Frequency&quot;, col = c(&quot;red&quot;, &quot;blue&quot;), #set colors for degree types, alphabetically legend = rownames(tbl2) #create legend with rows from the table ) Box plots: Box plots help us visualize a the distribution of a numerical variable; stacking box plots side by side help us compare the distribution of a numerical variable over several values of a categorical variable. No suprise, both it is easy to make these plots in both situations using the function boxplot. The syntax for creating a box plot for a single numerical variable is simple, as the next example shows. boxplot(teacher$base, ylab = &quot;Base salary (USD)&quot;, main = &quot;Distribution of MI teacher salaries&quot;) When creating a box plot for a numerical variable over several values of a categorical variable, you need to use R’s formula syntax. This syntax will appear a few more times throughout the course since it is used often when looking at relationships between two variables. The basic notation is Response variable ~ explanatory variables or, if your response variable is y and your explanatory variable is x, simply y ~ x. Note the ~ symbol is to the left of the 1 key on your keyboard. The next examples shows a breakdown of teacher salary by highest degree. boxplot(base ~ degree, data = fullTime, xlab = &quot;Degree&quot;, ylab = &quot;Base salary (USD)&quot;, main = &quot;MI teacher base salary by degree&quot;) Before moving on to plotting with ggplot2, we remind the reader that this is by no means a comprehensive list of all plotting features in R. Moreover, each of the plotting functions used above have many more features and are very customizable; you can go down a deep rabbit hole making plots look just how you want them to. 3.2.2 ggplot2 This section will guide you through using the package ggplot2 for data visualization in R. Many find the output from ggplot2 to be more visually appealing than plots from base R, but the plotting syntax in ggplot2 is often much more intuitive than plotting in base R, especially if you’re making more complicated visualizations. The basic idea of plotting with ggplot2 is to Open an empty plot based on a particular data set using the function ggplot(data, aes(...)) where data is the name of the data set you’re using and aes(...) specifies the aesthetic of the plot to come. You specify which variables are assigned to which axes inside aes(...). This will become clearer through examples. Add layers to the empty plot to construct the visualization you’re imagining, separating each layer with +. These layers are geoms, labels, legends, etc. Again, this will become much clearer through some examples. The ability to sequentially add layers to a plot makes complicated plotting more intuitive in ggplot2. For example, adding a line to a scatter plot is simple under this organization because you simply have to add another layer! We will reproduce the plots from the previous section in the examples that follow, tweaking some parameters here and there to give you an idea of how to change visualizations. Histograms: The geom for a histogram is geom_histogram(...). Notice that our plot below does open an empty plot first, then adds layers to it. # open empty plot ggplot(teacher, aes(x = base)) + # add histogram layer with 15 bins. fill changes bar color, but color changes outline color geom_histogram(bins = 15, fill = &quot;steelblue&quot;, color = &quot;black&quot;) + # adds label layer with only a title. can add subtitle and caption. labs( title = &quot;Distribution of MI teacher base salary&quot;) + # changes x-axis label xlab(&quot;Base salary (USD)&quot;) Scatter plots: The geom for a scatter plot is geom_point(...). You can tweak the appearance quite a bit using different arguments inside this (and all!) geoms. For instance the option alpha sets the transparency, color the color of each point, shape the shape, etc. #open empty plot and specify explanatory (x) and response (y) variables ggplot(teacher, aes(x = years, y = total)) + geom_point(color = &quot;steelblue&quot;, alpha = .5) + labs(title = &quot;Teacher salary vs years experience&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) Adding a line of best fit (see Chapter 8 for more information on modeling) to a scatter plot is straightforward with a geom_smooth(...) layer, setting method = 'lm' (this means add a linear model to your plot) ggplot(teacher, aes(x = years, y = total)) + geom_point() + geom_smooth( method = &#39;lm&#39;, formula = y~x, se = FALSE) + #adds line of best fit without error bars labs(title = &quot;Teacher salary vs years experience&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) We can make the same plot, but color the points by each teacher’s degree and change the shape of each point by full/part time status. ggplot(teacher, aes(x = years, y = total, color = degree)) + geom_point(aes(shape = fte)) + labs(title = &quot;Teacher salary vs years experience, by highest degree&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) This school district’s pay scale becomes much more apparent! Teachers with masters degrees earn a little bit more, but everyone’s salary increases at roughly the same rate during their first 11 years, then flattens out. We also see that the outlier with minimal experience and low pay is also the only part-time employee. As a final example of tweaking scatter plots, we can even add a line of best fit for each degree group! ggplot(teacher, aes(x = years, y = total, color = degree)) + geom_point(aes(shape = fte)) + geom_smooth(method = &#39;lm&#39;, formula = y~x, se = FALSE) + labs(title = &quot;Teacher salary vs years experience, by highest degree&quot;) + xlab(&quot;Experience (years)&quot;) + ylab(&quot;Total salary (USD)&quot;) You can see that it is easy to start adding a lot of information to a plot! Be careful, however; there is a fine line between a detailed, informative visualization, and a confusing, overly busy one. Bar graphs: We can make bar graphs easily with ggplot2 using the geom_bar(...) layer. The first two plots below look identical at first pass, but the one on the left is a frequency bar chart, so displays the raw count on the \\(y\\)-axis, but the chart on the right is a relative bar chart. To make the relative frequency bar chart, we include the option y = ..prop.. in the aesthetic. We also have to specify the number of groups to ensure that our proportions are out of the total. # first load a package for side by side plots library(gridExtra) # Frequency bar plot p1 &lt;- ggplot(teacher, aes(x = degree)) + geom_bar(fill = &quot;aquamarine3&quot;, color = &quot;black&quot;) + labs( title = &quot;MI teacher education (count)&quot;) # relative frequency bar plot p2 &lt;- ggplot(teacher, aes(x = degree, y = ..prop.., group =1)) + geom_bar(fill = &quot;aquamarine3&quot;, color = &quot;black&quot;) + labs( title = &quot;MI teacher education (proportion)&quot;) #display both plots side by side. grid.arrange(p1,p2, ncol = 2) Next we’ll make stacked and side by side box plots. #stacked p1 &lt;- ggplot(teacher, aes(x = degree, fill = fte)) + geom_bar(color = &quot;black&quot;) + labs(title = &quot;MI teacher education&quot;) #side by side p2 &lt;- ggplot(teacher, aes(x = degree, fill = fte)) + geom_bar( position = position_dodge(), color = &quot;black&quot;) + # big change is setting position! labs(title = &quot;MI teacher education&quot;) #display both plots side by side grid.arrange(p1, p2, ncol = 2) One could combine our first and second examples of box plots to get side-by-side and stacked relative frequency plots. This takes a little bit of extra fuss that can be avoided by first manipulating the data you input into ggplot2. In other words, to make these types of plots, make the summary calculations on your own earlier, then feed those into the appropriate geom. Box plots: We will finish up our brief tour of ggplot2 by making a few box plots. The geom for box plots is, no surprise, geom_boxplot(...). Let’s first look at a box plot of the teacher’s total salary. ggplot(teacher, aes(y = total)) + geom_boxplot() + theme(axis.text.x=element_blank(), #this layer just removes the scale on the x-axis axis.ticks.x=element_blank()) + labs(title = &quot;Total MI teacher salary&quot;) By adding an x value to our aesthetic, we can produce boxplots of the range of values of a categorical variable. The example below also adds a point representing the mean of each group and colors the outlier red. ggplot(teacher, aes(x = degree, y = total)) + geom_boxplot(outlier.color = &quot;red&quot;, outlier.size = 2) + stat_summary_bin(fun = &quot;mean&quot;, geom = &quot;point&quot;, color = &quot;blue&quot;, size = 2) + labs(title = &quot;MI teacher salaries by degree&quot;) There are many, many more geoms, options, and possible visualizations you can make with ggplot2; this section should give you enough to get started and make some high quality graphics. For more information, check out the official ggplot2 guide. 3.2.3 Saving plots Now that you know how to make plots in R, what do you do with them? Data visualization is essential for two aspects of data analysis: exploration and communication. Initial data visualization can help guide your analysis by illuminating patterns and trends that may be difficult to observe in your raw data. For the same reason, you will want to include high quality visualizations when you write up your work; visualizations help readers get a good understanding of your work without digging deep into the details. So how do you get your visualizations into a write up? There are two possibilities: Do your entire analysis and write-up in an RMarkdown document! You can hide code chunks so that output only shows plots and calculation results. You can use RMarkdown to create impressive, professional documents. You can save your plot in your favorite file format. There are two ways to do this, from the console or from the File/Plot explorer window in RStudio. Both options will save the file to whatever directory or folder you are currently working in. File/Plot Explorer: When you make a plot in RStudio it displays in the bottom right Plot window. That window has an “Export” button. Clicking this button reveals a bunch of options for saving your plot. Pick whichever you think is best for your application. From the console: You can also export an image programmatically. The basic process is 1) open a “graphics device”, 2) code your plot, then close the graphics device. This option is best if you have to save multiple images or want to customize the file you’re saving. As an example, the following code will create a PDF file called “salary_viz.pdf” of the side-by-side box plot we created at the end of the base R plotting section. pdf(&quot;salary_viz.pdf&quot;) #opens graphic device boxplot(base ~ degree, data = fullTime, xlab = &quot;Degree&quot;, ylab = &quot;Base salary (USD)&quot;, main = &quot;MI teacher base salary by degree&quot;) dev.off() You can save your plot in many different file formats; the command to open the appropriate graphics device is usually obvious. For instance, a png graphics device is opened with the function png(...). "],["wrangling-data.html", "Chapter 4 Wrangling data 4.1 The goal: “tidy” data. 4.2 The pipe 4.3 Common tidying operations 4.4 Mutate, group by, and summarize", " Chapter 4 Wrangling data “Wrangling data” is a term used to describe the processes of manipulating or transforming raw data into a format that is easier to analyze and use. Data professionals often spend large chunks of time on the data wrangling phase of a project since the analysis and use flows much more smoothly when the wrangling is done appropriately. Like most chapters in this book, we won’t go too in depth into this subject, but we will cover enough to get you started. In particular, this section aims to help you understand what the end goal of data wrangling might look like. In other words, start understanding what a good, easy to use data set might look like. be able to use R’s tidyverse package to perform some basic data wrangling tasks. This section of notes will can be thought of as a condensed version of the Wrangle and Program sections of R for Data Science and references this book substantially. Remember, since we’re using the tidyverse package in this section, you have to load the package with library(tidyverse) 4.1 The goal: “tidy” data. In the early days of STAT216, we stipulated that data sets should contain variables in columns and observations in rows. This is the common convention in data science, but this convention is not always followed, especially when you’re collecting data from out in the wild. This can come about for a myriad of reasons, but one common reason you’ll encounter “unconventional data organization” is that the data enter prioritizes human-readability over machine-readability and ease of analysis. For example, suppose you were giving a blind taste test of three types of diet cola: diet Coke, diet Pepsi, and diet RC. You give 3 groups of 10 random people a drink and ask them to rate their preference on a scale of 1-5 with 5 being great and 1 being awful. A natural way to organize the data from your experiment may look something like the table below (the numbers are randomly generated). set.seed(1123) df &lt;- tibble( coke = sample(1:5, 10, replace = T), pepsi = sample(1:5, 10, replace = T), rc = sample(1:5, 10, replace = T)) #generate table of random numbers. df %&gt;% knitr::kable() coke pepsi rc 3 3 1 4 2 1 4 3 2 1 4 3 3 4 5 4 1 2 1 5 3 2 3 3 3 5 2 3 4 5 Note, however, that the drink someone is tasting is actually a variable! This means our data set doesn’t have variables in columns, it splits one variable up into three different columns. This type of thing is quite common. Fortunately, R gives us a way to easily transform such data sets as we’ll see. The appropriate format for our convention of variables in columns, observations in rows would be set.seed(1123) # make sure random numbers generated above are the same. dfTidy &lt;- tibble( #first a column of the drink participants had drink = c( rep(&quot;coke&quot;, 10), rep(&quot;pepsi&quot;, 10), rep(&quot;rc&quot;, 10)), # then a column of their scores score = c(sample(1:5, 10, replace = T), sample(1:5, 10, replace = T), sample(1:5, 10, replace = T) )) head(dfTidy) ## # A tibble: 6 × 2 ## drink score ## &lt;chr&gt; &lt;int&gt; ## 1 coke 3 ## 2 coke 4 ## 3 coke 4 ## 4 coke 1 ## 5 coke 3 ## 6 coke 4 This data set is less easy to read as a human (since it has 30 rows and two columns), but is much easier to analyze in R (and pretty much all other available software). This example typifies our goal of tidy data: Each variable should have its own column. Each observation/observational unit should have its own row. Again, the purpose of tidy data is to streamline and make the analysis stage of your data journey easier. Most functionality in R is substantially easier to implement when you start with a tidy data set. As an example, if we’d like to make side-by-side box plots of the results of our cola taste test using the original data set df in ggplot2, we might try something like ggplot(df, aes(x = ???, y = ???)) + geom_boxplot() but run into an immediate problem. What is the \\(x\\) variable? What is the \\(y\\)? If we use the tidy version of the same data, the answer is clear however: \\(x\\) is the drink, \\(y\\) is the score, since drink is the explanatory variable and taste preference is the response. Using dfTidy from above instead, we’d simply use the following. ggplot(dfTidy, aes(x = drink, y = score)) + geom_boxplot() Essentially all tools in the tidyverse package are designed around having tidy data. In the next section, we’ll learn about one of the main functions from the tidyverse package used in tidying and analyzing data, the pipe. 4.2 The pipe The pipe, typed as %&gt;%, is a function from the tidyverse package whose intention is to make a sequence of transformations or operations on a data set more clear and easier for humans to follow and write. In some sense, you can think of it as adding layers of transformations to a data set, just like you use the + function to add layers to a graph in ggplot2. Note: this analogy is by design from the nice folks who created the tidyverse! In essence, the pipe tells R to do the operation on the right side of the pipe to the data set on the left of it; if there are multiple pipes in a sequence, they are evaluated left to right/top to bottom. Let’s try an example to see how it works. We’ll start with the dfTidy data from above. Suppose we forgot to add the results from a blind taste test of Diet Dr. Pepper. We load that data into R (simulating it of course in this case) set.seed(1123) dp &lt;- tibble(drink = rep(&quot;dp&quot;, 10), score = sample(1:5, 10, replace = TRUE)) Now suppose want to Add this data to dfTidy. Add a new column that rescales the scores so that they’re between 0 and 1 instead of 1 and 5. We can do this quickly using the following newDf &lt;- dfTidy %&gt;% #start with dfTidy bind_rows(dp) %&gt;% #adds dp to the bottom of dfTidy mutate(newScale = .25*score - .25) #add new column to whole dataset. To see that this did what we hope, let’s look at a random sample of 10 rows using the slice_sample function set.seed(1123) #more piping! newDf %&gt;% slice_sample(n = 10) ## # A tibble: 10 × 3 ## drink score newScale ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 dp 2 0.25 ## 2 coke 1 0 ## 3 pepsi 1 0 ## 4 dp 4 0.75 ## 5 dp 3 0.5 ## 6 dp 3 0.5 ## 7 rc 3 0.5 ## 8 rc 2 0.25 ## 9 coke 2 0.25 ## 10 dp 1 0 It does as we’d hoped! Great. You may be wondering why piping exists. Earlier we said it was to make code easier to read and intuitive to write. But with respect to what? There are two older programming conventions that piping is trying to clarify: “overwriting the original” and “function composition”. Let’s create dfTidy using these two strategies. Overwriting the original: At each stage, overwrite what you did at the previous stage. It’s not too bad in this example, but can be tedious if you have say 10 different transformations to make. It is especially inconvenient if you need to make changes since you have to track those changes through all your steps. newDf1 &lt;- bind_rows(dfTidy, dp) newDf1 &lt;- mutate(newDf1, newScale = .25*score - .25) #check to see we got the same thing all.equal(newDf, newDf1) ## [1] TRUE Function composition: This is probably the least human readable way to write create our new data set, but it also makes explicit what the computer is doing and it looks shorter. Notice that you have to read from the inside out and right to left. Once again, this isn’t too bad with only two transformations, but you could imagine how much of a nightmare this could be with 10 transformations, especially in terms of reading the code. newDf2 &lt;- mutate( bind_rows(dfTidy, dp), newScale = .25*score - .25) #are they equal? all.equal(newDf, newDf2) ## [1] TRUE Hopefully you think the piped sequence of transformations is more straightforward to read and write than the other two options. Now that we’re more comfortable with the pipe, let’s use it to implement some common forms of transformations! 4.3 Common tidying operations If you’re dealing with a data set that isn’t tidy, it typically violates our tidy criteria in one of two ways: Variables can be split across several columns. In this case, the column names are not variable names, but values of a variable like the original data set for our diet cola blind taste test df above. Observations can be split across several rows. There are two functions in the tidyverse used to handle these cases: pivot_longer(...) and pivot_wider(...). Caution: it can take a little bit of time get these functions to do exactly what you want. Have a bit of patience and you’ll improve with time! The function pivot_longer(...) is used to tidy a data set when variables are split across several columns; it takes the values of the variable that are the untidy column names, makes a new column for them, then organizes the values of the columns appropriately. This is best observed in an example: instead of reentering the data in our cola example, we could instead use pivot longer. dfTidy1 &lt;- df %&gt;% #specify the columns are actually values pivot_longer(cols = c(&quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;), #then specify column/variable name names_to = &quot;drink&quot;, #finally specify the name of the values values_to = &quot;score&quot; ) glimpse(dfTidy1) ## Rows: 30 ## Columns: 2 ## $ drink &lt;chr&gt; &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;, &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;, &quot;coke&quot;, &quot;pepsi&quot;, &quot;rc&quot;, &quot;coke&quot;, &quot;… ## $ score &lt;int&gt; 3, 3, 1, 4, 2, 1, 4, 3, 2, 1, 4, 3, 3, 4, 5, 4, 1, 2, 1, 5, 3, 2, 3, 3, 3, 5, … At first it doesn’t seem like this data set is the same as dfTidy, but notice that R arranged the values as “coke”, “pepsi”, “rc”. If we rearrange the rows alphabetically by drink, we’ll see that they are in fact that same data sets. all.equal( dfTidy %&gt;% arrange(drink), dfTidy1%&gt;% arrange(drink)) ## [1] TRUE The function pivot_wider(...) is used to tidy a data set when observations are split across several rows. The tidyverse has a data set simply called table2 that provides an example of observations being split across several rows. This table shows “the number of TB cases documented by the World Health Organization in Afghanistan, Brazil, and China between 1999 and 2000” (from the documentation). table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 In this example, the observations are a country in a year, so cases and population are not values. Instead, they are variables, since in a given year each country has a population and a certain number of TB cases. table2 %&gt;% #first specify where the new column names are coming from pivot_wider(names_from = &quot;type&quot;, #next specify where the new column values are coming from values_from = &quot;count&quot;) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 Much better! Notice that the same information is displayed in both tables. The latter option is simply easier to work with. 4.4 Mutate, group by, and summarize Once you’re working with a tidy data set, you may want to add columns to it, create a summary of the data, or both. The key functions for performing these operations are mutate(...), group_by(...), summarize(...), and transmute(...), all of which are from the dplyr package, which is part of the tidyverse. Before looking at specific examples of these functions in action, we’ll define what they do. mutate(...) adds columns to an existing data set and keeps the old columns. transmute(...) creates new columns from a data set, but drops the old existing columns. group_by(...) takes column names as input and converts your data set to a “grouped” table. This means all operations and summary/aggregate functions are performed by group. The groups are defined by the values in the columns you specified. summarize(...) creates a new data set giving a customized summary of your data set. All operations are done by the grouping variables. For examples of each function, we will use the gss2010 data set, which is part of the openintro package. As a reminder, to follow along you need to attach this package with library(openintro) data(gss2010) The gss2010 data set has 2044 measurements of 5 varibles: hrsrelax: hours relaxed after work on an average work day. mntlhlth: number of days in the last month where mental health was not good. hrs1: weekly hours worked. degree: highest educational attainment or degree. grass: Should marijuana be legalized? Working with this data set provides an obstacle that we haven’t encountered much so far in these notes: NA values. A quick glimpse at the data reveals that there are quite a few missing values. In learning how to use the functions listed above, we’ll also see a few strategies for dealing with missing values. glimpse(gss2010) ## Rows: 2,044 ## Columns: 5 ## $ hrsrelax &lt;int&gt; 2, 4, NA, NA, NA, NA, 3, NA, 0, 5, 5, NA, 3, NA, NA, NA, NA, NA, NA, 2, 5, … ## $ mntlhlth &lt;int&gt; 3, 6, NA, NA, NA, NA, 0, NA, 0, 10, 0, NA, 0, NA, NA, NA, NA, NA, NA, 5, 0,… ## $ hrs1 &lt;int&gt; 55, 45, NA, NA, NA, NA, 45, NA, 40, 48, 26, NA, 40, NA, NA, NA, NA, NA, NA,… ## $ degree &lt;fct&gt; BACHELOR, BACHELOR, LT HIGH SCHOOL, LT HIGH SCHOOL, LT HIGH SCHOOL, LT HIGH… ## $ grass &lt;fct&gt; NA, LEGAL, NA, NOT LEGAL, NOT LEGAL, LEGAL, NA, NA, NOT LEGAL, NA, NA, NA, … First, let’s add a new column to our data set that gives the total number of hours each person in this survey spend at work or relaxing and another column that reports the proportion of their time spent working or relaxing Monday through Friday. Note that since hrsrelax gives the number of hours they relax in a day, we should add 5 times that amount for total time spent relaxing during the work work. new_gss &lt;- gss2010 %&gt;% mutate(totalHours = 5*hrsrelax + hrs1, #hours spent working and relaxing propTimeWorking = totalHours/(5*24)) #proportion of time spent working/relaxing # random sample of 7 rows from the data set glimpse( slice_sample(new_gss, n = 7) ) ## Rows: 7 ## Columns: 7 ## $ hrsrelax &lt;int&gt; 4, 6, 6, NA, NA, 7, NA ## $ mntlhlth &lt;int&gt; 0, 0, 2, NA, NA, 0, NA ## $ hrs1 &lt;int&gt; 55, 35, 9, NA, NA, 20, NA ## $ degree &lt;fct&gt; LT HIGH SCHOOL, HIGH SCHOOL, LT HIGH SCHOOL, HIGH SCHOOL, HIGH SCHOO… ## $ grass &lt;fct&gt; NOT LEGAL, NA, LEGAL, NA, NA, NA, LEGAL ## $ totalHours &lt;dbl&gt; 75, 65, 39, NA, NA, 55, NA ## $ propTimeWorking &lt;dbl&gt; 0.6250000, 0.5416667, 0.3250000, NA, NA, 0.4583333, NA Note that whenever one of our variables was unavailable, both of our new columns gave an NA value as well. Suppose now that we wanted the same information, but we wanted to exclude all rows with NA values, and we only wanted to see our new columns and the respondent’s degree. This is a perfect situation for transmute(...). new_gss &lt;- gss2010 %&gt;% transmute(degree = degree, #keeps the degree column totalHours = 5*hrsrelax + hrs1, #hours spent working and relaxing propTimeWorking = totalHours/(5*24)) %&gt;% drop_na() #drops all rows with an NA slice_sample(new_gss, n = 7) ## # A tibble: 7 × 3 ## degree totalHours propTimeWorking ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GRADUATE 70 0.583 ## 2 LT HIGH SCHOOL 54 0.45 ## 3 HIGH SCHOOL 52 0.433 ## 4 BACHELOR 55 0.458 ## 5 BACHELOR 65 0.542 ## 6 BACHELOR 70 0.583 ## 7 GRADUATE 50 0.417 Next up, let’s start making some summary tables. First, let’s see if there’s a correlation between educational attainment and hours worked, hours relaxing, or days with mental health struggles. To do this, we want to group our dataset by the degree variable so that all of our summary functions are applied to the groups of respondents with the same degrees. gss2010 %&gt;% group_by(degree) %&gt;% summarize( meanHrsWrk = mean(hrs1, na.rm = TRUE), # avg hours worked, removing NA sdHrsWrk = sd(hrs1, na.rm = TRUE), # sd of hours worked, removing NA meanHrsRlx = mean(hrsrelax, na.rm = TRUE), meanMntlHlth = mean(mntlhlth, na.rm = TRUE) ) %&gt;% arrange(desc(meanHrsWrk)) # arrange results in descending order ## # A tibble: 5 × 5 ## degree meanHrsWrk sdHrsWrk meanHrsRlx meanMntlHlth ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BACHELOR 42.5 13.6 3.48 2.67 ## 2 JUNIOR COLLEGE 41.4 18.1 3.53 3.56 ## 3 GRADUATE 40.8 15.5 3.67 2.40 ## 4 HIGH SCHOOL 39.6 15.0 3.79 4.58 ## 5 LT HIGH SCHOOL 38.7 15.8 3.73 4.87 So everyone works fairly similarly and relax a similar number of hours every night, but we observe that the number of days per month with mental health struggles seems to decrease with an increase in educational attainment. We could also see if there’s any sort of relationship between degree and stance on marijuana legalization. First, let’s throw out all the rows with an NA for the grass variable and see how many people have type of degree. gss2010 %&gt;% filter( !is.na(grass) ) %&gt;% #selects all rows where grass is *not* NA group_by(degree) %&gt;% summarize( degCount = n(), # special function to count the number in each group legalCount = sum(grass == &quot;LEGAL&quot; ), legalProp = legalCount / degCount ) %&gt;% arrange(desc(legalProp)) ## # A tibble: 5 × 4 ## degree degCount legalCount legalProp ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 GRADUATE 136 73 0.537 ## 2 BACHELOR 231 119 0.515 ## 3 HIGH SCHOOL 611 304 0.498 ## 4 JUNIOR COLLEGE 86 42 0.488 ## 5 LT HIGH SCHOOL 195 65 0.333 So, roughly speaking, the percentage of people who believe marijuana should be legalized increases with educational attainment. This is likely not too surprising, but interesting to see shake out in the summary table none the less! As usual, one can (and should!), do quite a bit more with these functions, but these examples should give you a reasonable idea about how they work and what they can do for you. "],["distribution-calculations.html", "Chapter 5 Distribution calculations 5.1 Finite discrete distribution calculations 5.2 Named distribution calculations", " Chapter 5 Distribution calculations The second module of STAT216 at FVCC focuses on the basics of probability theory. We start out learning the foundations: interpretations of probability (frequentist vs Bayesian) along with the notions of independence, mutually exclusive events, conditional probability, and Bayes’ Theorem. We then move on to thinking about random variables. Recall that a random variable is simply the assignment of a number to every element of the sample space for the random process in question. From this assignment we can assign probabilities to the values our random variable can take on. Random variables also give us a way to define the average outcome of a random process and measure the spread of outcomes around that average. Recall that random variables come in two primary flavors, discrete and continuous. Continuous random variables can take on at least some interval worth of real numbers. Discrete random variables, on the other hand, can only take on values with sufficiently large gaps in between. For example, the height of a STAT216 student is a continuous random variable, but the number of books a STAT216 student owns in discrete. Distinguishing between these two types of random variables is important because the flavor dictates quite a bit about how we use and make calculations associated with a given random variable. The probability distribution of a discrete random variable can be represented in a tabular form, but the probability distribution of a continuous random variable must be represented by a function. The table below helps compare and contrast discrete and continuous probability distributions. Discrete Continuous all probabilities \\(\\geq 0\\) graph of function above or on \\(x\\)-axis sum of probabilities is 1 area between graph of function and \\(x\\)-axis is one Short technical note: Using the right mathematical perspective you do not actually treat discrete and continuous random variables any differently, but this perspective takes a lot of technical machinery and isn’t necessary. Suffice it to say that the analogies above are more than just analogies. There are many, many different types of each flavor of random variable - too many to name or describe - but statisticians and mathematicians have studied many families of random variables that have commonalities. In STAT216, we learn about a handful of these families, discussed below. You can skip the details for now and come back to these as we learn more about them in class. Discrete distributions: Bernoulli random variables: only two outcomes, typically 0 and 1 with 0 corresponding to failure and 1 to success for the random process you’re studying. \\(x_i\\) 0 1 \\(P(X = x_i)\\) \\(1-p\\) \\(p\\) Geometric random variables: repeat independent identical (iid) trials of a Bernoulli random variable until the first success and count the number of trials. If \\(p\\) is the probability of success in a single trial, \\[ P(X = k) = (1-p)^{k-1}p \\] Binomial random variables: repeat a fixed number \\(n\\) of iid trials of a Bernoulli random variable and count the number of successes, \\(k\\). \\[ P(X = k) = {n \\choose k} p^k (1-p)^{n-k}\\] Continuous distributions: Normal distributions: a family of symmetric, unimodal continuous distributions determined by an average and standard deviation. Student’s \\(t\\)-distribution: a family of symmetric, unimondal continuous distributions determined by a single quantity: degrees of freedom, \\(df\\). These distributions have fatter tails than standard normal distribution. \\(\\chi_^2\\)-distributions: a family of right-skewed distributions determined by a single parameter, \\(df\\), or degrees of freedom. A \\(\\chi^2\\)-distribution is a sum of \\(df\\) squared standard normal distributions. \\(F\\)-distributions; a family of right-skewed distributions determined by two degrees of freedom \\(df_1\\) and \\(df_2\\). Roughly speaking, an \\(F\\)-distribution is the ratio of two \\(\\chi^2\\) distributions The remainder of this chapter focuses on how to make calculations for each of these distributions in R. This may come as a welcome relief from the last few chapters because we’ll be using R more as a big calculator again for a bit. 5.1 Finite discrete distribution calculations This short section will describe how to calculate the expected value, variance, and standard deviation of a finite, discrete probability distribution. The key is to remember that R performs all list calculations component-wise. With this in mind, recall that the expected value and variance of a finite discrete random variable \\(X\\) are \\[ E(X) = \\sum_{i= 1}^n x_i P(X = x_i) \\quad \\text{and} \\quad V(X) = \\sum_{i = 1}^n P(X = x_i)\\left(x_i - E(X) \\right)^2\\] so \\(E(X)\\) is just the average outcome of \\(X\\), weighted by the probabilities associated to each value of \\(X\\). Similarly \\(V(X)\\) is just the average squared deviation of the values of \\(X\\) from the mean. Let’s learn how to make these calculations in R from an example. Suppose you and your friends invent a silly game to pass the time based on flipping a coin and rolling a standard six-sided die. If you flip a heads and roll an even number, you win a dollar. If you flip a heads and roll an odd prime, you win two dollars. If you flip a tails and roll a 6, you win five dollars. Otherwise, you win nothing. Question How much should you and your friends charge to play the game? To answer this, we need to know the expected winnings since expected value gives us the average outcome. You and your friends need to charge at least the expected winnings. To make this calculation in R we need to store the values of our random variable and the associated probabilities in lists. The sample space for our game has size 12; you can think about is as the collection of ordered pairs \\(\\{ (x,y) | x \\in \\{H, T\\}, y \\in \\{1,2,\\ldots, 6\\} \\}\\). There are 3 ways to flip a heads and roll an even number, two ways to flip a heads and roll an odd prime, and only one way to flip a tails and roll a six. Thus the probability distribution of our winnings is \\(x_i\\) 1 2 5 0 \\(P(X = x_i)\\) 3/12 2/12 1/12 6/12 Storing this in R val &lt;- c(1,2,5,0) prob &lt;- c(3/12, 2/12, 1/12, 6/12) Now, to calculate the expected value, we just need to add up the product of val and prob: exp_val &lt;- sum(val*prob) exp_val ## [1] 1 So on average, a player of this game will win one dollar. This means you and your friends should charge at least one dollar to play the game. Let’s also calculate the variance and standard deviation of the winnings for this game. The strategy for calculating variance is exactly the same as with the expected value. varX &lt;- sum(prob*(val - exp_val)^2) varX ## [1] 2 As usual, the standard deviation is just the square root of the variance so sdX &lt;- sqrt(varX) sdX ## [1] 1.414214 This means, on average, someone playing this game will win $1 with an average change between games of about $1.41. Next up, we’ll learn about making probability calculations for some of the named distributions discussed in the beginning of this chapter. 5.2 Named distribution calculations R has a robust library of probability distribution calculators built in. The functions are organized so that they behave similarly for all of the families of distributions. The basic syntax is \\[ \\text{function type} + \\text{distribution} \\] The table below describes the type of function and describes what they do for both continuous and discrete probability distributions Function input Continuous Discrete d (density) value of random variable value of function / height of graph of probability density function at a given \\(x\\)-value Probability that random variable takes on a specific value p (percentile) value of random variable returns a percentile, ie a lower tail probability of associated to a value of the random variable, \\(P(X \\leq x)\\). Same as continuous q (quantile) a percentile returns a quantile, ie the value of a random variable corresponding to the percentile input Same as continuous r (random) whole number \\(n\\) \\(n\\) randomly sampled numbers from the distribution Same as continuous Note that the biggest difference lies in the the density functions. This is because probabilities correspond to areas under the graph of the probability density function for continuous random variables, so the probability of observing any particular value is zero in this case. For a discrete probability distribution, however, the density function actually returns probabilities. The percentile p and the quantile q functions both have an option called lower.tail that is a logical, so either TRUE or FALSE. If lower.tail=TRUE, the percentile functions return \\(P(X \\leq x)\\) and the quantile functions return the value \\(x\\) such that \\(P(X \\leq x)\\). If lower.tail=FALSE, the percentile functions return \\(P(X &gt; x)\\) and the quantile functions return the value \\(x\\) such that \\(P(X &gt; x)\\). We will look at examples in what follows that will clarify these functions and their arguments. 5.2.1 Discrete random variables Let’s start off with some named families of discrete random variables. We’ll only look at binomial and geometric distributions, but once you have these down, you should be be able to figure out how to use any other discrete random variable distribution functions such as those associated to Poisson or hypergeometric random variables. Note: we do not cover Poisson on or hypergeometric distributions in this class! 5.2.1.1 Binomial distribution We’ll start with the definition and a motivating example. Definition: Suppose you repeat independent and identical trials of a Bernoulli experiment a fixed number \\(n\\) times. The random variable \\(X\\) counting the number of successes \\(k\\) is called a binomial random variable. If the probability of success in each trial is \\(p\\), then the probability of failure is \\(1-p\\) and A little work shows \\[ P(X = k) = { n \\choose k } p^k (1-p)^{n-k}. \\] In class we learn formulas for the expected value and variance of geometric random variables: \\[ E(X) = np \\quad \\text{and} \\quad V(X) = np(1-p). \\] The R shorthand for binomial distributions is binom, so the primary functions for binomial calculations and their outputs are: dbinom(k, n, p) \\(= P( X = k) = { n \\choose k } p^k (1-p)^{n-k}\\) pbinom(k, n, p) \\(= P( X \\leq k) = \\sum_{i=0}^k { n \\choose i } p^i (1-p)^{n-i}\\) pbinom(k, n, p, lower.tail = FALSE) \\(= P( X &gt; k) = \\sum_{i=k+1}^n { n \\choose i } p^i (1-p)^{n-i}\\) qbinom(prob, n, p) is the integer \\(k\\) such that \\(P(X \\leq k) =\\) prob. qbinom(prob, n, p, lower.tail = FALSE) is the integer \\(k\\) such that \\(P(X &gt; k) =\\) prob. rbinom(N, n, p) generates N numbers between \\(0\\) and \\(n\\) by repeating the binomial experiment N times. Now let’s look at an example to see how we can use these functions in practice. Example: A Gallup poll showed that roughly 43% of all voters in the USA identify as politically independent as of September 16, 2022. You decide to loosely test this claim in the Flathead Valley by polling a random sample of 15 people about their political ideology. Assuming the distribution of political party affiliation in the Flathead is the same as the nation, counting the number of independents in your poll determines a binomial random variable. We can get a feel for the distribution of this variable with a bar chart. bf &lt;- tibble(k = 0:15, prob = dbinom(k, 15, .43)) ggplot(bf, aes(x = as.factor(k), y = prob)) + geom_col(color = &quot;black&quot;, fill = &quot;steelblue&quot;) + xlab(&quot;k&quot;) + ylab(&quot;P(X=k)&quot;) + labs(title = &quot;Binomial Prob dist w/ p = .43 and n = 15&quot;) Now let’s use R to answer the following questions: What is the probability of exactly 4 independents in your poll? We want \\(P(X = 4)\\), with \\(n = 15\\), \\(k = 4\\) and \\(p = .43\\), so dbinom(4, 15, .43) ## [1] 0.0963008 What is the probability of at most 4 independents in your poll? We want \\(P(X \\leq 4)\\), with \\(n = 15\\), \\(k = 4\\) and \\(p = .43\\), so pbinom(4, 15, .43) ## [1] 0.1545517 What is the probability of at least 4 independents in your poll? We want \\(P(X \\geq 4) = P(X &gt; 3)\\), with \\(n = 15\\), \\(k = 4\\) and \\(p = .43\\), so pbinom(3, 15, .43, lower.tail= FALSE) ## [1] 0.9417491 #or 1 - pbinom(3, 15, .43) ## [1] 0.9417491 What is the third quartile of this distribution? We want to find the value \\(k\\) so that \\(P(X \\leq k) = .75\\), so qbinom(.75, 15, .43) ## [1] 8 Now note: pbinom(8, 15, .43) ## [1] 0.857269 which is greater than .75; R finds the closest integer valued quantile without going under the given percentile. 5.2.1.2 Geometric distribution As above, we’ll start with a definition then move to a motivating example. Definition: Suppose you repeat independent and identical trials of a Bernoulli experiment until your first success. The random variable \\(X\\) counting the number of trials you perform is a geometric random variable. If the probability of success in each trial is \\(p\\), then the probability of failure is \\(1-p\\) and some work shows \\[ P(X = k ) = p(1-p)^{k-1}\\] In class we learn formulas for the expected value and variance of geometric random variables: \\[ E(X) = \\frac{1}{p} \\quad \\text{and} \\quad V(X) = \\frac{1-p}{p^2}. \\] The R shorthand for binomial distributions is geom. R requires that you enter the number of failures before your first success, so the primary functions for binomial calculations and their outputs are: dgeom(k-1, n, p) \\(= P(X = k ) = p(1-p)^{k-1}\\) pgeom(k-1, n, p) \\(= P( X \\leq k) = \\sum_{i=1}^{k} p (1-p)^{i-1}\\) pgeom(k-1, n, p, lower.tail = FALSE) \\(= P( X &gt; k) = \\sum_{i=k+1}^\\infty p (1-p)^{i-1}\\) qgeom(prob, n, p) is the integer \\(k\\) such that \\(P(X \\leq k) =\\) prob. qgeom(prob, n, p, lower.tail = FALSE) is the integer \\(k\\) such that \\(P(X &gt; k) =\\) prob. rbinom(N, n, p) generates N integers by performing the experiment \\(N\\) times and count the number of trials until the first success. Let’s use the same example, but in a slightly different context to help differentiate between binomial and geometric random variables. Example: A Gallup poll showed that roughly 43% of all voters in the USA identify as politically independent as of September 16, 2022. You decide to loosely test this claim in the Flathead Valley by polling random people until you find someone who identify as politically independent. You count the number of people you survey. Assuming the distribution of political party affiliation in the Flathead is the same as the nation, counting the number of people you survey until your first independent determines a geometric random variable. We can get a feel for the distribution of this variable with a bar chart. gf &lt;- tibble(k = 1:15, prob = dgeom(k-1,.43)) ggplot(gf, aes(x = as.factor(k), y = prob)) + geom_col(color = &quot;black&quot;, fill = &quot;steelblue&quot;) + xlab(&quot;k&quot;) + ylab(&quot;P(X=k)&quot;) + labs(title = &quot;Geometric Prob. dist. w/ p = .43 for k = 1 to 15&quot;) What is the probability you the 6th person you poll is the first independent? We’re trying to find \\(P(X = 6)\\), so dgeom(5, .43) ## [1] 0.02587276 # or (1-.43)^5*.43 ## [1] 0.02587276 What is the probability you will survey at most 6 people before meeting your first independent? We’re trying to find \\(P(X \\leq 6)\\) so pgeom(5, .43) ## [1] 0.9657036 What is the probability you will survey at least 6 people before meeting your first independent? We’re trying to find \\(P(X \\geq 6) = P(X &gt; 5)\\) so pgeom(4, .43, lower.tail = FALSE) # 1 - P(X &lt;= 5) = P(X &gt;5) = P(X &gt;=6) ## [1] 0.06016921 # or 1 - (dgeom(0, .43) + dgeom(1, .43) + dgeom(2, .43) + dgeom(3, .43) + dgeom(4, .43)) ## [1] 0.06016921 What is the third quartile of this distribution? qgeom(.75, .43) ## [1] 2 As with a binomial random variable, note that pgeom(2, .43) ## [1] 0.814807 is greater than .75; R finds the nearest integer yielding a larger percentile. 5.2.2 Continuous random variables As mentioned earlier in this Chapter, we will encounter numerous families of continuous distributions throughout this course. The normal distributions are likely the most important for our purposes. Moreover, once you get a feel for making normal distribution calculations in R, you’ll be in a great place for understanding and making calculations using any of the remaining distributions. Thus, we will mostly focus on normal distributions in this section. We remind the reader that if \\(X\\) is a continuous random variable, then \\(X\\) has an associated function called a probability density function \\(f\\). An area between the graph of \\(f\\) and the horizontal axis represents a probability. With this in mind \\(P(X = k) = 0\\) for any value \\(k\\) since the area of a line is 0. Thus: \\(P(X \\leq k ) = P(X &lt; k)\\) and \\(P(X \\geq k ) = P(X &gt; k)\\) For us, from a calculation perspective, this is one of the main differences between discrete and continuous random variables. 5.2.2.1 Normal distributions Long before you enrolled in this class you likely interacted with normal distributions: they are the famous “bell-shaped curves.” In more mathematical language, they are symmetric, unimodal distributions. Note that they are not the only distributions with these properties, but in some sense, they are the most common. Normal distributions arise often in practice because many naturally observed quantities follow approximately normal distributions (eg height, weight, etc of populations of critters) and the distributions of many sample statistics are approximately normal. For instance, imagine all possible random samples of 1000 Flathead county residents. For each of these samples, record the proportion of people who identify as politically Republican. This is a random variable that is approximately normally distributed. More on this in later chapters - this is a quick example of a sampling distribution which a key tool for statistical inference. From an intuitive perspective, a bell shaped curve should be determined by two quantities: its center (determining the line of symmetry) and its spread. In fact, these two quantities are the parameters determining a normal distribution! We call the center the mean \\(\\mu\\) and the spread standard deviation \\(\\sigma\\). If a random variable \\(X\\) we may sometimes write \\[ X \\sim N( \\mu, \\sigma ) \\] to indicate that \\(X\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). There is one special distinguished normal distribution that we call the standard normal distribution: it has a mean of \\(\\mu = 0\\) and a standard deviation of \\(\\sigma = 1\\). We call this the standard normal distribution and typically denote it with \\(Z\\), so \\[ Z \\sim N(\\mu = 0, \\sigma = 1). \\] The plot below shows the standard normal curve. ggplot( tibble( x = c(-3,3)), aes(x = x)) + stat_function(fun = dnorm) + xlab(&quot;Z&quot;) + ylab(&quot;&quot;) + labs(title = &quot;Prob. Density Fun. of Std. Normal Dist.&quot;) The standard normal is distinguished because any normal distribution \\(X\\) can be transformed into the standard normal by taking Z-scores. The \\(Z\\)-score of a measurement \\(x\\) from a normal distribution $ X N( , )$ is \\[ z = \\frac{x - \\mu}{\\sigma}.\\] Notice that if \\(x = \\mu\\), then \\(z = 0\\), \\(x = \\mu \\pm \\sigma\\), then \\(z = \\pm 1\\). From this we see that \\(X\\) in fact is transformed into the standard normal distribution. Now that we’ve reviewed the basics of normal distributions, let’s see how we can make some probability calculations using R. It should be no surprise that the main functions are pnom(...) and qnorm(...). If \\(X \\sim N(m, s)\\) dnorm(k, m, s) is the height of the graph of the probability density function at \\(X = k\\). !Caution! this is not a probability! pnorm(k, m, s) \\(= P( X \\leq k)\\) pnorm(k, m, s, lower.tail = FALSE) \\(= P( X &gt; k) = P(X \\geq k)\\) qnorm(p, m, s) = k is the value \\(k\\) such that \\(P(X \\leq k) = p\\). qnorm(p, m, s, lower.tail = FALSE) = k is the value \\(k\\) such that \\(P(X &gt; k) = p\\). rnorm(n, m, s) returns a list of \\(n\\) randomly sampled numbers from \\(N(m, s)\\). Note: if you don’t specify a mean or standard deviation, R will default to the standard normal distribution. Let’s close this section with an example, starting with a sample and approximating the sample’s distribution with a normal distribution to make some probability estimates. The data we’ll use is the bdims data set from the openintro package. This data set gives many measurements of 507 physically active individuals; we will investigate the height variable hgt, which records a participant’s height in centimeters. The sample contains data on both men and women; let’s focus on men in this sample. maleBdim &lt;- bdims %&gt;% filter(sex == 1) Male height is approximately normally distributed as evidenced by the histogram below. ggplot(maleBdim, aes(x = hgt)) + geom_histogram(bins = 14, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + labs(x = &quot;height (cm)&quot;, title = &quot;Height of active men&quot;) the mean and standard deviation of this sample are m &lt;- mean(maleBdim$hgt) s &lt;- sd(maleBdim$hgt) c(m, s) ## [1] 177.745344 7.183629 and we can overlay a normal curve onto our histogram to heuristically assess the fit. ggplot(maleBdim, aes(x = hgt)) + # scales y-axis to a probability density geom_histogram(aes(y = ..density..), bins = 14, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + # adds our normal curve stat_function(fun = dnorm, args = list(mean = m, sd = s), color = &quot;red&quot;) + labs(x = &quot;height (cm)&quot;, title = &quot;Height of active men&quot;) It’s not a perfect fit, of course, but it’s looking pretty good! If we think of our sample as an approximation of the population we’re sampling from, we can use the normal distribution above to make probability calculations for the population. Below we’ll answer these questions and plot the regions corresponding to the probabilities in question. What is the probability a randomly sampled active male is shorter than 185 centimeters? First we’ll plot the region. ggplot(tibble(height = c(155,200)), aes(x = height)) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;area&quot;, xlim = c(155, 185), #define fill region fill = &quot;steelblue&quot;) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;line&quot;, color = &quot;black&quot;) + labs(x = &quot;height (cm)&quot;, y = &quot;density&quot;, title = &quot;Height of active men, P(H &lt;= 185)&quot;) From the image above, we infer that the probability better be greater than .5. To make this calculation exactly we use pnorm(185, mean = m, sd = s) ## [1] 0.8437254 recalling that we stored the mean as m and the standard deviation as s above. What is the probability a randomly sampled active male is taller than 175 cm? Again we’ll plot the region. ggplot(tibble(height = c(155,200)), aes(x = height)) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;area&quot;, xlim = c(175, 200), #define fill region fill = &quot;steelblue&quot;) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;line&quot;, color = &quot;black&quot;) + labs(x = &quot;height (cm)&quot;, y = &quot;density&quot;, title = &quot;Height of active men, P(H &gt;= 175)&quot;) The exact probablity can be calculated as pnorm(175, mean = m, sd = s, lower.tail = FALSE) ## [1] 0.6488312 What is the probability a randomly sampled active male is between 165 and 175 cm? Again we’ll plot the region. ggplot(tibble(height = c(155,200)), aes(x = height)) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;area&quot;, xlim = c(165, 175), #define fill region fill = &quot;steelblue&quot;) + stat_function( fun = dnorm, args = list(mean = m, sd = s), geom = &quot;line&quot;, color = &quot;black&quot;) + labs(x = &quot;height (cm)&quot;, y = &quot;density&quot;, title = &quot;Height of active men, P(165 &lt;= H &lt;= 175)&quot;) To calculate the probability, observe that \\(P(165 \\leq H \\leq 175) = P(H \\leq 175) - P(H \\leq 165)\\). In other words, we take the larger lower tail and subtract off the smaller lower tail. Making this calculation in R: pnorm(175, mean = m, sd = s) - pnorm(165, mean = m, sd = s) ## [1] 0.3131555 You meet someone online and they say that they’re an active guy in the 80th percentile for height. How tall is this person? We are given a probability/percentile and want the value associated to that, so we need to use the qnorm(...) function. In particular, qnorm(.8, mean = m, sd = s) ## [1] 183.7912 We can check this calculation with pnorm(...). pnorm(183.7912, mean = m, sd = s) ## [1] 0.7999985 Pretty good! This means if you’re about 183.8 cm tall (about 6ft.) you’d be in the 80th percentile for active adult US men. These calculations and visuals should help you get started in calculating normal distribution probabilities. It is crucial to visualize or even actually sketch out the probability you’re trying to calculate, especially when you’re getting started. Visualizing these probabilities helps you figure out exactly what combination of R functions you need to use. The next distributions will be used later in this course when we discuss more involved types of sample statistics. 5.2.2.2 Chi-squared (or \\(\\chi^2\\) ) distributions This section will be completed later in the semester when these distributions are needed. 5.2.2.3 Student’s \\(t\\)-distribution This section will be completed later in the semester when these distributions are needed. 5.2.2.4 \\(F\\)-distributions This section will be completed later in the semester when these distributions are needed. Now that we’ve finished up the basics of of probability, we’re ready to move on to inferential statistics! "],["inferential-statistics-take-1.html", "Chapter 6 Inferential statistics, take 1 6.1 Basics of statistical inference 6.2 Analyzing categorical variables 6.3 Analyzing numerical variables", " Chapter 6 Inferential statistics, take 1 This chapter and the next are going to introduce inferential statistics in R, but from two different perspectives. This chapter will focus on doing all of the calculations involved in statistical inference “by hand” (where here we really mean using R to implement the formulas/ideas). The next chapter will show you how to perform many of the same calculations using functions native to R. It is valuable knowing how to do both so that you can check one or the other or possibly write your own type of statistical test one day! Both chapters will be organized according to the type of variables we will be analyzing. So, for example, we’ll start with statistical inference for a single categorical variable. One can (and probably should) read the “by hand” section of this chapter in tandem with the corresponding native function section of the next chapter. This chapter will include a brief summary of the ideas involved in statistical inference, but the next will just go straight into the details. The code chunk below attaches all packages we’ll use in this chapter and sets a seed which basically determines a starting point for some of the randomization we’ll employ in the chapter. Setting a seed insures that the same random samples will be drawn every time the code is run. library(tidyverse) library(openintro) library(infer) set.seed(1187) 6.1 Basics of statistical inference We will learn about three primary types of statistical inference in this class, each aimed at answering different types of questions. To illustrate the types of questions we endeavor to answer, think about analyzing the political landscape in America. You could ask: What proportion of Americans identify as politically independent? Are there more politically independent people in Montana than in Idaho? How what proportion of voters will identify as politically independent in the future? The first question is asking for an estimate of a population parameter (the true proportion of independents in America). The second question, on the other hand, is asking about a difference between two population parameters. The third requires a prediction based on past data. These types of questions are certainly related (for instance, you could answer the second question by estimating each of the two population parameters), but the most common tools for answering them are slightly different. Our main tool for answering the first question above is a confidence interval, which uses an estimate of the amount of variable we expect between samples to provided a range of plausible values for the population parameter. The tool we use to answer the second question is called a hypothesis test; these test assess how likely or unlikely your sample is if there were no differences. Hypothesis tests involve understanding the amount of variability we expect to see between samples. We access this this quantity by understanding the distribution of possible samples. The probability distribution associated to all possible samples is called a sampling distribution, which we discuss more in the next section. 6.1.1 Sampling distributions Taking a simple random sample is, of course, a random process. Assigning a number like the sample proportion or sample average to this sample, then, naturally turns sample statistics into random variables. Since we can think of each sample statistic as a random variable, each sample statistic has an associated probability distribution called a sampling distribution! The overall population under consideration determines the sampling distribution and we almost never have access to population-level data, so you may wonder how or if we have any hope of understanding the sampling distribution of our statistic of interest. In particular, we hope to understand the shape, the center, and the spread of the sampling distribution. There are two primary was of doing this: through simulation and through theory. This class touches briefly on the simulation approach, but focuses mostly on the theoretical approach to accessing sampling distributions. Before moving on to describe these approaches, we need a quick definition that helps us differentiate from talking about a population distribution and talking about a sampling distribution. The standard error, abbreviated SE, of a sampling distribution is simply its standard deviation. One of the miracles of statistics([^1]: Note: it is not a real miracle since it is a mathematical theorem, but it amazes the author to this day.) is the Central Limit Theorem. We will state the Central Limit Theorem in a few different ways throughout this chapter, but each version essentially says the same thing. Namely, if you have a sufficiently large([^2]: caution: this means different things in different contexts) and high quality sample([^3]: IE a sample with independent observations, usually obtained through random sampling), the sampling distribution will be symmetric and unimodal with its center at the true population parameter. Moreover, as the sample size increases, the standard error decreases. In other words, all sample statistics will cluster around the true population parameter and observing a sample stat above the population parameter is just as likely as observing one below. The fact that the standard error decreases as the sample size increases codifies our intuition that “large samples are usually better,” since it implies that a sample statistic calculated from a larger sample is more likely to be close to the true population parameter than one calculated from a smaller sample. Before moving on to discuss how we use sampling distributions, let’s try to make this a bit more concrete by simulating a sampling distribution using a technique called bootstrapping which takes a large, random sample, and resamples it many times with replacement to simulate the process of taking many different samples. We’ll use the gss2010 data set from the openintro package which records the highest educational attainment (among other things) of 2044 randomly sampled Americans in 2010. Suppose we’re interested in knowing more about the proportion of all Americans who do not have a high school diploma. The gss2010 data set serves as our sample and we can use the sample proportion \\(\\hat{p}\\), read as “p hat”, as a point estimate for the true population proportion \\(p\\). pHat &lt;- sum(gss2010$degree == &quot;LT HIGH SCHOOL&quot;) / length(gss2010$degree) pHat ## [1] 0.1492172 So our sample indicates that in 2010, about 15% of the American population had less than a high school diploma. But this is only a sample of 2044 Americans. If we took another sample of 2044 people and calculated the same proportion, how different do we expect that proportion to be? We can simulate this by drawing from our original sample, but doing so with replacement. # Take sample sample2 &lt;- sample( gss2010$degree, size = 2044, replace = TRUE) #Find proportion of high school dropouts pHat2 &lt;- sum(sample2 == &quot;LT HIGH SCHOOL&quot;) / length(sample2) pHat2 ## [1] 0.1364971 This sample proportion is different, but not by too much. Of course, 2 different samples don’t provide enough evidence to make any conclusions about the amount of variability we should expect among all possible samples. To estimate the standard error, we need to simulate many more samples. We’ll use the following code, using commands from the infer package, to do this. The code below takes 5000 samples of size 2044 from our original sample again with replacement, changes the degree column in each sample to a binary outcome (no HS diploma, or other), counts the number of people in each sample without a high school diploma, then calculates the corresponding sample proportion. simulated_sampling_distr &lt;- gss2010 %&gt;% rep_sample_n(size = 2044, reps = 5000, replace = TRUE) %&gt;% mutate( degree = if_else(degree == &quot;LT HIGH SCHOOL&quot;, &quot;LT HIGH SCHOOL&quot;, &quot;other&quot;)) %&gt;% count(degree) %&gt;% mutate(p_hat = n / sum(n)) %&gt;% filter( degree == &quot;LT HIGH SCHOOL&quot;) head(simulated_sampling_distr) ## # A tibble: 6 × 4 ## # Groups: replicate [6] ## replicate degree n p_hat ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 LT HIGH SCHOOL 322 0.158 ## 2 2 LT HIGH SCHOOL 315 0.154 ## 3 3 LT HIGH SCHOOL 317 0.155 ## 4 4 LT HIGH SCHOOL 310 0.152 ## 5 5 LT HIGH SCHOOL 314 0.154 ## 6 6 LT HIGH SCHOOL 311 0.152 We’ve simulated a sampling distribution! Let’s look at a histogram of this simulated distribution to assess it’s shape and spread. Remember, the Central Limit Theorem says this distribution should likely by symmetric and unimodal. ggplot(simulated_sampling_distr, aes(x = p_hat)) + geom_histogram(bins = 15, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + labs(main = &quot;Distribution of sample proportions, n = 2044&quot;, subtitle = &quot;Estimating SE for 2010 proportion of Americans without a HS diploma&quot;, x = &quot;Sample proportion, p-hat&quot;) Voila! Our sampling distribution does seem to be symmetric and unimodal. Where precisely is the center? #average sample proportion mean(simulated_sampling_distr$p_hat) ## [1] 0.1492041 What, approximately, is the standard error? Simply the standard deviation of the column p_hat! sd(simulated_sampling_distr$p_hat) ## [1] 0.007862322 This means if we were to repeatedly sample 2044 Americans and record the proportion of people with less than a college degree, we should expect to see a difference of about 0.8% between the proportions on average. As will see in the next two sections, understanding the standard error is the key to performing inferential statistics. However, before we move on to using sampling distributions, let’s simulate the Central Limit Theorem’s other main claim: the standard error decreases as the sample size increases. To show this, we’ll go the opposite direction and take smaller samples from the gss2010 data set. We should see the standard error of this simulated distribution increase. The code below simulates a sampling distribution with sample size \\(n = 100\\). sampling_dist2 &lt;- gss2010 %&gt;% rep_sample_n(size = 100, reps = 5000, replace = TRUE) %&gt;% mutate( degree = if_else(degree == &quot;LT HIGH SCHOOL&quot;, &quot;LT HIGH SCHOOL&quot;, &quot;other&quot;)) %&gt;% count(degree) %&gt;% mutate(p_hat = n / sum(n)) %&gt;% filter( degree == &quot;LT HIGH SCHOOL&quot;) head(sampling_dist2) ## # A tibble: 6 × 4 ## # Groups: replicate [6] ## replicate degree n p_hat ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 LT HIGH SCHOOL 8 0.08 ## 2 2 LT HIGH SCHOOL 20 0.2 ## 3 3 LT HIGH SCHOOL 21 0.21 ## 4 4 LT HIGH SCHOOL 15 0.15 ## 5 5 LT HIGH SCHOOL 15 0.15 ## 6 6 LT HIGH SCHOOL 20 0.2 Note that just looking at the first 6 sample proportions we can already see more variability than our first distribution. Let’s visualize this one. ggplot(sampling_dist2, aes(x = p_hat)) + geom_histogram(bins = 15, color = &quot;black&quot;, fill = &quot;steelblue&quot;) + labs(main = &quot;Distribution of sample proportions, n = 100&quot;, subtitle = &quot;Estimating SE for 2010 proportion of Americans without a HS diploma&quot;, x = &quot;Sample proportion, p-hat&quot;) Notice that the center is about the same as our first sampling distribution, but our second is more spread out as we claimed above! To check these claims, let’s look at the average sample proportion and the estimated standard error. mean(sampling_dist2$p_hat) ## [1] 0.14884 sd(sampling_dist2$p_hat) ## [1] 0.0353421 The average sample proportions are super close, but the standard error of our distribution with \\(n = 100\\) is about 5 times as large as the standard error of the distribution with \\(n = 2044\\). This fits our intuition: small samples are less reliable because they have more variability around the population parameter. Now that we have a feel for sampling distributions, let’s use these simulated sampling distributions to make some inferences. 6.1.2 Confidence intervals 6.1.3 Hypothesis tests 6.2 Analyzing categorical variables Now we will move in to using theoretical sampling distributions to calculate confidence intervals and perform hypothesis tests. 6.2.1 Single sample proportion 6.2.1.1 Confidence intervals 6.2.1.2 Hypothesis testing 6.2.2 Two sample proportion data set: openintro::burger 6.2.2.1 Confidence intervals 6.2.2.2 Hypothesis testing 6.2.3 Chi-squared goodness of fit test 6.2.4 Chi-squared test of independence 6.3 Analyzing numerical variables 6.3.1 Single sample mean 6.3.1.1 Confidence intervals 6.3.1.2 Hypothesis testing 6.3.2 Two sample mean 6.3.2.1 Confidence intervals 6.3.2.2 Hypothesis testing 6.3.3 Paired data 6.3.3.1 Confidence intervals 6.3.3.2 Hypothesis testing 6.3.4 Analysis of variance (ANOVA) "],["inferential-statistics-take-2.html", "Chapter 7 Inferential statistics, take 2 7.1 Analyzing categorical variables 7.2 Analyzing numerical variables", " Chapter 7 Inferential statistics, take 2 TO DO: hypothesis testing and confidence intervals using in-built functions in R. 7.1 Analyzing categorical variables 7.1.1 Single sample 7.1.2 Two sample 7.1.3 Chi-squared goodness of fit test 7.1.4 Chi-squared test of independence 7.2 Analyzing numerical variables 7.2.1 Single sample mean 7.2.2 Two sample mean 7.2.3 Paired data 7.2.4 Analysis of variance (ANOVA) "],["modeling-relationships.html", "Chapter 8 Modeling relationships 8.1 Simple linear regression 8.2 Multiple linear regression 8.3 Bonus topic: logistic regression", " Chapter 8 Modeling relationships To do 8.1 Simple linear regression 8.2 Multiple linear regression 8.3 Bonus topic: logistic regression "],["references.html", "References", " References "]]
